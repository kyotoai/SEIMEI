{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "109a640f-af2b-4873-92c6-a380385a11fb",
   "metadata": {},
   "source": [
    "# Search-Engine-Integrated Multi-Expert Inference (SEIMEI)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5b26197-acb9-46bd-aeac-e432183adda5",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#!pip install datasets\n",
    "!pip install transformers\n",
    "#!pip install accelerate\n",
    "!pip install numpy, pandas\n",
    "!pip install matplotlib\n",
    "!pip install sentence_transformers\n",
    "!pip install huggingface_hub\n",
    "!pip install flask\n",
    "\n",
    "# For vLLM\n",
    "!pip install vllm\n",
    "!pip install ray\n",
    "!pip install packaging\n",
    "!pip install typing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "dbef1b6e-b509-4814-8be5-ed2d849eadce",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from huggingface_hub import login\n",
    "login(token=\"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\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d27ee604",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20c71bb9",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from Prepare import Prepare\n",
    "\n",
    "data_path = \"../miller\"\n",
    "save_path = \"./processed/miller\"\n",
    "\n",
    "# designate all the files with 'extensions' inside 'folder_path'\n",
    "file_info = [\n",
    "    {\"folder_path\":\"\", \"extensions\":[\".tex\"]},\n",
    "#    {\"folder_path\":\"src\", \"extensions\":[\".f90\"]},\n",
    "#    {\"folder_path\":\"run\", \"extensions\":[\"\", \".q\"]},\n",
    "#    {\"folder_path\":\"lib\", \"extensions\":[\".f90\"]},\n",
    "#    {\"folder_path\":\"\", \"extensions\":[\".txt\",\".md\"]},\n",
    "]\n",
    "\n",
    "\n",
    "# about where the key starts to split the text\n",
    "\n",
    "# index : words to be where text should be split\n",
    "# first element(0 to 1): process_text_size * element is the start point of the key splitting. the samller the element is, the more likely it is for the key to split the text.\n",
    "# second element(0 or 1): the first element should become   if 0: <text1><key> | <text2>,  if 1: <text1> | <key><text2>\n",
    "rules = [\n",
    "    {\n",
    "        #\"SUBROUTINE \" : 1,\n",
    "        #\"class \" : 1,\n",
    "        \"\\\\\\\\section*\" : 1,\n",
    "    },\n",
    "\n",
    "    {\n",
    "        \"\\\\\\\\subsection*\" : 1,\n",
    "        #\"def \" : 1,\n",
    "        #\"void \" : 1,\n",
    "    },\n",
    "\n",
    "    {\n",
    "        #\"if \" : 1,\n",
    "        #\"end if\" : 0,\n",
    "        \"\\\\\\\\begin{center}\" : 1,\n",
    "        \"\\\\\\\\end{gather*}\" : 0,\n",
    "        \"\\\\\\\\end{align*}\" : 0,   \n",
    "        \"\\\\\\\\end{equation*}\" : 0,\n",
    "        \"\\\\\\\\end{enumerate}\" : 0,\n",
    "    },\n",
    "\n",
    "    {\n",
    "        #\"else \" : 1,\n",
    "        #\"elif \" : 1,\n",
    "    },\n",
    "    \n",
    "\n",
    "    {\n",
    "        \"\\n\\n\" : 0,\n",
    "        \"<0x0A><0x0A>\" : 0,\n",
    "        \"\\x0A\\x0A\" : 0,\n",
    "    },\n",
    "\n",
    "    {\n",
    "        \"\\n\" : 0,\n",
    "        \"<0x0A>\" : 0,\n",
    "        \"\\x0A\" : 0,\n",
    "    },\n",
    "]\n",
    "\n",
    "prepare = Prepare(\n",
    "    database_path = data_path,\n",
    "    save_path = save_path,\n",
    "    rules = rules, \n",
    "    file_info=file_info, \n",
    "    model_name = \"gpt2\",\n",
    "    max_tokens = 10000,\n",
    "    min_tokens = 3000,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "633be532-fe47-4726-8d3e-bf4d13eae549",
   "metadata": {},
   "outputs": [],
   "source": [
    "prepare.make_chunks()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a984f0d2-288b-4ac4-8b93-0b40022a97f8",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Gather all save_paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4dfc4a72-e9da-4bcb-b30d-bd2d66a7dc37",
   "metadata": {},
   "outputs": [],
   "source": [
    "save_dirs = [\n",
    "    \"./processed/gkv-code\",\n",
    "    \"./processed/gkw-manual\",\n",
    "    \"./processed/miller\",\n",
    "]\n",
    "\n",
    "new_save_dir = \"./processed\"\n",
    "\n",
    "prepare.gather_save_dirs(save_dirs, new_save_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93686c65-fdef-4133-b6ca-57e4c2006599",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Manual modifiaction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c9d1db5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# If you wanna modify the chunk manually, run the code below\n",
    "prepare.modify_chunks_manually()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "003ea205",
   "metadata": {},
   "outputs": [],
   "source": [
    "# After modifying the chunks, remember to run the code below\n",
    "prepare.finish_modifying()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75e26438",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Examples of rules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db90436c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# about where the key starts to split the text\n",
    "# index : words to be where text should be split\n",
    "# first element(0 to 1): process_text_size * element is the start point of the key splitting. the samller the element is, the more likely it is for the key to split the text.\n",
    "# second element(0 or 1): the first element should become   if 0: <text1><key> | <text2>,  if 1: <text1> | <key><text2>\n",
    "\n",
    "\n",
    "# For Fortran code\n",
    "\n",
    "rules = [\n",
    "    {\n",
    "        \"SUBROUTINE \" : 1,\n",
    "    },\n",
    "\n",
    "    {\n",
    "        \"if \" : 1,\n",
    "        \"end if\" : 0,\n",
    "    },\n",
    "\n",
    "    {\n",
    "        \"\\n\\n\" : 0,\n",
    "        \"<0x0A><0x0A>\" : 0,\n",
    "        \"\\x0A\\x0A\" : 0,\n",
    "    },\n",
    "\n",
    "    {\n",
    "        \"\\n\" : 0,\n",
    "        \"<0x0A>\" : 0,\n",
    "        \"\\x0A\" : 0,\n",
    "    },\n",
    "]\n",
    "\n",
    "\n",
    "\n",
    "# For python code\n",
    "\n",
    "rules = [\n",
    "    {\n",
    "        \"class \" : 1,\n",
    "    },\n",
    "\n",
    "    {\n",
    "        \"def \" : 1,\n",
    "        #\"void \" : 1,\n",
    "    },\n",
    "\n",
    "    {\n",
    "        \"if \" : 1,\n",
    "        \"end if\" : 0,\n",
    "    },\n",
    "\n",
    "    {\n",
    "        \"else \" : 1,\n",
    "        \"elif \" : 1,\n",
    "    },\n",
    "    \n",
    "\n",
    "    {\n",
    "        \"\\n\\n\" : 0,\n",
    "        \"<0x0A><0x0A>\" : 0,\n",
    "        \"\\x0A\\x0A\" : 0,\n",
    "    },\n",
    "\n",
    "    {\n",
    "        \"\\n\" : 0,\n",
    "        \"<0x0A>\" : 0,\n",
    "        \"\\x0A\" : 0,\n",
    "    },\n",
    "]\n",
    "\n",
    "\n",
    "# For latex papers or textbooks\n",
    "\n",
    "rules = [\n",
    "    {\n",
    "        \"\\\\\\\\section*\" : 1,\n",
    "    },\n",
    "\n",
    "    {\n",
    "        \"\\\\\\\\subsection*\" : 1,\n",
    "    },\n",
    "\n",
    "    {\n",
    "        \"\\\\\\\\begin{center}\" : 1,\n",
    "        \"\\\\\\\\end{gather*}\" : 0,\n",
    "        \"\\\\\\\\end{align*}\" : 0,   \n",
    "        \"\\\\\\\\end{equation*}\" : 0,\n",
    "        \"\\\\\\\\end{enumerate}\" : 0,\n",
    "    },\n",
    "\n",
    "    {\n",
    "        \"\\n\\n\" : 0,\n",
    "        \"<0x0A><0x0A>\" : 0,\n",
    "        \"\\x0A\\x0A\" : 0,\n",
    "    },\n",
    "\n",
    "    {\n",
    "        \"\\n\" : 0,\n",
    "        \"<0x0A>\" : 0,\n",
    "        \"\\x0A\" : 0,\n",
    "    },\n",
    "]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2857456e",
   "metadata": {},
   "source": [
    "# Chat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9fe8062",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from SEIMEI import SEIMEI\n",
    "import asyncio\n",
    "\n",
    "processed_path = \"./processed\"  # input path same as save_path you used in Preparation\n",
    "expert_class_names = [\"Answer\", \"CheckInf\", \"MetaSurvey\"] # \"StructureAnalysis\", \"ChunkSurvey\", \"FileSurvey\", \"MetaSurvey\"]\n",
    "se_restrictions = [\"MetaSurvey\"]  # search engine only hits classes in this list usually (except when adding expert_restriction in kwargs)\n",
    "expert_module_names = [\"Experts.Code.Modify\"]\n",
    "\n",
    "seimei = SEIMEI(\n",
    "    processed_path = processed_path,\n",
    "    expert_class_names = expert_class_names,\n",
    "    expert_module_names = expert_module_names,\n",
    "    se_restrictions = se_restrictions,\n",
    "    max_inference_time = 300,\n",
    "    tensor_parallel_size = 1,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0d79f83",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "original_question = \"How to implement a new equilibrium state called Miller equilibrium into gyro-kinetic vlasov simulation?\"\n",
    "final_answer = await seimei.get_answer(query = original_question) # return final answer\n",
    "\n",
    "print()\n",
    "print()\n",
    "print(final_answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08946a3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from SEIMEI import Log\n",
    "Log().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5158012a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for debug\n",
    "query = \"How to implement a new equilibrium state called Miller equilibrium into gyro-kinetic vlasov simulation?\"\n",
    "\n",
    "outputs = seimei.search(query, topk = 50)\n",
    "\n",
    "import json\n",
    "with open(f\"/workspace/processed/gkv-code/chunks.json\") as json_file: chunks = json.load(json_file)\n",
    "with open(f\"/workspace/processed/gkv-code/file_paths.json\") as json_file: file_paths = json.load(json_file)\n",
    "\n",
    "print(outputs)\n",
    "\n",
    "for (expert, id) in outputs:\n",
    "    print()\n",
    "    print(f\"--- chunk id {id} ---\")\n",
    "    print(f\"file_path: {file_paths[id]}\")\n",
    "    print()\n",
    "    print(seimei.job_keys[id])\n",
    "    print()\n",
    "#print(len(seimei.infs))\n",
    "#print(seimei.get_num_tokens(seimei.infs[1][\"inf\"]))\n",
    "#print(seimei.infs[3][\"inf\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04adc1ad-46b4-4cdd-bb85-8c9404e522ad",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Kaggle AIMO Test"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84705001-f395-47af-b690-6b5c5d751c53",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### Test1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "30a1712c-3cd8-4c05-936b-6074ed28bf93",
   "metadata": {},
   "outputs": [],
   "source": [
    "from SEIMEI import SEIMEI\n",
    "import asyncio\n",
    "\n",
    "database_name = \"gkv-code\"\n",
    "job_classes = [\"SearchJob\", \"StepInference\", \"SuggestMethod\", \"EvaluateAnswer\", \"MakeAnswer\", \"CheckAnswer2\", \"SelfCorrection\", \"GiveHint\"]\n",
    "seimei = SEIMEI(database_name, job_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "074008a3-8dcd-4736-8c00-ce666b1360d5",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\n",
    "original_question = \"Find the three-digit number n such that writing any other three-digit number 10^2024 times in a row and 10^2024 + 2 times in a row results in two numbers divisible by n.\"\n",
    "\n",
    "correct_answer = \"\"\"Let M = 10^1024. Let a be any three-digit number. Writing M copies of a in a row results\n",
    "in a number X where\n",
    "X =a×100100100...1001001\n",
    "and there are M copies of the digit one in the long number. If instead we wrote M + 2 copies of a in a row, the resulting number would be 106X + 1001a. We use the notation (u, v) to denote the greatest common divisor of two integers u and v which are not both 0.\n",
    "We apply Euclid’s algorithm so\n",
    "((106X + 1001a), X) = (1001a, X).\n",
    "It is therefore a necessary condition that our three-digit number n should divide (1001a,X) for all three-digit numbers a. By considering a = 100 and a = 101, we see that any candidate for n must divide 1001 × 101 − 1001 × 100 = 1001. Moreover, if n is a divisor of 1001, then n will divide X because 1001 divides 10010010010 . . . 01001001 which is\n",
    "1001 × 10000010000010 . . . 01000001.\n",
    "The second factor involves M/2 copies of the digit one. Such an n will also divide 106X + 1001a.\n",
    "Thus it is a necessary and sufficient condition for n to satisfy the conditions of the problem that n be a three-digit divisor of 1001 (= 7 × 11 × 13). There is a unique such number: 143.\n",
    "\"\"\"\n",
    "\n",
    "await seimei.get_answer(query = original_question, correct_answer = correct_answer) # return final answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b52ea15c-2abc-4d0d-a9a6-4bc341132766",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print()\n",
    "print(\"hint\")\n",
    "print(SEIMEI.correct_answers[0][\"hint\"])\n",
    "print()\n",
    "print(\"pre_answer\")\n",
    "print(SEIMEI.correct_answers[0][\"pre_answer\"])\n",
    "print()\n",
    "print(\"answer\")\n",
    "print(SEIMEI.correct_answers[0][\"answer\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c87c91a-50e1-43df-8076-870bd14edb17",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### Test2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84310743-3355-4707-ac22-3ae00cc5f4d3",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from SEIMEI import SEIMEI\n",
    "import asyncio\n",
    "\n",
    "expert_class_names = [\"MakeStrategy\", \"EvaluateAnswer\", \"MakeAnswer2\"]\n",
    "expert_module_names = [\"Experts.AIMO2.RyuSystem\"]\n",
    "seimei = SEIMEI(expert_module_names = expert_module_names, expert_class_names = expert_class_names)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e574223c-b6e6-41ec-95df-e2dfd6880fd0",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\n",
    "original_question = \"Find the three-digit number n such that writing any other three-digit number 10^2024 times in a row and 10^2024 + 2 times in a row results in two numbers divisible by n.\"\n",
    "\n",
    "correct_answer = \"\"\"Let M = 10^1024. Let a be any three-digit number. Writing M copies of a in a row results\n",
    "in a number X where\n",
    "X =a×100100100...1001001\n",
    "and there are M copies of the digit one in the long number. If instead we wrote M + 2 copies of a in a row, the resulting number would be 106X + 1001a. We use the notation (u, v) to denote the greatest common divisor of two integers u and v which are not both 0.\n",
    "We apply Euclid’s algorithm so\n",
    "((106X + 1001a), X) = (1001a, X).\n",
    "It is therefore a necessary condition that our three-digit number n should divide (1001a,X) for all three-digit numbers a. By considering a = 100 and a = 101, we see that any candidate for n must divide 1001 × 101 − 1001 × 100 = 1001. Moreover, if n is a divisor of 1001, then n will divide X because 1001 divides 10010010010 . . . 01001001 which is\n",
    "1001 × 10000010000010 . . . 01000001.\n",
    "The second factor involves M/2 copies of the digit one. Such an n will also divide 106X + 1001a.\n",
    "Thus it is a necessary and sufficient condition for n to satisfy the conditions of the problem that n be a three-digit divisor of 1001 (= 7 × 11 × 13). There is a unique such number: 143.\n",
    "\"\"\"\n",
    "\n",
    "await seimei.get_answer(query = original_question, correct_answer = correct_answer) # return final answer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "726e313a-5540-4cf9-a727-d5713390a5ee",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from SEIMEI import Log\n",
    "Log().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "319e0075-2e15-49d8-a388-2c09318288e0",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "#### Log system test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4be65a47-0b52-4727-a53f-e6669d7cd4a7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f703e0bd-8bb2-496e-96da-583e7bd18247",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\n",
    "import ipywidgets as widgets\n",
    "from IPython.display import display\n",
    "import json\n",
    "\n",
    "\n",
    "class Log:\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.log_dict_ids = []\n",
    "        self.selected_id = 0\n",
    "        \n",
    "        with open(\"log.json\") as json_file:\n",
    "            self.logs = json.load(json_file)\n",
    "        self.all_log_dict = self.logs[-1]\n",
    "        \n",
    "        self.log_dict = self.all_log_dict\n",
    "\n",
    "\n",
    "    def get_log_dict_text(self):\n",
    "        \n",
    "        text = \"\\n<pre><span style='color:black;'>\" + self.log_dict[\"expert_class_name\"] + \"\\n\"\n",
    "    \n",
    "        for i in range(len(self.log_dict[\"called_experts\"])):\n",
    "            if i == self.selected_id:\n",
    "                text += \"<span style='color:green;'>    \" + self.log_dict[\"called_experts\"][i][\"expert_class_name\"] + \"</span>\\n\"\n",
    "                for j in range(len(self.log_dict[\"called_experts\"][i][\"called_experts\"])):\n",
    "                    text += \"       \" + self.log_dict[\"called_experts\"][i][\"called_experts\"][j][\"expert_class_name\"] + \"\\n\"\n",
    "            else:\n",
    "                text += \"    \" + self.log_dict[\"called_experts\"][i][\"expert_class_name\"] + \"\\n\"\n",
    "            \n",
    "        text += \"</span></pre>\"\n",
    "    \n",
    "        return text\n",
    "\n",
    "\n",
    "    def get_arg_return_text(self):\n",
    "        text = f\"\"\"<pre>\\n\\n--- args ---\\n{self.json_show(self.log_dict[\"called_experts\"][self.selected_id][\"args\"], 0)}\\n\\n\"\"\"\n",
    "        text += f\"\"\"--- return ---\\n{self.json_show(self.log_dict[\"called_experts\"][self.selected_id][\"return\"], 0)}</pre>\"\"\"\n",
    "        text = text.replace(\"<s>\",\"\")\n",
    "        return text\n",
    "\n",
    "    # recursive function\n",
    "    def json_show(self, element, num_column):\n",
    "        \n",
    "        text = \"\"\n",
    "        \n",
    "        if type(element) == list:\n",
    "            text += \" \"*3*num_column + \"[\\n\"\n",
    "            for i, e in enumerate(element):\n",
    "                text += \" \"*3*(num_column+1) + f\"- {i+1} -\\n\"\n",
    "                text += self.json_show(e, num_column+1) + \"\\n\"\n",
    "            text += \" \"*3*num_column + \"]\\n\"\n",
    "                \n",
    "        elif type(element) == dict:\n",
    "            for i, key in enumerate(element):\n",
    "                text += \" \"*3*num_column + f\"- {i+1} -\" + key + \" :\\n\"\n",
    "                text += self.json_show(element[key], num_column+1) + \"\\n\"\n",
    "\n",
    "        elif type(element) == str or type(element) == int or type(element) == bool or element == None:\n",
    "            text += \" \"*3*num_column + str(element) + \"\\n\"\n",
    "\n",
    "        else:\n",
    "            raise Exception(\"element must be list, dict, str or int\")\n",
    "\n",
    "        return text\n",
    "        \n",
    "    # Create a GridBox\n",
    "    def show(self):\n",
    "\n",
    "        text_display = widgets.HTML(value=self.get_log_dict_text())\n",
    "        \n",
    "        # Define functions to handle button clicks\n",
    "        def on_up_button_clicked(b):\n",
    "            if self.selected_id > 0:\n",
    "                self.selected_id -= 1\n",
    "            text_display.value = self.get_log_dict_text()\n",
    "        \n",
    "        def on_down_button_clicked(b):\n",
    "            if self.selected_id < len(self.log_dict[\"called_experts\"]) - 1:\n",
    "                self.selected_id += 1\n",
    "            text_display.value = self.get_log_dict_text()\n",
    "        \n",
    "        def on_left_button_clicked(b):\n",
    "            if self.log_dict_ids!=[]: self.log_dict_ids.pop()\n",
    "            self.log_dict = self.all_log_dict\n",
    "            for id in self.log_dict_ids:\n",
    "                self.log_dict = self.log_dict[\"called_experts\"][id]\n",
    "            text_display.value = self.get_log_dict_text()\n",
    "        \n",
    "        def on_right_button_clicked(b):\n",
    "            if self.log_dict[\"called_experts\"] != []:\n",
    "                self.log_dict = self.log_dict[\"called_experts\"][self.selected_id]\n",
    "                self.log_dict_ids.append(self.selected_id)\n",
    "                self.selected_id = 0\n",
    "            text_display.value = self.get_log_dict_text()\n",
    "        \n",
    "        def on_center_button_clicked(b):\n",
    "            text = self.get_log_dict_text()\n",
    "            text += self.get_arg_return_text()\n",
    "            text_display.value = text\n",
    "        \n",
    "        def on_left_up_button_clicked(b):\n",
    "            pass\n",
    "    \n",
    "        up_button = widgets.Button(description='Up')\n",
    "        down_button = widgets.Button(description='Down')\n",
    "        left_button = widgets.Button(description='Back')\n",
    "        right_button = widgets.Button(description='Next')\n",
    "        center_button = widgets.Button(description='Select')\n",
    "        left_up_button = widgets.Button(description='Menu')\n",
    "    \n",
    "        # Attach functions to button click events\n",
    "        up_button.on_click(on_up_button_clicked)\n",
    "        down_button.on_click(on_down_button_clicked)\n",
    "        left_button.on_click(on_left_button_clicked)\n",
    "        right_button.on_click(on_right_button_clicked)\n",
    "        center_button.on_click(on_center_button_clicked)\n",
    "        left_up_button.on_click(on_left_up_button_clicked)\n",
    "    \n",
    "        buttons = [\n",
    "            left_up_button,\n",
    "            up_button,\n",
    "            widgets.Button(description=''),\n",
    "            left_button,\n",
    "            center_button,\n",
    "            right_button,\n",
    "            widgets.Button(description=''),\n",
    "            down_button,\n",
    "            widgets.Button(description=''),\n",
    "        ]\n",
    "        \n",
    "        grid = widgets.GridBox(children=buttons,\n",
    "                               layout=widgets.Layout(grid_template_columns='repeat(3, 150px)',\n",
    "                                                     grid_template_rows='repeat(3, 30px)',\n",
    "                                                     grid_gap='10px'))\n",
    "    \n",
    "        # Display the GridBox\n",
    "        display(grid, text_display)\n",
    "\n",
    "Log().show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0a591a5-9350-49c5-a0e0-b693113e7c1d",
   "metadata": {},
   "source": [
    "## GKV test chat"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a709b00-4e6d-447b-87ca-a5cdc185c08b",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### Basic Questions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c26c58ce-2782-406f-9a04-bc00e6fccf0b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from SEIMEI import SEIMEI\n",
    "\n",
    "database_name = \"gkv-code\"\n",
    "max_llm_iter = 10\n",
    "job_classes = [\"SearchJob\", \"Answer\", \"ChunkSurvey\", \"FileSurvey\", \"MetaSurvey\", \"CheckInf\", \"StructureAnalysis\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54a6ea67-4662-4e4c-b608-e97ec384bd80",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "seimei = SEIMEI(database_name, max_llm_iter, job_classes)\n",
    "original_question = \"How to change the parameters for simulating by gkv-code? Start answering this question with figuring out what folder or file is related to user question.\"\n",
    "final_answer = seimei.get_answer(original_question) # return final answer\n",
    "print(final_answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "137448b5-c777-4400-be1a-f0125c4cf785",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "seimei = SEIMEI(database_name, max_llm_iter, job_classes)\n",
    "original_question = \"How to run the entire simulation code? Start answering this question with figuring out what folder or file is related to user question.\"\n",
    "final_answer = seimei.get_answer(original_question) # return final answer\n",
    "print(final_answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bd46a33-279c-4a0e-a335-2a88efc4be49",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "seimei = SEIMEI(database_name, max_llm_iter, job_classes)\n",
    "original_question = \"Where should I define the file name of namelist of entire simulation? Start answering this question with figuring out what folder or file is related to user question.\"\n",
    "final_answer = seimei.get_answer(original_question) # return final answer\n",
    "print(final_answer) # hullucination"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fac3a9ee-94e5-4523-bee5-4ea8676a0bd5",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "seimei = SEIMEI(database_name, max_llm_iter, job_classes)\n",
    "original_question = \"How to input the number of MPI process? Start answering this question with figuring out what folder or file is related to user question.\"\n",
    "final_answer = seimei.get_answer(original_question) # return final answer\n",
    "print(final_answer)  # could mention sub.q but not about header"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25dd4f1f-1e6c-4621-b274-d6bc14e67a71",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "seimei = SEIMEI(database_name, max_llm_iter, job_classes)\n",
    "original_question = \"I wanna add a particle which has different mass. How to change the namelist in this case? Start answering this question with figuring out what folder or file is related to user question.\"\n",
    "final_answer = seimei.get_answer(original_question) # return final answer\n",
    "print(final_answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fb2b5d5-4fed-433a-b4f2-d251e9612d49",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "seimei = SEIMEI(database_name, max_llm_iter, job_classes)\n",
    "original_question = \"I wanna run nonlinear gyro kinetic vlasov simulation. Which part of the gkv code and how should I modify? Start answering this question with figuring out what folder or file is related to user question.\"\n",
    "final_answer = seimei.get_answer(original_question) # return final answer\n",
    "print(final_answer)  \n",
    "# it seems to be a good answer, but it didn't mention name_list because there is no info about it in the database and also 9b-llm isn't good enough to speculate namelist is somewhere in the database. In this case llm should notice some parameters in README_for_namelist are not in headers. \n",
    "# to achive this inference, job to investigate further should be required."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b001884-b3f7-4837-a976-dfb0cea972fa",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### Advanced Questions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33f13da1-bb97-4010-ba44-74551238b7cc",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from SEIMEI import SEIMEI\n",
    "import asyncio\n",
    "\n",
    "database_name = \"gkv-code\"\n",
    "expert_class_names = [\"Answer\", \"CheckInf\", \"MetaSurvey\"] # \"StructureAnalysis\", \"ChunkSurvey\", \"FileSurvey\", \"MetaSurvey\"]\n",
    "se_restrictions = [\"MetaSurvey\"]  # search engine only hits classes in this list usually (except when adding expert_restriction in kwargs)\n",
    "expert_module_names = [\"Experts.Code.Modify\"]\n",
    "\n",
    "seimei = SEIMEI(\n",
    "    database_name = database_name,\n",
    "    expert_class_names = expert_class_names,\n",
    "    expert_module_names = expert_module_names,\n",
    "    se_restrictions = se_restrictions,\n",
    "    max_inference_time = 300,\n",
    "    tensor_parallel_size = 1,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "759627a5-3aaa-43f0-b5a6-e6870907fe9c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "original_question = \"How to implement a new equilibrium state called Miller equilibrium into gyro-kinetic vlasov simulation?\"\n",
    "final_answer = await seimei.get_answer(query = original_question) # return final answer\n",
    "\n",
    "print()\n",
    "print()\n",
    "print(final_answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "902384f0-1beb-4947-8c1a-381300e700a1",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from SEIMEI import Log\n",
    "Log().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8bb79f7-b021-473f-8beb-bd43a3625f1f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# for debug\n",
    "query = \"How to implement a new equilibrium state called Miller equilibrium into gyro-kinetic vlasov simulation?\"\n",
    "\n",
    "outputs = seimei.search(query, topk = 50)\n",
    "\n",
    "import json\n",
    "with open(f\"/workspace/processed/gkv-code/chunks.json\") as json_file: chunks = json.load(json_file)\n",
    "with open(f\"/workspace/processed/gkv-code/file_paths.json\") as json_file: file_paths = json.load(json_file)\n",
    "\n",
    "print(outputs)\n",
    "\n",
    "for (expert, id) in outputs:\n",
    "    print()\n",
    "    print(f\"--- chunk id {id} ---\")\n",
    "    print(f\"file_path: {file_paths[id]}\")\n",
    "    print()\n",
    "    print(seimei.job_keys[id])\n",
    "    print()\n",
    "#print(len(seimei.infs))\n",
    "#print(seimei.get_num_tokens(seimei.infs[1][\"inf\"]))\n",
    "#print(seimei.infs[3][\"inf\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "436c1635-66db-4ccb-9c30-55c9748e6907",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "original_question = \"How a variable Anum in name_list is used in the simulation?\"\n",
    "final_answer = await seimei.get_answer(query = original_question) # return final answer\n",
    "print(final_answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ccb8771-777c-4b34-abe1-ca26196ed1fc",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from SEIMEI import Log\n",
    "Log().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f9a078d-c256-417b-b9d8-f4d8e96adcc0",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "seimei = SEIMEI(database_name, max_llm_iter, job_classes)\n",
    "original_question = \"How a variable Anum in name_list is used in the simulation? Give me all the relevant calculation code.\"\n",
    "final_answer = seimei.get_answer(original_question) # return final answer\n",
    "print(final_answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb6b68eb-c1f0-4182-8b23-67e213d48636",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#for inf in seimei.infs:\n",
    "#    print(inf[\"inf\"])\n",
    "import json\n",
    "database_name = \"gkv-code\"\n",
    "\n",
    "with open(f\"../processed/{database_name}/chunks.json\") as json_file:\n",
    "    chunks = json.load(json_file)\n",
    "\n",
    "print(\"---------\")\n",
    "print(chunks[839])\n",
    "print(\"---------\")\n",
    "print(chunks[690])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44a1b6db-67bf-4e02-a491-382b5461118a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "seimei = SEIMEI(database_name, max_llm_iter, job_classes)\n",
    "original_question = \"I wanna know how does the simulation code run step by step. Analyze the structure of code and figure out the flow of the simulation.\"\n",
    "final_answer = seimei.get_answer(original_question) # return final answer\n",
    "print(final_answer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "963e30ec-dad1-43b0-8b31-0ca3ccdcbc6d",
   "metadata": {},
   "source": [
    "## Transformers test chat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1fdaa55c-80d8-40bf-8d65-ef89886c07be",
   "metadata": {},
   "outputs": [],
   "source": [
    "database_name = \"transformers\"\n",
    "max_more = 5\n",
    "max_dispose = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8f72f77-421e-4ce1-8f93-a9d543f2ae63",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# model load\n",
    "import torch\n",
    "from sentence_transformers import SentenceTransformer, util\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, TextStreamer\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available else \"cpu\"\n",
    "emb_model = SentenceTransformer(\"mixedbread-ai/mxbai-embed-large-v1\").to(device)\n",
    "\n",
    "\"\"\"\n",
    "# Model load for japanese\n",
    "import transformers\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, TextStreamer\n",
    "\n",
    "assert transformers.__version__ >= \"4.34.1\"\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\"cyberagent/calm2-7b-chat\", device_map=\"auto\", torch_dtype=\"auto\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"cyberagent/calm2-7b-chat\")\n",
    "streamer = TextStreamer(tokenizer, skip_prompt=True, skip_special_tokens=True)\n",
    "\"\"\"\n",
    "\n",
    "model_id = \"mistralai/Mistral-7B-Instruct-v0.2\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id, \n",
    "    padding_side=\"left\",\n",
    "    add_eos_token=False,\n",
    "    add_bos_token=False,)\n",
    "streamer = TextStreamer(tokenizer, skip_prompt=True, skip_special_tokens=True)\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(model_id).to(device)\n",
    "\n",
    "\n",
    "# for json enforcer\n",
    "from pydantic import BaseModel\n",
    "from lmformatenforcer import JsonSchemaParser\n",
    "from lmformatenforcer.integrations.transformers import build_transformers_prefix_allowed_tokens_fn\n",
    "from transformers import pipeline\n",
    "\n",
    "hf_pipeline = pipeline('text-generation', model=model, tokenizer = tokenizer, device = 0)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00943da4-ca6c-4f45-8747-9e12c03fd0f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available else \"cpu\"\n",
    "\n",
    "class FRAG:\n",
    "    def __init__(self, database_name, max_more, max_dispose):\n",
    "        # path for making function-explanation\n",
    "        self.path_call = f\"processed/{database_name}/calls.json\"\n",
    "        self.path_def = f\"processed/{database_name}/defs.json\"\n",
    "        self.file_paths = f\"processed/{database_name}/file_paths.json\"\n",
    "\n",
    "        self.max_more = max_more\n",
    "        self.max_dispose = max_dispose\n",
    "\n",
    "    \n",
    "    def get_answer(self, original_question):\n",
    "        generate = False\n",
    "        next_question = original_question\n",
    "        self.code_mem_list = []\n",
    "        self.keep_id_list = []\n",
    "        self.dispose_list = []\n",
    "\n",
    "        i = 0\n",
    "        while generate == False and i < self.max_more:\n",
    "            j=0\n",
    "            i += 1\n",
    "            keep = False\n",
    "            \n",
    "            while keep == False and j < self.max_dispose:\n",
    "                j += 1\n",
    "                infs, id = self.get_infs(next_question, self.dispose_list, self.keep_id_list)\n",
    "                keep, thought = self.LLM1(next_question, infs[0], id)\n",
    "                \n",
    "                if keep:\n",
    "                    self.keep_id_list.append(id)\n",
    "                    break\n",
    "                else:\n",
    "                    self.dispose_list.append(id)\n",
    "            \n",
    "            generate, next_question, thought = self.LLM2(original_question, next_question, self.code_mem_list, self.keep_id_list)\n",
    "            \n",
    "            code_mem, relation = self.SUMLLM(original_question, next_question, infs[0], id)\n",
    "            \n",
    "            self.code_mem_list.append(code_mem)\n",
    "\n",
    "        answer = self.GENELLM(original_question, self.code_mem_list, self.keep_id_list)\n",
    "\n",
    "        return answer\n",
    "\n",
    "\n",
    "    \n",
    "    def LLM1(self, question, code_inf, code_id):\n",
    "        func_des = self.get_func_description(code_id)\n",
    "\n",
    "        # for restricting answer to be json \n",
    "        class LLM1Format(BaseModel):\n",
    "            thought: str\n",
    "            keep: bool\n",
    "\n",
    "        parser = JsonSchemaParser(LLM1Format.schema())\n",
    "        prefix_function = build_transformers_prefix_allowed_tokens_fn(hf_pipeline.tokenizer, parser)\n",
    "\n",
    "        prompt = f\"\"\"\n",
    "[INST]<<SYS>>You are an excellent commander. Based on the code from system and the question provided by the user, you decide the next action and answer it in json text. Use the following criteria to make your decision:\n",
    "\n",
    "- Include {{\"keep\":true}} in json text if the code provided is related, even partially, to the user's question. This indicates that the code, while possibly incomplete or not entirely covering all aspects, still has relevance and may contain useful elements or logic that pertains to the question.\n",
    "- Include {{\"keep\":false}} in json text if the code provided is completely unrelated to the user's question. This means the code does not contribute in any way to answering the question and should be disregarded.\n",
    "\n",
    "<</SYS>>\n",
    "\n",
    "User question: {question}\n",
    "\n",
    "<<SYS>>\n",
    "Function description:\n",
    "{func_des}\n",
    "\n",
    "Code from system:\n",
    "```\n",
    "{code_inf}\n",
    "```\n",
    "\n",
    "Firstly, you need to share your opinion about the reason for your decision, then you need to share your decision. Use the json format below:\n",
    "{{\n",
    "    \"thought\": (Explain whether the given code is necessary to answer the user's question, and how it relates, even if partially.),\n",
    "    \"keep\": (Choose from \"true\" or \"false\".)\n",
    "}}\n",
    "<</SYS>>\n",
    "[/INST]\"\"\"\n",
    "        \n",
    "        output = self.get_output(prompt, max_new_tokens = 1000, prefix_function = prefix_function)\n",
    "        processed, json_mode = self.text2json(output)\n",
    "\n",
    "        if json_mode:\n",
    "            keep = processed[\"keep\"]\n",
    "            thought = processed[\"thought\"]\n",
    "        else:\n",
    "            keep = True if \"True\" in processed else False\n",
    "            thought = processed\n",
    "            \n",
    "        return keep, thought\n",
    "        \n",
    "\n",
    "\n",
    "    def LLM2(self, original_question, next_question, code_mem_list, keep_id_list):\n",
    "        combined_code = self.combine_codes(code_mem_list,keep_id_list)\n",
    "\n",
    "        # for restricting answer to be json \n",
    "        class LLM2Format(BaseModel):\n",
    "            thought: str\n",
    "            generate: bool\n",
    "            next_question: str\n",
    "\n",
    "        parser = JsonSchemaParser(LLM2Format.schema())\n",
    "        prefix_function = build_transformers_prefix_allowed_tokens_fn(hf_pipeline.tokenizer, parser)\n",
    "\n",
    "        prompt = f\"\"\"[INST]<<SYS>>You are an excellent commander. Based on the code from system and question provided by the user, you decide the next action and answer it in json text. Use the following criteria to make your decision:\n",
    "\n",
    "- Include {{\"generate\":false}} in json text if the code is related but not comprehensive enough to answer the question. This means some elements are missing, which are necessary to complete the answer or to cover all aspects of the question.\n",
    "- Include {{\"generate\":true}} in json text if the code provided fully satisfies the requirements to answer the user's question comprehensively.\n",
    "\n",
    "<</SYS>>\n",
    "\n",
    "User question: {original_question}\n",
    "Last search question:{next_question}\n",
    "\n",
    "<<SYS>>\n",
    "#Pieces of code from system:\n",
    "{combined_code}\n",
    "\n",
    "Firstly, you need to share your opinion about that the provided code is sufficient or insufficient, then you need to share your decision. Additionally, you must formulate a follow-up question to collect the missing information necessary to complete the code. Use the json format below:\n",
    "{{\n",
    "    \"thought\": (Explain why the provided code is sufficient or insufficient),\n",
    "    \"generate\": (Choose from 'true' or 'false'),\n",
    "    \"next_question\": (Formulate a question to help gather the missing or additional code required)\n",
    "}}\n",
    "<</SYS>>\n",
    "[/INST]\"\"\"\n",
    "        \n",
    "        output = self.get_output(prompt, max_new_tokens = 3500, prefix_function = prefix_function)\n",
    "        processed, json_mode = self.text2json(output)\n",
    "\n",
    "        if json_mode:\n",
    "            generate = processed[\"generate\"]\n",
    "            next_question = processed[\"next_question\"]\n",
    "            thought = processed[\"thought\"]\n",
    "        else:\n",
    "            generate = True if \"True\" in processed else False\n",
    "            next_question = processed\n",
    "            thought = processed\n",
    "            \n",
    "        return generate, next_question, thought\n",
    "\n",
    "\n",
    "\n",
    "    def SUMLLM(self, original_question, next_question, code_inf, id):\n",
    "        func_des = self.get_func_description(id)\n",
    "        add_code, folder_des = self.get_address_folder(id)\n",
    "\n",
    "        # for restricting answer to be json \n",
    "        class SUMLLMFormat(BaseModel):\n",
    "            code: str\n",
    "            relation: str\n",
    "\n",
    "        parser = JsonSchemaParser(SUMLLMFormat.schema())\n",
    "        prefix_function = build_transformers_prefix_allowed_tokens_fn(hf_pipeline.tokenizer, parser)\n",
    "        \n",
    "        prompt = f\"\"\"[INST]<<SYS>>\n",
    "You are a skilled programmer proficient in explaining code in json text. Your primary task is to identify and extract the crucial parts of code based on the pairings of user-submitted questions and corresponding code snippets. While the code often relates to the users questions, not all parts may be necessary to answer these questions. Users are specifically interested in those portions of the code that are most relevant to their inquiries. Therefore, you must focus solely on extracting these pertinent sections without modifying or editorializing the code. If no relevant code sections are found, output \"Nothing\".\n",
    "<</SYS>>\n",
    "\n",
    "User question:\n",
    "{original_question}\n",
    "\n",
    "<<SYS>>\n",
    "Question for Searching the code below:{next_question}\n",
    "#Code from system:\n",
    "\n",
    "##Code Overview Set\n",
    "{add_code}\n",
    "\n",
    "{folder_des}\n",
    "\n",
    "{func_des}\n",
    "\n",
    "Code:\n",
    "```\n",
    "{code_inf}\n",
    "```\n",
    "\n",
    "You are required to extract the significant sections from the provided code that are essential for answering the user's question and return it in json text. Highlight these sections and explain their relevance to the question without altering the original code format or content. Please follow the json format below:\n",
    "\n",
    "{{\n",
    "    \"thought\": (Quick explanation of the answer you will give in the folloing.),\n",
    "    \"code\": (The critical parts of the code necessary to answer the user's question. Do not modify or editorialize the code. If no sections of the code are critical, you should explicitly output \"Nothing\".),\n",
    "    \"relation\": (Tell me relation between the code and Users question. If no sections of the code are related, you should explicitly output \"Nothing\")\n",
    "}}\n",
    "\n",
    "<</SYS>>\n",
    "[/INST]\"\"\"\n",
    "        \n",
    "        output = self.get_output(prompt, max_new_tokens = 2500, prefix_function = prefix_function)\n",
    "        processed, json_mode = self.text2json(output)\n",
    "\n",
    "        if json_mode:\n",
    "            code = processed[\"code\"]\n",
    "            relation = processed[\"relation\"]\n",
    "        else:\n",
    "            code = processed\n",
    "            relation = processed\n",
    "            \n",
    "        return code, relation\n",
    "\n",
    "\n",
    "    \n",
    "    def GENELLM(self, original_question, code_mem_list, keep_id_list):\n",
    "        combined_code = self.combine_codes(code_mem_list,keep_id_list)\n",
    "        \n",
    "        prompt = f\"\"\"[INST]<<SYS>>\n",
    "You are an excellent programmer and are adept at explaining code. You will be provided with one or more pieces of code along with corresponding questions from systems. The provided code is selected from a larger codebase specifically to enable you to answer these questions. Your task is to answer the user’s questions as thoroughly and clearly as possible, demonstrating your understanding and ability to communicate key coding concepts.\n",
    "\n",
    "<</SYS>>\n",
    "\n",
    "User question:\n",
    "{original_question}\n",
    "\n",
    "<<SYS>>\n",
    "#Pieces of code from system:\n",
    "\n",
    "{combined_code}\n",
    "<</SYS>>[/INST]\"\"\"\n",
    "        \n",
    "        return self.get_output(prompt, max_new_tokens = 2500)\n",
    "\n",
    "    \n",
    "    def get_output(self, prompt, max_new_tokens = 1000, prefix_function = None):\n",
    "        print()\n",
    "        print(\"=== input ===\")\n",
    "        print(prompt)\n",
    "        \n",
    "        if prefix_function == None:\n",
    "            print()\n",
    "            print(\"=== normal output ===\")\n",
    "            input_ids = tokenizer(prompt, return_tensors=\"pt\").to(device)\n",
    "            output_ids = model.generate(\n",
    "                **input_ids,\n",
    "                max_new_tokens=max_new_tokens,\n",
    "                do_sample=True,\n",
    "                temperature=0.8,\n",
    "                streamer=streamer\n",
    "            )\n",
    "            output = tokenizer.decode(output_ids[0][len(input_ids[0]):], skip_special_tokens = True)\n",
    "            return output\n",
    "\n",
    "        else:\n",
    "            print()\n",
    "            print(\"=== json output ===\")\n",
    "\n",
    "            #hf_pipeline.max_length = max_new_tokens\n",
    "            output_dict = hf_pipeline(prompt, max_new_tokens = max_new_tokens, prefix_allowed_tokens_fn = prefix_function)\n",
    "            print(output_dict[0]['generated_text'][len(prompt):])\n",
    "            \n",
    "            return output_dict[0]['generated_text'][len(prompt):]\n",
    "        \n",
    "\n",
    "    def text2json(self, text):\n",
    "        try:\n",
    "            output = json.loads(text)\n",
    "            return output, True\n",
    "    \n",
    "        except:\n",
    "            print()\n",
    "            print(\"Failed to get json type object\")\n",
    "            return text, False\n",
    "\n",
    "    \n",
    "    def get_infs(self, question, disposed_id_list, keep_id_list):\n",
    "        # 問題文に基づいて検索する\n",
    "        q_embs = torch.tensor(emb_model.encode(question)).to(device)\n",
    "        inf_embs = torch.load(f\"processed/{database_name}/summary_embs.pt\").to(device)\n",
    "        \n",
    "        with open(f\"processed/{database_name}/chunks.json\") as json_file:\n",
    "            chunks = json.load(json_file)\n",
    "    \n",
    "        relevance = torch.matmul(q_embs, inf_embs.T) \n",
    "        \n",
    "        # Top-3 のIDを取得\n",
    "        values, inf_ids = torch.topk(relevance, k=3, dim=0)  # dim=1 で行ごとのTop-Kを取得\n",
    "        \n",
    "        infs = []\n",
    "        selected_id = None\n",
    "        for id in inf_ids:\n",
    "            if id.item() not in disposed_id_list:\n",
    "                if id.item() not in keep_id_list:\n",
    "                    selected_id = id.item()\n",
    "                    infs.append(chunks[selected_id])\n",
    "                    break  # 最初に見つかった適切なIDで終了\n",
    "    \n",
    "        if selected_id == None:\n",
    "            values, inf_ids = torch.topk(relevance, k=relevance.shape[0], dim=0)\n",
    "            for id in inf_ids:\n",
    "                if id.item() not in disposed_id_list:\n",
    "                    if id.item() not in keep_id_list:\n",
    "                        selected_id = id.item()\n",
    "                        infs.append(chunks[selected_id])\n",
    "                        break  # 最初に見つかった適切なIDで終了\n",
    "                \n",
    "        return infs, selected_id\n",
    "    \n",
    "    \n",
    "    \n",
    "    def get_func_description(self, id):\n",
    "        #initialize func_list\n",
    "        func_list = []\n",
    "        func_set = set()\n",
    "        # open calls folder\n",
    "        with open(self.path_call, 'r') as file:\n",
    "            data = json.load(file)\n",
    "            functions = data[id]\n",
    "        for key1, value1 in functions.items():\n",
    "            # open defs folder\n",
    "            with open(self.path_def, 'r') as file:\n",
    "                 defs_data = json.load(file)\n",
    "            \n",
    "            for def_item in defs_data:\n",
    "                for key2, value2 in def_item.items():\n",
    "                    if key2 == key1:\n",
    "                        if key2 not in func_set:\n",
    "                            func_set.add(key2)\n",
    "                            func_list.append(f\"{key2}:{value2}\")\n",
    "    \n",
    "        if not func_list:\n",
    "            return \"\"\n",
    "        \n",
    "        formatted_descriptions = [\n",
    "            f\"- {desc.split(':')[0]}: {desc.split(':')[1].strip()}.\"\n",
    "            for desc in func_list\n",
    "        ]\n",
    "    \n",
    "        # 最終的な説明文を生成\n",
    "        description_of_functions = \"Description of the functions used in the code below:\\n\" + \"\\n\".join(formatted_descriptions)\n",
    "        \n",
    "        return description_of_functions\n",
    "    \n",
    "    \n",
    "    \n",
    "    def get_address_folder(self, id):\n",
    "        # get file_paths from id\n",
    "        with open(self.file_paths, 'r') as file:\n",
    "            data = json.load(file)\n",
    "            file_path = data[id]\n",
    "        f_name_list, f_summary_list = self.get_path_summaries(file_path, database_name)\n",
    "        address_code = self.generate_tree_structure(f_name_list)\n",
    "        formatted_descripitions = self.format_descriptions(f_name_list, f_summary_list)\n",
    "        return address_code, formatted_descripitions\n",
    "    \n",
    "    \n",
    "    \n",
    "    def get_path_summaries(self, file_path, dataset_name):\n",
    "        file_path_json = f\"processed/{database_name}/f_summary.json\"\n",
    "        with open(file_path_json) as json_file:\n",
    "            f_summary = json.load(json_file)\n",
    "    \n",
    "        f_name_list = []\n",
    "        f_summary_list = []\n",
    "        while \"/\" in file_path: # not run when path == data where summary of dataset_name folder is already added to the list\n",
    "            f_name_list.insert(0, os.path.basename(file_path))\n",
    "            f_summary_list.insert(0, f_summary[file_path])\n",
    "            file_path = os.path.dirname(file_path)\n",
    "            \n",
    "        return f_name_list, f_summary_list\n",
    "    \n",
    "    \n",
    "    \n",
    "    def generate_tree_structure(self, folders_files):\n",
    "        # 基本のパスを設定\n",
    "        base = \"The address of code below:{\\n\"\n",
    "        # 各フォルダやファイルに対してツリーノードを追加\n",
    "        indent = \"\"\n",
    "        for i, item in enumerate(folders_files):\n",
    "            if i < len(folders_files) - 1:  # 最後の要素でない場合\n",
    "                base += f\"{indent}|─ {item}/\\n\"\n",
    "                indent += \"|   \"  # インデントを追加\n",
    "            else:  # 最後の要素の場合\n",
    "                base += f\"{indent}|─ {item}/\\n\"\n",
    "        base += \"}\"\n",
    "        return base\n",
    "\n",
    "    \n",
    "    # フォルダとファイルの説明をフォーマットする関数\n",
    "    def format_descriptions(self, f_name_list, f_summary_list):\n",
    "        formatted_text = \"Folder and file descriptions:\\n\"\n",
    "        for name, desc in zip(f_name_list, f_summary_list):\n",
    "            formatted_text += f\"  - name: {name}\\n    description: {desc}\\n\"\n",
    "        return formatted_text\n",
    "        \n",
    "        \n",
    "    \n",
    "    def get_prompt(self, q, inf_list):\n",
    "        prompt = q + \"\\nCode:\"\n",
    "        for inf in inf_list:\n",
    "            prompt += \"\\n```\" + inf + \"```\"\n",
    "            \n",
    "        return prompt\n",
    "    \n",
    "    def combine_codes(self, code_mem_list,keep_id_list):\n",
    "        combined_code = \"\"\n",
    "        for id, code in zip(keep_id_list, code_mem_list):\n",
    "            set = \"##Code Overview Set\"\n",
    "            add_code, folder_des = self.get_address_folder(id)\n",
    "            func_des = self.get_func_description(id)\n",
    "            set += f\"\\n{add_code}\\n\\n{folder_des}\\n\\n{func_des}\\n\\n```\\n{code}\\n```\\n\\n\"\n",
    "            combined_code += set\n",
    "        return combined_code\n",
    "\n",
    "    def get_new_question(self, output):\n",
    "        # 'Next question:' または 'Next question :' のインデックスを取得\n",
    "        next_question_index = output.find('Next question:')\n",
    "        if next_question_index != -1:\n",
    "            # 'Next question:'の後の空白をスキップ\n",
    "            question_start_index = next_question_index + len('Next question:')\n",
    "            while output[question_start_index] == ' ':\n",
    "                question_start_index += 1\n",
    "            \n",
    "            # 質問文を取得し、不要なタグを削除\n",
    "            question_end_index = output.find('</s>', question_start_index)\n",
    "            if question_end_index == -1:\n",
    "                question_end_index = None  # タグがない場合は文字列の最後までが質問\n",
    "            question = output[question_start_index:question_end_index].strip()\n",
    "        else:\n",
    "            question = \"Next question not found in input\"\n",
    "        \n",
    "        return question\n",
    "\n",
    "\n",
    "# jsonでerrorが出た時に、もっといい方法があると思う（LLMのoutputをrelationなどにわけず柔軟に対応できたらもっといい）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3af390b8-7853-4974-8987-9b492f438c2a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "original_question = \"\"\"\n",
    "where is a folder to define input of the pretrained_model?\n",
    "\"\"\"\n",
    "frag = FRAG(database_name, max_more, max_dispose)\n",
    "frag.get_answer(original_question)\n",
    "\n",
    "# この質問に関しては検索エンジンの性能とenvironmentの説明をもっと詳しくしていくだけ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f83a5c35-f8e2-4f0d-917b-5cea1c8d0394",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "original_question = \"\"\"\n",
    "what's the difference between mistral and mixtral?\n",
    "\"\"\"\n",
    "frag = FRAG(database_name, max_more, max_dispose)\n",
    "frag.get_answer(original_question)\n",
    "\n",
    "#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c613e58e-c2a7-4e06-a81d-73732a5ea0e9",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "original_question = \"\"\"\n",
    "Explain the structure of Trainer class.\n",
    "\"\"\"\n",
    "frag = FRAG(database_name, max_more, max_dispose)\n",
    "frag.get_answer(original_question)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7c5e193-4617-431d-8b54-39abd7d61a06",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "original_question = \"\"\"\n",
    "I wanna modify mistral_modeling.py file so that I can designate layers of hidden state and only those outputs are returned. How to modify the code?\n",
    "\"\"\"\n",
    "frag = FRAG(database_name, max_more, max_dispose)\n",
    "frag.get_answer(original_question)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1578e68-d024-435c-a08a-6202a6dfe622",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "original_question = \"\"\"\n",
    "I wanna add LeakyReLU function into transformers source code. Tell me where to insert the function and the code to be inserted.\"\"\"\n",
    "frag = FRAG(database_name, max_more, max_dispose)\n",
    "frag.get_answer(original_question)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
