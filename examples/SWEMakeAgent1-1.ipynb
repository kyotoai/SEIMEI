{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe0bf83e-cd05-417e-bacb-bbc1501704df",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "cd /workspace\n",
    "pip install \"huggingface_hub[hf_transfer]\"\n",
    "pip install hf_transfer\n",
    "HF_HUB_ENABLE_HF_TRANSFER=1 huggingface-cli download Qwen/Qwen2.5-3B-Instruct --local-dir ./qwen3b/\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4b0cb6f2-8bd1-4815-bb2c-b3acba957f2a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting datasets\n",
      "  Downloading datasets-3.5.0-py3-none-any.whl.metadata (19 kB)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from datasets) (3.9.0)\n",
      "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from datasets) (1.24.1)\n",
      "Collecting pyarrow>=15.0.0 (from datasets)\n",
      "  Downloading pyarrow-19.0.1-cp310-cp310-manylinux_2_28_x86_64.whl.metadata (3.3 kB)\n",
      "Collecting dill<0.3.9,>=0.3.0 (from datasets)\n",
      "  Downloading dill-0.3.8-py3-none-any.whl.metadata (10 kB)\n",
      "Collecting pandas (from datasets)\n",
      "  Downloading pandas-2.2.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (89 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m89.9/89.9 kB\u001b[0m \u001b[31m4.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting requests>=2.32.2 (from datasets)\n",
      "  Downloading requests-2.32.3-py3-none-any.whl.metadata (4.6 kB)\n",
      "Collecting tqdm>=4.66.3 (from datasets)\n",
      "  Downloading tqdm-4.67.1-py3-none-any.whl.metadata (57 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m57.7/57.7 kB\u001b[0m \u001b[31m29.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting xxhash (from datasets)\n",
      "  Downloading xxhash-3.5.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (12 kB)\n",
      "Collecting multiprocess<0.70.17 (from datasets)\n",
      "  Downloading multiprocess-0.70.16-py310-none-any.whl.metadata (7.2 kB)\n",
      "Requirement already satisfied: fsspec<=2024.12.0,>=2023.1.0 in /usr/local/lib/python3.10/dist-packages (from fsspec[http]<=2024.12.0,>=2023.1.0->datasets) (2023.4.0)\n",
      "Collecting aiohttp (from datasets)\n",
      "  Downloading aiohttp-3.11.14-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (7.7 kB)\n",
      "Collecting huggingface-hub>=0.24.0 (from datasets)\n",
      "  Downloading huggingface_hub-0.29.3-py3-none-any.whl.metadata (13 kB)\n",
      "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from datasets) (23.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from datasets) (6.0.1)\n",
      "Collecting aiohappyeyeballs>=2.3.0 (from aiohttp->datasets)\n",
      "  Downloading aiohappyeyeballs-2.6.1-py3-none-any.whl.metadata (5.9 kB)\n",
      "Collecting aiosignal>=1.1.2 (from aiohttp->datasets)\n",
      "  Downloading aiosignal-1.3.2-py2.py3-none-any.whl.metadata (3.8 kB)\n",
      "Collecting async-timeout<6.0,>=4.0 (from aiohttp->datasets)\n",
      "  Downloading async_timeout-5.0.1-py3-none-any.whl.metadata (5.1 kB)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (23.1.0)\n",
      "Collecting frozenlist>=1.1.1 (from aiohttp->datasets)\n",
      "  Downloading frozenlist-1.5.0-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (13 kB)\n",
      "Collecting multidict<7.0,>=4.5 (from aiohttp->datasets)\n",
      "  Downloading multidict-6.2.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.9 kB)\n",
      "Collecting propcache>=0.2.0 (from aiohttp->datasets)\n",
      "  Downloading propcache-0.3.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (10 kB)\n",
      "Collecting yarl<2.0,>=1.17.0 (from aiohttp->datasets)\n",
      "  Downloading yarl-1.18.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (69 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m69.2/69.2 kB\u001b[0m \u001b[31m26.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hINFO: pip is looking at multiple versions of huggingface-hub to determine which version is compatible with other requirements. This could take a while.\n",
      "Collecting huggingface-hub>=0.24.0 (from datasets)\n",
      "  Downloading huggingface_hub-0.29.2-py3-none-any.whl.metadata (13 kB)\n",
      "  Downloading huggingface_hub-0.29.1-py3-none-any.whl.metadata (13 kB)\n",
      "  Downloading huggingface_hub-0.29.0-py3-none-any.whl.metadata (13 kB)\n",
      "  Downloading huggingface_hub-0.28.1-py3-none-any.whl.metadata (13 kB)\n",
      "  Downloading huggingface_hub-0.28.0-py3-none-any.whl.metadata (13 kB)\n",
      "  Downloading huggingface_hub-0.27.1-py3-none-any.whl.metadata (13 kB)\n",
      "  Downloading huggingface_hub-0.27.0-py3-none-any.whl.metadata (13 kB)\n",
      "INFO: pip is still looking at multiple versions of huggingface-hub to determine which version is compatible with other requirements. This could take a while.\n",
      "  Downloading huggingface_hub-0.26.5-py3-none-any.whl.metadata (13 kB)\n",
      "  Downloading huggingface_hub-0.26.3-py3-none-any.whl.metadata (13 kB)\n",
      "  Downloading huggingface_hub-0.26.2-py3-none-any.whl.metadata (13 kB)\n",
      "  Downloading huggingface_hub-0.26.1-py3-none-any.whl.metadata (13 kB)\n",
      "  Downloading huggingface_hub-0.26.0-py3-none-any.whl.metadata (13 kB)\n",
      "INFO: This is taking longer than usual. You might need to provide the dependency resolver with stricter constraints to reduce runtime. See https://pip.pypa.io/warnings/backtracking for guidance. If you want to abort this run, press Ctrl + C.\n",
      "  Downloading huggingface_hub-0.25.2-py3-none-any.whl.metadata (13 kB)\n",
      "  Downloading huggingface_hub-0.25.1-py3-none-any.whl.metadata (13 kB)\n",
      "  Downloading huggingface_hub-0.25.0-py3-none-any.whl.metadata (13 kB)\n",
      "  Downloading huggingface_hub-0.24.7-py3-none-any.whl.metadata (13 kB)\n",
      "  Downloading huggingface_hub-0.24.6-py3-none-any.whl.metadata (13 kB)\n",
      "  Downloading huggingface_hub-0.24.5-py3-none-any.whl.metadata (13 kB)\n",
      "  Downloading huggingface_hub-0.24.4-py3-none-any.whl.metadata (13 kB)\n",
      "  Downloading huggingface_hub-0.24.3-py3-none-any.whl.metadata (13 kB)\n",
      "  Downloading huggingface_hub-0.24.2-py3-none-any.whl.metadata (13 kB)\n",
      "  Downloading huggingface_hub-0.24.1-py3-none-any.whl.metadata (13 kB)\n",
      "  Downloading huggingface_hub-0.24.0-py3-none-any.whl.metadata (13 kB)\n",
      "Collecting fsspec[http]<=2024.12.0,>=2023.1.0 (from datasets)\n",
      "  Downloading fsspec-2024.12.0-py3-none-any.whl.metadata (11 kB)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.24.0->datasets) (4.4.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets) (2.1.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets) (1.26.13)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets) (2022.12.7)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2.8.2)\n",
      "Collecting pytz>=2020.1 (from pandas->datasets)\n",
      "  Downloading pytz-2025.2-py2.py3-none-any.whl.metadata (22 kB)\n",
      "Collecting tzdata>=2022.7 (from pandas->datasets)\n",
      "  Downloading tzdata-2025.2-py2.py3-none-any.whl.metadata (1.4 kB)\n",
      "Requirement already satisfied: six>=1.5 in /usr/lib/python3/dist-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.16.0)\n",
      "Downloading datasets-3.5.0-py3-none-any.whl (491 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m491.2/491.2 kB\u001b[0m \u001b[31m139.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading dill-0.3.8-py3-none-any.whl (116 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m116.3/116.3 kB\u001b[0m \u001b[31m75.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading fsspec-2024.12.0-py3-none-any.whl (183 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m183.9/183.9 kB\u001b[0m \u001b[31m105.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading huggingface_hub-0.29.3-py3-none-any.whl (468 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m469.0/469.0 kB\u001b[0m \u001b[31m180.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading aiohttp-3.11.14-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.6 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.6/1.6 MB\u001b[0m \u001b[31m270.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading multiprocess-0.70.16-py310-none-any.whl (134 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m134.8/134.8 kB\u001b[0m \u001b[31m79.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading pyarrow-19.0.1-cp310-cp310-manylinux_2_28_x86_64.whl (42.1 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m42.1/42.1 MB\u001b[0m \u001b[31m173.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading requests-2.32.3-py3-none-any.whl (64 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m64.9/64.9 kB\u001b[0m \u001b[31m34.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading tqdm-4.67.1-py3-none-any.whl (78 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m78.5/78.5 kB\u001b[0m \u001b[31m45.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading pandas-2.2.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (13.1 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.1/13.1 MB\u001b[0m \u001b[31m240.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading xxhash-3.5.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (194 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m194.1/194.1 kB\u001b[0m \u001b[31m96.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading aiohappyeyeballs-2.6.1-py3-none-any.whl (15 kB)\n",
      "Downloading aiosignal-1.3.2-py2.py3-none-any.whl (7.6 kB)\n",
      "Downloading async_timeout-5.0.1-py3-none-any.whl (6.2 kB)\n",
      "Downloading frozenlist-1.5.0-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (241 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m241.9/241.9 kB\u001b[0m \u001b[31m107.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading multidict-6.2.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (129 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m129.8/129.8 kB\u001b[0m \u001b[31m73.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading propcache-0.3.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (206 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m206.6/206.6 kB\u001b[0m \u001b[31m101.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading pytz-2025.2-py2.py3-none-any.whl (509 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m509.2/509.2 kB\u001b[0m \u001b[31m165.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading tzdata-2025.2-py2.py3-none-any.whl (347 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m347.8/347.8 kB\u001b[0m \u001b[31m136.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading yarl-1.18.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (319 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m319.7/319.7 kB\u001b[0m \u001b[31m136.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: pytz, xxhash, tzdata, tqdm, requests, pyarrow, propcache, multidict, fsspec, frozenlist, dill, async-timeout, aiohappyeyeballs, yarl, pandas, multiprocess, huggingface-hub, aiosignal, aiohttp, datasets\n",
      "  Attempting uninstall: requests\n",
      "    Found existing installation: requests 2.31.0\n",
      "    Uninstalling requests-2.31.0:\n",
      "      Successfully uninstalled requests-2.31.0\n",
      "  Attempting uninstall: fsspec\n",
      "    Found existing installation: fsspec 2023.4.0\n",
      "    Uninstalling fsspec-2023.4.0:\n",
      "      Successfully uninstalled fsspec-2023.4.0\n",
      "Successfully installed aiohappyeyeballs-2.6.1 aiohttp-3.11.14 aiosignal-1.3.2 async-timeout-5.0.1 datasets-3.5.0 dill-0.3.8 frozenlist-1.5.0 fsspec-2024.12.0 huggingface-hub-0.29.3 multidict-6.2.0 multiprocess-0.70.16 pandas-2.2.3 propcache-0.3.1 pyarrow-19.0.1 pytz-2025.2 requests-2.32.3 tqdm-4.67.1 tzdata-2025.2 xxhash-3.5.0 yarl-1.18.3\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.3.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.0.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython -m pip install --upgrade pip\u001b[0m\n",
      "Collecting transformers\n",
      "  Downloading transformers-4.50.3-py3-none-any.whl.metadata (39 kB)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.9.0)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.26.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.29.3)\n",
      "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (1.24.1)\n",
      "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (23.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0.1)\n",
      "Collecting regex!=2019.12.17 (from transformers)\n",
      "  Downloading regex-2024.11.6-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (40 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m40.5/40.5 kB\u001b[0m \u001b[31m4.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.32.3)\n",
      "Collecting tokenizers<0.22,>=0.21 (from transformers)\n",
      "  Downloading tokenizers-0.21.1-cp39-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.8 kB)\n",
      "Collecting safetensors>=0.4.3 (from transformers)\n",
      "  Downloading safetensors-0.5.3-cp38-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (3.8 kB)\n",
      "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.67.1)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.26.0->transformers) (2024.12.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.26.0->transformers) (4.4.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2.1.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (1.26.13)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2022.12.7)\n",
      "Downloading transformers-4.50.3-py3-none-any.whl (10.2 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m10.2/10.2 MB\u001b[0m \u001b[31m113.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading regex-2024.11.6-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (781 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m781.7/781.7 kB\u001b[0m \u001b[31m175.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading safetensors-0.5.3-cp38-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (471 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m471.6/471.6 kB\u001b[0m \u001b[31m150.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading tokenizers-0.21.1-cp39-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.0 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.0/3.0 MB\u001b[0m \u001b[31m235.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: safetensors, regex, tokenizers, transformers\n",
      "Successfully installed regex-2024.11.6 safetensors-0.5.3 tokenizers-0.21.1 transformers-4.50.3\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.3.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.0.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython -m pip install --upgrade pip\u001b[0m\n",
      "Collecting accelerate\n",
      "  Downloading accelerate-1.5.2-py3-none-any.whl.metadata (19 kB)\n",
      "Requirement already satisfied: numpy<3.0.0,>=1.17 in /usr/local/lib/python3.10/dist-packages (from accelerate) (1.24.1)\n",
      "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from accelerate) (23.2)\n",
      "Requirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (from accelerate) (5.9.6)\n",
      "Requirement already satisfied: pyyaml in /usr/local/lib/python3.10/dist-packages (from accelerate) (6.0.1)\n",
      "Requirement already satisfied: torch>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from accelerate) (2.1.0+cu118)\n",
      "Requirement already satisfied: huggingface-hub>=0.21.0 in /usr/local/lib/python3.10/dist-packages (from accelerate) (0.29.3)\n",
      "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.10/dist-packages (from accelerate) (0.5.3)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.21.0->accelerate) (3.9.0)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.21.0->accelerate) (2024.12.0)\n",
      "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.21.0->accelerate) (2.32.3)\n",
      "Requirement already satisfied: tqdm>=4.42.1 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.21.0->accelerate) (4.67.1)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.21.0->accelerate) (4.4.0)\n",
      "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=2.0.0->accelerate) (1.12)\n",
      "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=2.0.0->accelerate) (3.0)\n",
      "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=2.0.0->accelerate) (3.1.2)\n",
      "Requirement already satisfied: triton==2.1.0 in /usr/local/lib/python3.10/dist-packages (from torch>=2.0.0->accelerate) (2.1.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=2.0.0->accelerate) (2.1.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.21.0->accelerate) (2.1.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.21.0->accelerate) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.21.0->accelerate) (1.26.13)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.21.0->accelerate) (2022.12.7)\n",
      "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=2.0.0->accelerate) (1.3.0)\n",
      "Downloading accelerate-1.5.2-py3-none-any.whl (345 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m345.1/345.1 kB\u001b[0m \u001b[31m8.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: accelerate\n",
      "Successfully installed accelerate-1.5.2\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.3.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.0.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython -m pip install --upgrade pip\u001b[0m\n",
      "\u001b[31mERROR: Invalid requirement: 'numpy,'\u001b[0m\u001b[31m\n",
      "\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.3.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.0.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython -m pip install --upgrade pip\u001b[0m\n",
      "Collecting matplotlib\n",
      "  Downloading matplotlib-3.10.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (11 kB)\n",
      "Collecting contourpy>=1.0.1 (from matplotlib)\n",
      "  Downloading contourpy-1.3.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (5.4 kB)\n",
      "Collecting cycler>=0.10 (from matplotlib)\n",
      "  Downloading cycler-0.12.1-py3-none-any.whl.metadata (3.8 kB)\n",
      "Collecting fonttools>=4.22.0 (from matplotlib)\n",
      "  Downloading fonttools-4.56.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (101 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m101.9/101.9 kB\u001b[0m \u001b[31m3.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting kiwisolver>=1.3.1 (from matplotlib)\n",
      "  Downloading kiwisolver-1.4.8-cp310-cp310-manylinux_2_12_x86_64.manylinux2010_x86_64.whl.metadata (6.2 kB)\n",
      "Requirement already satisfied: numpy>=1.23 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (1.24.1)\n",
      "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (23.2)\n",
      "Requirement already satisfied: pillow>=8 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (9.3.0)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in /usr/lib/python3/dist-packages (from matplotlib) (2.4.7)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (2.8.2)\n",
      "Requirement already satisfied: six>=1.5 in /usr/lib/python3/dist-packages (from python-dateutil>=2.7->matplotlib) (1.16.0)\n",
      "Downloading matplotlib-3.10.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (8.6 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m8.6/8.6 MB\u001b[0m \u001b[31m103.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading contourpy-1.3.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (324 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m325.0/325.0 kB\u001b[0m \u001b[31m93.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading cycler-0.12.1-py3-none-any.whl (8.3 kB)\n",
      "Downloading fonttools-4.56.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (4.6 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.6/4.6 MB\u001b[0m \u001b[31m190.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading kiwisolver-1.4.8-cp310-cp310-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (1.6 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.6/1.6 MB\u001b[0m \u001b[31m207.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: kiwisolver, fonttools, cycler, contourpy, matplotlib\n",
      "Successfully installed contourpy-1.3.1 cycler-0.12.1 fonttools-4.56.0 kiwisolver-1.4.8 matplotlib-3.10.1\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.3.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.0.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython -m pip install --upgrade pip\u001b[0m\n",
      "Collecting sentence_transformers\n",
      "  Downloading sentence_transformers-4.0.1-py3-none-any.whl.metadata (13 kB)\n",
      "Requirement already satisfied: transformers<5.0.0,>=4.41.0 in /usr/local/lib/python3.10/dist-packages (from sentence_transformers) (4.50.3)\n",
      "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from sentence_transformers) (4.67.1)\n",
      "Requirement already satisfied: torch>=1.11.0 in /usr/local/lib/python3.10/dist-packages (from sentence_transformers) (2.1.0+cu118)\n",
      "Collecting scikit-learn (from sentence_transformers)\n",
      "  Downloading scikit_learn-1.6.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (18 kB)\n",
      "Collecting scipy (from sentence_transformers)\n",
      "  Downloading scipy-1.15.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (61 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.0/62.0 kB\u001b[0m \u001b[31m2.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: huggingface-hub>=0.20.0 in /usr/local/lib/python3.10/dist-packages (from sentence_transformers) (0.29.3)\n",
      "Requirement already satisfied: Pillow in /usr/local/lib/python3.10/dist-packages (from sentence_transformers) (9.3.0)\n",
      "Collecting typing_extensions>=4.5.0 (from sentence_transformers)\n",
      "  Downloading typing_extensions-4.13.0-py3-none-any.whl.metadata (3.0 kB)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.20.0->sentence_transformers) (3.9.0)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.20.0->sentence_transformers) (2024.12.0)\n",
      "Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.20.0->sentence_transformers) (23.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.20.0->sentence_transformers) (6.0.1)\n",
      "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.20.0->sentence_transformers) (2.32.3)\n",
      "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=1.11.0->sentence_transformers) (1.12)\n",
      "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.11.0->sentence_transformers) (3.0)\n",
      "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.11.0->sentence_transformers) (3.1.2)\n",
      "Requirement already satisfied: triton==2.1.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.11.0->sentence_transformers) (2.1.0)\n",
      "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers<5.0.0,>=4.41.0->sentence_transformers) (1.24.1)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers<5.0.0,>=4.41.0->sentence_transformers) (2024.11.6)\n",
      "Requirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.10/dist-packages (from transformers<5.0.0,>=4.41.0->sentence_transformers) (0.21.1)\n",
      "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.10/dist-packages (from transformers<5.0.0,>=4.41.0->sentence_transformers) (0.5.3)\n",
      "Collecting joblib>=1.2.0 (from scikit-learn->sentence_transformers)\n",
      "  Downloading joblib-1.4.2-py3-none-any.whl.metadata (5.4 kB)\n",
      "Collecting threadpoolctl>=3.1.0 (from scikit-learn->sentence_transformers)\n",
      "  Downloading threadpoolctl-3.6.0-py3-none-any.whl.metadata (13 kB)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.11.0->sentence_transformers) (2.1.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.20.0->sentence_transformers) (2.1.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.20.0->sentence_transformers) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.20.0->sentence_transformers) (1.26.13)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.20.0->sentence_transformers) (2022.12.7)\n",
      "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=1.11.0->sentence_transformers) (1.3.0)\n",
      "Downloading sentence_transformers-4.0.1-py3-none-any.whl (340 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m340.6/340.6 kB\u001b[0m \u001b[31m27.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading typing_extensions-4.13.0-py3-none-any.whl (45 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m45.7/45.7 kB\u001b[0m \u001b[31m25.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading scikit_learn-1.6.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (13.5 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.5/13.5 MB\u001b[0m \u001b[31m232.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading scipy-1.15.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (37.6 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m37.6/37.6 MB\u001b[0m \u001b[31m158.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading joblib-1.4.2-py3-none-any.whl (301 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m301.8/301.8 kB\u001b[0m \u001b[31m122.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading threadpoolctl-3.6.0-py3-none-any.whl (18 kB)\n",
      "Installing collected packages: typing_extensions, threadpoolctl, scipy, joblib, scikit-learn, sentence_transformers\n",
      "  Attempting uninstall: typing_extensions\n",
      "    Found existing installation: typing_extensions 4.4.0\n",
      "    Uninstalling typing_extensions-4.4.0:\n",
      "      Successfully uninstalled typing_extensions-4.4.0\n",
      "Successfully installed joblib-1.4.2 scikit-learn-1.6.1 scipy-1.15.2 sentence_transformers-4.0.1 threadpoolctl-3.6.0 typing_extensions-4.13.0\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.3.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.0.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython -m pip install --upgrade pip\u001b[0m\n",
      "Collecting evaluate\n",
      "  Downloading evaluate-0.4.3-py3-none-any.whl.metadata (9.2 kB)\n",
      "Requirement already satisfied: datasets>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from evaluate) (3.5.0)\n",
      "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from evaluate) (1.24.1)\n",
      "Requirement already satisfied: dill in /usr/local/lib/python3.10/dist-packages (from evaluate) (0.3.8)\n",
      "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from evaluate) (2.2.3)\n",
      "Requirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.10/dist-packages (from evaluate) (2.32.3)\n",
      "Requirement already satisfied: tqdm>=4.62.1 in /usr/local/lib/python3.10/dist-packages (from evaluate) (4.67.1)\n",
      "Requirement already satisfied: xxhash in /usr/local/lib/python3.10/dist-packages (from evaluate) (3.5.0)\n",
      "Requirement already satisfied: multiprocess in /usr/local/lib/python3.10/dist-packages (from evaluate) (0.70.16)\n",
      "Requirement already satisfied: fsspec>=2021.05.0 in /usr/local/lib/python3.10/dist-packages (from fsspec[http]>=2021.05.0->evaluate) (2024.12.0)\n",
      "Requirement already satisfied: huggingface-hub>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from evaluate) (0.29.3)\n",
      "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from evaluate) (23.2)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from datasets>=2.0.0->evaluate) (3.9.0)\n",
      "Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.10/dist-packages (from datasets>=2.0.0->evaluate) (19.0.1)\n",
      "Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from datasets>=2.0.0->evaluate) (3.11.14)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from datasets>=2.0.0->evaluate) (6.0.1)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.7.0->evaluate) (4.13.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->evaluate) (2.1.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->evaluate) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->evaluate) (1.26.13)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->evaluate) (2022.12.7)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas->evaluate) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->evaluate) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.10/dist-packages (from pandas->evaluate) (2025.2)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (2.6.1)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (1.3.2)\n",
      "Requirement already satisfied: async-timeout<6.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (5.0.1)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (23.1.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (1.5.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (6.2.0)\n",
      "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (0.3.1)\n",
      "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (1.18.3)\n",
      "Requirement already satisfied: six>=1.5 in /usr/lib/python3/dist-packages (from python-dateutil>=2.8.2->pandas->evaluate) (1.16.0)\n",
      "Downloading evaluate-0.4.3-py3-none-any.whl (84 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m84.0/84.0 kB\u001b[0m \u001b[31m1.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: evaluate\n",
      "Successfully installed evaluate-0.4.3\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.3.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.0.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython -m pip install --upgrade pip\u001b[0m\n",
      "Collecting vllm==0.6.5\n",
      "  Downloading vllm-0.6.5-cp38-abi3-manylinux1_x86_64.whl.metadata (11 kB)\n",
      "Requirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (from vllm==0.6.5) (5.9.6)\n",
      "Collecting sentencepiece (from vllm==0.6.5)\n",
      "  Downloading sentencepiece-0.2.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (7.7 kB)\n",
      "Requirement already satisfied: numpy<2.0.0 in /usr/local/lib/python3.10/dist-packages (from vllm==0.6.5) (1.24.1)\n",
      "Requirement already satisfied: requests>=2.26.0 in /usr/local/lib/python3.10/dist-packages (from vllm==0.6.5) (2.32.3)\n",
      "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from vllm==0.6.5) (4.67.1)\n",
      "Collecting blake3 (from vllm==0.6.5)\n",
      "  Downloading blake3-1.0.4-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.2 kB)\n",
      "Collecting py-cpuinfo (from vllm==0.6.5)\n",
      "  Downloading py_cpuinfo-9.0.0-py3-none-any.whl.metadata (794 bytes)\n",
      "Requirement already satisfied: transformers>=4.45.2 in /usr/local/lib/python3.10/dist-packages (from vllm==0.6.5) (4.50.3)\n",
      "Requirement already satisfied: tokenizers>=0.19.1 in /usr/local/lib/python3.10/dist-packages (from vllm==0.6.5) (0.21.1)\n",
      "Collecting protobuf (from vllm==0.6.5)\n",
      "  Downloading protobuf-6.30.2-cp39-abi3-manylinux2014_x86_64.whl.metadata (593 bytes)\n",
      "Collecting fastapi!=0.113.*,!=0.114.0,>=0.107.0 (from vllm==0.6.5)\n",
      "  Downloading fastapi-0.115.12-py3-none-any.whl.metadata (27 kB)\n",
      "Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from vllm==0.6.5) (3.11.14)\n",
      "Collecting openai>=1.45.0 (from vllm==0.6.5)\n",
      "  Downloading openai-1.69.0-py3-none-any.whl.metadata (25 kB)\n",
      "Collecting uvicorn[standard] (from vllm==0.6.5)\n",
      "  Downloading uvicorn-0.34.0-py3-none-any.whl.metadata (6.5 kB)\n",
      "Collecting pydantic>=2.9 (from vllm==0.6.5)\n",
      "  Downloading pydantic-2.11.1-py3-none-any.whl.metadata (63 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m63.5/63.5 kB\u001b[0m \u001b[31m7.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: pillow in /usr/local/lib/python3.10/dist-packages (from vllm==0.6.5) (9.3.0)\n",
      "Requirement already satisfied: prometheus_client>=0.18.0 in /usr/local/lib/python3.10/dist-packages (from vllm==0.6.5) (0.18.0)\n",
      "Collecting prometheus-fastapi-instrumentator>=7.0.0 (from vllm==0.6.5)\n",
      "  Downloading prometheus_fastapi_instrumentator-7.1.0-py3-none-any.whl.metadata (13 kB)\n",
      "Collecting tiktoken>=0.6.0 (from vllm==0.6.5)\n",
      "  Downloading tiktoken-0.9.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.7 kB)\n",
      "Collecting lm-format-enforcer<0.11,>=0.10.9 (from vllm==0.6.5)\n",
      "  Downloading lm_format_enforcer-0.10.11-py3-none-any.whl.metadata (17 kB)\n",
      "Collecting outlines==0.1.11 (from vllm==0.6.5)\n",
      "  Downloading outlines-0.1.11-py3-none-any.whl.metadata (17 kB)\n",
      "Collecting xgrammar>=0.1.6 (from vllm==0.6.5)\n",
      "  Downloading xgrammar-0.1.17-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (3.5 kB)\n",
      "Requirement already satisfied: typing_extensions>=4.10 in /usr/local/lib/python3.10/dist-packages (from vllm==0.6.5) (4.13.0)\n",
      "Collecting filelock>=3.16.1 (from vllm==0.6.5)\n",
      "  Downloading filelock-3.18.0-py3-none-any.whl.metadata (2.9 kB)\n",
      "Collecting partial-json-parser (from vllm==0.6.5)\n",
      "  Downloading partial_json_parser-0.2.1.1.post5-py3-none-any.whl.metadata (6.1 kB)\n",
      "Requirement already satisfied: pyzmq in /usr/local/lib/python3.10/dist-packages (from vllm==0.6.5) (24.0.1)\n",
      "Collecting msgspec (from vllm==0.6.5)\n",
      "  Downloading msgspec-0.19.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.9 kB)\n",
      "Collecting gguf==0.10.0 (from vllm==0.6.5)\n",
      "  Downloading gguf-0.10.0-py3-none-any.whl.metadata (3.5 kB)\n",
      "Requirement already satisfied: importlib_metadata in /usr/lib/python3/dist-packages (from vllm==0.6.5) (4.6.4)\n",
      "Collecting mistral_common>=1.5.0 (from mistral_common[opencv]>=1.5.0->vllm==0.6.5)\n",
      "  Downloading mistral_common-1.5.4-py3-none-any.whl.metadata (4.5 kB)\n",
      "Requirement already satisfied: pyyaml in /usr/local/lib/python3.10/dist-packages (from vllm==0.6.5) (6.0.1)\n",
      "Collecting einops (from vllm==0.6.5)\n",
      "  Downloading einops-0.8.1-py3-none-any.whl.metadata (13 kB)\n",
      "Collecting compressed-tensors==0.8.1 (from vllm==0.6.5)\n",
      "  Downloading compressed_tensors-0.8.1-py3-none-any.whl.metadata (6.8 kB)\n",
      "Collecting depyf==0.18.0 (from vllm==0.6.5)\n",
      "  Downloading depyf-0.18.0-py3-none-any.whl.metadata (7.1 kB)\n",
      "Collecting ray>=2.9 (from vllm==0.6.5)\n",
      "  Downloading ray-2.44.1-cp310-cp310-manylinux2014_x86_64.whl.metadata (19 kB)\n",
      "Collecting nvidia-ml-py>=12.560.30 (from vllm==0.6.5)\n",
      "  Downloading nvidia_ml_py-12.570.86-py3-none-any.whl.metadata (8.7 kB)\n",
      "Collecting torch==2.5.1 (from vllm==0.6.5)\n",
      "  Downloading torch-2.5.1-cp310-cp310-manylinux1_x86_64.whl.metadata (28 kB)\n",
      "Collecting torchvision==0.20.1 (from vllm==0.6.5)\n",
      "  Downloading torchvision-0.20.1-cp310-cp310-manylinux1_x86_64.whl.metadata (6.1 kB)\n",
      "Collecting xformers==0.0.28.post3 (from vllm==0.6.5)\n",
      "  Downloading xformers-0.0.28.post3-cp310-cp310-manylinux_2_28_x86_64.whl.metadata (1.0 kB)\n",
      "Collecting astor (from depyf==0.18.0->vllm==0.6.5)\n",
      "  Downloading astor-0.8.1-py2.py3-none-any.whl.metadata (4.2 kB)\n",
      "Requirement already satisfied: dill in /usr/local/lib/python3.10/dist-packages (from depyf==0.18.0->vllm==0.6.5) (0.3.8)\n",
      "Collecting interegular (from outlines==0.1.11->vllm==0.6.5)\n",
      "  Downloading interegular-0.3.3-py37-none-any.whl.metadata (3.0 kB)\n",
      "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from outlines==0.1.11->vllm==0.6.5) (3.1.2)\n",
      "Collecting lark (from outlines==0.1.11->vllm==0.6.5)\n",
      "  Downloading lark-1.2.2-py3-none-any.whl.metadata (1.8 kB)\n",
      "Requirement already satisfied: nest_asyncio in /usr/local/lib/python3.10/dist-packages (from outlines==0.1.11->vllm==0.6.5) (1.5.8)\n",
      "Collecting cloudpickle (from outlines==0.1.11->vllm==0.6.5)\n",
      "  Downloading cloudpickle-3.1.1-py3-none-any.whl.metadata (7.1 kB)\n",
      "Collecting diskcache (from outlines==0.1.11->vllm==0.6.5)\n",
      "  Downloading diskcache-5.6.3-py3-none-any.whl.metadata (20 kB)\n",
      "Requirement already satisfied: referencing in /usr/local/lib/python3.10/dist-packages (from outlines==0.1.11->vllm==0.6.5) (0.30.2)\n",
      "Requirement already satisfied: jsonschema in /usr/local/lib/python3.10/dist-packages (from outlines==0.1.11->vllm==0.6.5) (4.19.2)\n",
      "Collecting pycountry (from outlines==0.1.11->vllm==0.6.5)\n",
      "  Downloading pycountry-24.6.1-py3-none-any.whl.metadata (12 kB)\n",
      "Collecting airportsdata (from outlines==0.1.11->vllm==0.6.5)\n",
      "  Downloading airportsdata-20250224-py3-none-any.whl.metadata (9.0 kB)\n",
      "Collecting outlines_core==0.1.26 (from outlines==0.1.11->vllm==0.6.5)\n",
      "  Downloading outlines_core-0.1.26-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (3.8 kB)\n",
      "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch==2.5.1->vllm==0.6.5) (3.0)\n",
      "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch==2.5.1->vllm==0.6.5) (2024.12.0)\n",
      "Collecting nvidia-cuda-nvrtc-cu12==12.4.127 (from torch==2.5.1->vllm==0.6.5)\n",
      "  Downloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
      "Collecting nvidia-cuda-runtime-cu12==12.4.127 (from torch==2.5.1->vllm==0.6.5)\n",
      "  Downloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
      "Collecting nvidia-cuda-cupti-cu12==12.4.127 (from torch==2.5.1->vllm==0.6.5)\n",
      "  Downloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
      "Collecting nvidia-cudnn-cu12==9.1.0.70 (from torch==2.5.1->vllm==0.6.5)\n",
      "  Downloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
      "Collecting nvidia-cublas-cu12==12.4.5.8 (from torch==2.5.1->vllm==0.6.5)\n",
      "  Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
      "Collecting nvidia-cufft-cu12==11.2.1.3 (from torch==2.5.1->vllm==0.6.5)\n",
      "  Downloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
      "Collecting nvidia-curand-cu12==10.3.5.147 (from torch==2.5.1->vllm==0.6.5)\n",
      "  Downloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
      "Collecting nvidia-cusolver-cu12==11.6.1.9 (from torch==2.5.1->vllm==0.6.5)\n",
      "  Downloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
      "Collecting nvidia-cusparse-cu12==12.3.1.170 (from torch==2.5.1->vllm==0.6.5)\n",
      "  Downloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
      "Collecting nvidia-nccl-cu12==2.21.5 (from torch==2.5.1->vllm==0.6.5)\n",
      "  Downloading nvidia_nccl_cu12-2.21.5-py3-none-manylinux2014_x86_64.whl.metadata (1.8 kB)\n",
      "Collecting nvidia-nvtx-cu12==12.4.127 (from torch==2.5.1->vllm==0.6.5)\n",
      "  Downloading nvidia_nvtx_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.7 kB)\n",
      "Collecting nvidia-nvjitlink-cu12==12.4.127 (from torch==2.5.1->vllm==0.6.5)\n",
      "  Downloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
      "Collecting triton==3.1.0 (from torch==2.5.1->vllm==0.6.5)\n",
      "  Downloading triton-3.1.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (1.3 kB)\n",
      "Collecting sympy==1.13.1 (from torch==2.5.1->vllm==0.6.5)\n",
      "  Downloading sympy-1.13.1-py3-none-any.whl.metadata (12 kB)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy==1.13.1->torch==2.5.1->vllm==0.6.5) (1.3.0)\n",
      "Collecting starlette<0.47.0,>=0.40.0 (from fastapi!=0.113.*,!=0.114.0,>=0.107.0->vllm==0.6.5)\n",
      "  Downloading starlette-0.46.1-py3-none-any.whl.metadata (6.2 kB)\n",
      "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from lm-format-enforcer<0.11,>=0.10.9->vllm==0.6.5) (23.2)\n",
      "Collecting jsonschema (from outlines==0.1.11->vllm==0.6.5)\n",
      "  Downloading jsonschema-4.23.0-py3-none-any.whl.metadata (7.9 kB)\n",
      "Collecting numpy<2.0.0 (from vllm==0.6.5)\n",
      "  Downloading numpy-1.26.4-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (61 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m61.0/61.0 kB\u001b[0m \u001b[31m9.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting pillow (from vllm==0.6.5)\n",
      "  Downloading pillow-11.1.0-cp310-cp310-manylinux_2_28_x86_64.whl.metadata (9.1 kB)\n",
      "Collecting opencv-python-headless>=4.0.0 (from mistral_common[opencv]>=1.5.0->vllm==0.6.5)\n",
      "  Downloading opencv_python_headless-4.11.0.86-cp37-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (20 kB)\n",
      "Requirement already satisfied: anyio<5,>=3.5.0 in /usr/local/lib/python3.10/dist-packages (from openai>=1.45.0->vllm==0.6.5) (4.0.0)\n",
      "Requirement already satisfied: distro<2,>=1.7.0 in /usr/lib/python3/dist-packages (from openai>=1.45.0->vllm==0.6.5) (1.7.0)\n",
      "Collecting httpx<1,>=0.23.0 (from openai>=1.45.0->vllm==0.6.5)\n",
      "  Downloading httpx-0.28.1-py3-none-any.whl.metadata (7.1 kB)\n",
      "Collecting jiter<1,>=0.4.0 (from openai>=1.45.0->vllm==0.6.5)\n",
      "  Downloading jiter-0.9.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (5.2 kB)\n",
      "Requirement already satisfied: sniffio in /usr/local/lib/python3.10/dist-packages (from openai>=1.45.0->vllm==0.6.5) (1.3.0)\n",
      "Collecting annotated-types>=0.6.0 (from pydantic>=2.9->vllm==0.6.5)\n",
      "  Downloading annotated_types-0.7.0-py3-none-any.whl.metadata (15 kB)\n",
      "Collecting pydantic-core==2.33.0 (from pydantic>=2.9->vllm==0.6.5)\n",
      "  Downloading pydantic_core-2.33.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.8 kB)\n",
      "Collecting typing-inspection>=0.4.0 (from pydantic>=2.9->vllm==0.6.5)\n",
      "  Downloading typing_inspection-0.4.0-py3-none-any.whl.metadata (2.6 kB)\n",
      "Collecting click>=7.0 (from ray>=2.9->vllm==0.6.5)\n",
      "  Downloading click-8.1.8-py3-none-any.whl.metadata (2.3 kB)\n",
      "Collecting msgpack<2.0.0,>=1.0.0 (from ray>=2.9->vllm==0.6.5)\n",
      "  Downloading msgpack-1.1.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (8.4 kB)\n",
      "Requirement already satisfied: aiosignal in /usr/local/lib/python3.10/dist-packages (from ray>=2.9->vllm==0.6.5) (1.3.2)\n",
      "Requirement already satisfied: frozenlist in /usr/local/lib/python3.10/dist-packages (from ray>=2.9->vllm==0.6.5) (1.5.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->vllm==0.6.5) (2.1.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->vllm==0.6.5) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->vllm==0.6.5) (1.26.13)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->vllm==0.6.5) (2022.12.7)\n",
      "Requirement already satisfied: regex>=2022.1.18 in /usr/local/lib/python3.10/dist-packages (from tiktoken>=0.6.0->vllm==0.6.5) (2024.11.6)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.16.4 in /usr/local/lib/python3.10/dist-packages (from tokenizers>=0.19.1->vllm==0.6.5) (0.29.3)\n",
      "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.10/dist-packages (from transformers>=4.45.2->vllm==0.6.5) (0.5.3)\n",
      "Collecting ninja (from xgrammar>=0.1.6->vllm==0.6.5)\n",
      "  Downloading ninja-1.11.1.4-py3-none-manylinux_2_12_x86_64.manylinux2010_x86_64.whl.metadata (5.0 kB)\n",
      "Collecting nanobind>=2.0.0 (from xgrammar>=0.1.6->vllm==0.6.5)\n",
      "  Downloading nanobind-2.6.1-py3-none-any.whl.metadata (1.8 kB)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->vllm==0.6.5) (2.6.1)\n",
      "Requirement already satisfied: async-timeout<6.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->vllm==0.6.5) (5.0.1)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->vllm==0.6.5) (23.1.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->vllm==0.6.5) (6.2.0)\n",
      "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->vllm==0.6.5) (0.3.1)\n",
      "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->vllm==0.6.5) (1.18.3)\n",
      "Collecting h11>=0.8 (from uvicorn[standard]->vllm==0.6.5)\n",
      "  Downloading h11-0.14.0-py3-none-any.whl.metadata (8.2 kB)\n",
      "Collecting httptools>=0.6.3 (from uvicorn[standard]->vllm==0.6.5)\n",
      "  Downloading httptools-0.6.4-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (3.6 kB)\n",
      "Collecting python-dotenv>=0.13 (from uvicorn[standard]->vllm==0.6.5)\n",
      "  Downloading python_dotenv-1.1.0-py3-none-any.whl.metadata (24 kB)\n",
      "Collecting uvloop!=0.15.0,!=0.15.1,>=0.14.0 (from uvicorn[standard]->vllm==0.6.5)\n",
      "  Downloading uvloop-0.21.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.9 kB)\n",
      "Collecting watchfiles>=0.13 (from uvicorn[standard]->vllm==0.6.5)\n",
      "  Downloading watchfiles-1.0.4-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.9 kB)\n",
      "Collecting websockets>=10.4 (from uvicorn[standard]->vllm==0.6.5)\n",
      "  Downloading websockets-15.0.1-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.8 kB)\n",
      "Requirement already satisfied: exceptiongroup>=1.0.2 in /usr/local/lib/python3.10/dist-packages (from anyio<5,>=3.5.0->openai>=1.45.0->vllm==0.6.5) (1.1.3)\n",
      "Collecting httpcore==1.* (from httpx<1,>=0.23.0->openai>=1.45.0->vllm==0.6.5)\n",
      "  Downloading httpcore-1.0.7-py3-none-any.whl.metadata (21 kB)\n",
      "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in /usr/local/lib/python3.10/dist-packages (from jsonschema->outlines==0.1.11->vllm==0.6.5) (2023.7.1)\n",
      "Requirement already satisfied: rpds-py>=0.7.1 in /usr/local/lib/python3.10/dist-packages (from jsonschema->outlines==0.1.11->vllm==0.6.5) (0.12.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->outlines==0.1.11->vllm==0.6.5) (2.1.2)\n",
      "Downloading vllm-0.6.5-cp38-abi3-manylinux1_x86_64.whl (201.1 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m201.1/201.1 MB\u001b[0m \u001b[31m42.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading compressed_tensors-0.8.1-py3-none-any.whl (87 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m87.5/87.5 kB\u001b[0m \u001b[31m44.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading depyf-0.18.0-py3-none-any.whl (38 kB)\n",
      "Downloading gguf-0.10.0-py3-none-any.whl (71 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m71.6/71.6 kB\u001b[0m \u001b[31m38.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading outlines-0.1.11-py3-none-any.whl (87 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m87.6/87.6 kB\u001b[0m \u001b[31m45.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading torch-2.5.1-cp310-cp310-manylinux1_x86_64.whl (906.4 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m906.4/906.4 MB\u001b[0m \u001b[31m15.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading torchvision-0.20.1-cp310-cp310-manylinux1_x86_64.whl (7.2 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.2/7.2 MB\u001b[0m \u001b[31m196.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading xformers-0.0.28.post3-cp310-cp310-manylinux_2_28_x86_64.whl (16.7 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m16.7/16.7 MB\u001b[0m \u001b[31m175.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl (363.4 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m33.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (13.8 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m187.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (24.6 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m196.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (883 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m194.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl (664.8 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m20.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl (211.5 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m54.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl (56.3 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m129.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl (127.9 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m79.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl (207.5 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m57.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_nccl_cu12-2.21.5-py3-none-manylinux2014_x86_64.whl (188.7 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m188.7/188.7 MB\u001b[0m \u001b[31m59.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (21.1 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m209.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_nvtx_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (99 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m99.1/99.1 kB\u001b[0m \u001b[31m51.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading outlines_core-0.1.26-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (343 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m343.6/343.6 kB\u001b[0m \u001b[31m124.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading sympy-1.13.1-py3-none-any.whl (6.2 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.2/6.2 MB\u001b[0m \u001b[31m247.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading triton-3.1.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (209.5 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m209.5/209.5 MB\u001b[0m \u001b[31m45.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading fastapi-0.115.12-py3-none-any.whl (95 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m95.2/95.2 kB\u001b[0m \u001b[31m46.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading filelock-3.18.0-py3-none-any.whl (16 kB)\n",
      "Downloading lm_format_enforcer-0.10.11-py3-none-any.whl (44 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m44.2/44.2 kB\u001b[0m \u001b[31m24.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading mistral_common-1.5.4-py3-none-any.whl (6.5 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.5/6.5 MB\u001b[0m \u001b[31m173.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading numpy-1.26.4-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (18.2 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m18.2/18.2 MB\u001b[0m \u001b[31m211.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_ml_py-12.570.86-py3-none-any.whl (44 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m44.4/44.4 kB\u001b[0m \u001b[31m24.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading openai-1.69.0-py3-none-any.whl (599 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m599.1/599.1 kB\u001b[0m \u001b[31m173.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading pillow-11.1.0-cp310-cp310-manylinux_2_28_x86_64.whl (4.5 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.5/4.5 MB\u001b[0m \u001b[31m252.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading prometheus_fastapi_instrumentator-7.1.0-py3-none-any.whl (19 kB)\n",
      "Downloading pydantic-2.11.1-py3-none-any.whl (442 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m442.6/442.6 kB\u001b[0m \u001b[31m153.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading pydantic_core-2.33.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.0 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.0/2.0 MB\u001b[0m \u001b[31m240.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading ray-2.44.1-cp310-cp310-manylinux2014_x86_64.whl (67.9 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m67.9/67.9 MB\u001b[0m \u001b[31m63.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading protobuf-6.30.2-cp39-abi3-manylinux2014_x86_64.whl (316 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m316.2/316.2 kB\u001b[0m \u001b[31m119.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading sentencepiece-0.2.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m213.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading tiktoken-0.9.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.2 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m206.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading xgrammar-0.1.17-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (4.3 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.3/4.3 MB\u001b[0m \u001b[31m197.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading blake3-1.0.4-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (376 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m376.4/376.4 kB\u001b[0m \u001b[31m136.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading einops-0.8.1-py3-none-any.whl (64 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m64.4/64.4 kB\u001b[0m \u001b[31m36.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading msgspec-0.19.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (211 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.6/211.6 kB\u001b[0m \u001b[31m95.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading partial_json_parser-0.2.1.1.post5-py3-none-any.whl (10 kB)\n",
      "Downloading py_cpuinfo-9.0.0-py3-none-any.whl (22 kB)\n",
      "Downloading annotated_types-0.7.0-py3-none-any.whl (13 kB)\n",
      "Downloading click-8.1.8-py3-none-any.whl (98 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m98.2/98.2 kB\u001b[0m \u001b[31m53.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading h11-0.14.0-py3-none-any.whl (58 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.3/58.3 kB\u001b[0m \u001b[31m32.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading httptools-0.6.4-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (442 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m442.1/442.1 kB\u001b[0m \u001b[31m150.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading httpx-0.28.1-py3-none-any.whl (73 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m73.5/73.5 kB\u001b[0m \u001b[31m40.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading httpcore-1.0.7-py3-none-any.whl (78 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m78.6/78.6 kB\u001b[0m \u001b[31m43.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading interegular-0.3.3-py37-none-any.whl (23 kB)\n",
      "Downloading jiter-0.9.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (352 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m352.9/352.9 kB\u001b[0m \u001b[31m114.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading jsonschema-4.23.0-py3-none-any.whl (88 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m88.5/88.5 kB\u001b[0m \u001b[31m48.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading msgpack-1.1.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (378 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m378.0/378.0 kB\u001b[0m \u001b[31m134.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading nanobind-2.6.1-py3-none-any.whl (238 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m239.0/239.0 kB\u001b[0m \u001b[31m104.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading opencv_python_headless-4.11.0.86-cp37-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (50.0 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m50.0/50.0 MB\u001b[0m \u001b[31m82.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading python_dotenv-1.1.0-py3-none-any.whl (20 kB)\n",
      "Downloading starlette-0.46.1-py3-none-any.whl (71 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m72.0/72.0 kB\u001b[0m \u001b[31m33.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading typing_inspection-0.4.0-py3-none-any.whl (14 kB)\n",
      "Downloading uvloop-0.21.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.8 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.8/3.8 MB\u001b[0m \u001b[31m148.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading watchfiles-1.0.4-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (452 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m452.9/452.9 kB\u001b[0m \u001b[31m141.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading websockets-15.0.1-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (181 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m181.6/181.6 kB\u001b[0m \u001b[31m79.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading airportsdata-20250224-py3-none-any.whl (913 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m913.7/913.7 kB\u001b[0m \u001b[31m166.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading astor-0.8.1-py2.py3-none-any.whl (27 kB)\n",
      "Downloading cloudpickle-3.1.1-py3-none-any.whl (20 kB)\n",
      "Downloading diskcache-5.6.3-py3-none-any.whl (45 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m45.5/45.5 kB\u001b[0m \u001b[31m18.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading lark-1.2.2-py3-none-any.whl (111 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m111.0/111.0 kB\u001b[0m \u001b[31m22.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading ninja-1.11.1.4-py3-none-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (422 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m422.8/422.8 kB\u001b[0m \u001b[31m114.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading pycountry-24.6.1-py3-none-any.whl (6.3 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.3/6.3 MB\u001b[0m \u001b[31m246.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading uvicorn-0.34.0-py3-none-any.whl (62 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.3/62.3 kB\u001b[0m \u001b[31m23.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: sentencepiece, py-cpuinfo, nvidia-ml-py, nanobind, blake3, websockets, uvloop, typing-inspection, sympy, python-dotenv, pydantic-core, pycountry, protobuf, pillow, partial-json-parser, nvidia-nvtx-cu12, nvidia-nvjitlink-cu12, nvidia-nccl-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, numpy, ninja, msgspec, msgpack, lark, jiter, interegular, httptools, h11, filelock, einops, diskcache, cloudpickle, click, astor, annotated-types, airportsdata, watchfiles, uvicorn, triton, tiktoken, starlette, pydantic, opencv-python-headless, nvidia-cusparse-cu12, nvidia-cudnn-cu12, httpcore, gguf, depyf, prometheus-fastapi-instrumentator, nvidia-cusolver-cu12, lm-format-enforcer, jsonschema, httpx, fastapi, torch, ray, outlines_core, openai, mistral_common, xgrammar, xformers, torchvision, outlines, compressed-tensors, vllm\n",
      "  Attempting uninstall: sympy\n",
      "    Found existing installation: sympy 1.12\n",
      "    Uninstalling sympy-1.12:\n",
      "      Successfully uninstalled sympy-1.12\n",
      "  Attempting uninstall: pillow\n",
      "    Found existing installation: Pillow 9.3.0\n",
      "    Uninstalling Pillow-9.3.0:\n",
      "      Successfully uninstalled Pillow-9.3.0\n",
      "  Attempting uninstall: numpy\n",
      "    Found existing installation: numpy 1.24.1\n",
      "    Uninstalling numpy-1.24.1:\n",
      "      Successfully uninstalled numpy-1.24.1\n",
      "  Attempting uninstall: filelock\n",
      "    Found existing installation: filelock 3.9.0\n",
      "    Uninstalling filelock-3.9.0:\n",
      "      Successfully uninstalled filelock-3.9.0\n",
      "  Attempting uninstall: triton\n",
      "    Found existing installation: triton 2.1.0\n",
      "    Uninstalling triton-2.1.0:\n",
      "      Successfully uninstalled triton-2.1.0\n",
      "  Attempting uninstall: jsonschema\n",
      "    Found existing installation: jsonschema 4.19.2\n",
      "    Uninstalling jsonschema-4.19.2:\n",
      "      Successfully uninstalled jsonschema-4.19.2\n",
      "  Attempting uninstall: torch\n",
      "    Found existing installation: torch 2.1.0+cu118\n",
      "    Uninstalling torch-2.1.0+cu118:\n",
      "      Successfully uninstalled torch-2.1.0+cu118\n",
      "  Attempting uninstall: torchvision\n",
      "    Found existing installation: torchvision 0.16.0+cu118\n",
      "    Uninstalling torchvision-0.16.0+cu118:\n",
      "      Successfully uninstalled torchvision-0.16.0+cu118\n",
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "torchaudio 2.1.0+cu118 requires torch==2.1.0, but you have torch 2.5.1 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0mSuccessfully installed airportsdata-20250224 annotated-types-0.7.0 astor-0.8.1 blake3-1.0.4 click-8.1.8 cloudpickle-3.1.1 compressed-tensors-0.8.1 depyf-0.18.0 diskcache-5.6.3 einops-0.8.1 fastapi-0.115.12 filelock-3.18.0 gguf-0.10.0 h11-0.14.0 httpcore-1.0.7 httptools-0.6.4 httpx-0.28.1 interegular-0.3.3 jiter-0.9.0 jsonschema-4.23.0 lark-1.2.2 lm-format-enforcer-0.10.11 mistral_common-1.5.4 msgpack-1.1.0 msgspec-0.19.0 nanobind-2.6.1 ninja-1.11.1.4 numpy-1.26.4 nvidia-cublas-cu12-12.4.5.8 nvidia-cuda-cupti-cu12-12.4.127 nvidia-cuda-nvrtc-cu12-12.4.127 nvidia-cuda-runtime-cu12-12.4.127 nvidia-cudnn-cu12-9.1.0.70 nvidia-cufft-cu12-11.2.1.3 nvidia-curand-cu12-10.3.5.147 nvidia-cusolver-cu12-11.6.1.9 nvidia-cusparse-cu12-12.3.1.170 nvidia-ml-py-12.570.86 nvidia-nccl-cu12-2.21.5 nvidia-nvjitlink-cu12-12.4.127 nvidia-nvtx-cu12-12.4.127 openai-1.69.0 opencv-python-headless-4.11.0.86 outlines-0.1.11 outlines_core-0.1.26 partial-json-parser-0.2.1.1.post5 pillow-11.1.0 prometheus-fastapi-instrumentator-7.1.0 protobuf-6.30.2 py-cpuinfo-9.0.0 pycountry-24.6.1 pydantic-2.11.1 pydantic-core-2.33.0 python-dotenv-1.1.0 ray-2.44.1 sentencepiece-0.2.0 starlette-0.46.1 sympy-1.13.1 tiktoken-0.9.0 torch-2.5.1 torchvision-0.20.1 triton-3.1.0 typing-inspection-0.4.0 uvicorn-0.34.0 uvloop-0.21.0 vllm-0.6.5 watchfiles-1.0.4 websockets-15.0.1 xformers-0.0.28.post3 xgrammar-0.1.17\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.3.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.0.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython -m pip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install datasets\n",
    "!pip install transformers\n",
    "!pip install accelerate\n",
    "!pip install numpy, pandas, polars\n",
    "!pip install matplotlib\n",
    "!pip install sentence_transformers\n",
    "#!pip install huggingface_hub\n",
    "!pip install evaluate\n",
    "#!pip install mistralai\n",
    "#!pip install flask\n",
    "\n",
    "# For vLLM\n",
    "!pip install -U vllm==0.6.5\n",
    "#!pip install ray\n",
    "#!pip install packaging\n",
    "#!pip install typing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64badf32-effe-45a4-aeaf-37898e63f23a",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# Process class"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bbdbdd0-5729-477f-8470-7ccdb26e172f",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## vLLM.AsyncEngine + asyncio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe2593c9-936a-486a-8afc-9d8c457e1e6e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Use Model from local environment\n",
    "\n",
    "import asyncio\n",
    "from transformers import AutoTokenizer\n",
    "from vllm import AsyncLLMEngine, AsyncEngineArgs, SamplingParams\n",
    "import time, warnings\n",
    "\n",
    "model_name = \"deepseek_r1_qwen14b\"\n",
    "tensor_parallel_size = 2\n",
    "\n",
    "engine_args = AsyncEngineArgs(\n",
    "    model = model_name,\n",
    "    tensor_parallel_size = tensor_parallel_size,\n",
    "    gpu_memory_utilization=0.95,\n",
    ")\n",
    "engine = AsyncLLMEngine.from_engine_args(engine_args)\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name, padding_side='left')\n",
    "\n",
    "\n",
    "class AllRequests:\n",
    "    \n",
    "    def __init__(self, max_request):\n",
    "        self.max_request = max_request\n",
    "        self.requests = []\n",
    "        self.request_ids = []\n",
    "        self.request_id = 0\n",
    "        self.results = []\n",
    "        self.finished_ids = []\n",
    "        \n",
    "    def add(self, request):\n",
    "        self.requests.append(request)\n",
    "        self.request_ids.append(self.request_id)\n",
    "        self.request_id += 1\n",
    "    \n",
    "    async def process(self, model=model_name, max_tokens = 3000, temperature=0.4, save_dir = \"progress_log\", restart = False):\n",
    "\n",
    "        if not os.path.exists(save_dir):\n",
    "            os.makedirs(save_dir)\n",
    "\n",
    "        if restart:\n",
    "            if os.path.exists(f\"{save_dir}/finished_ids.json\") and os.path.exists(f\"{save_dir}/results.json\"):\n",
    "                with open(f\"{save_dir}/finished_ids.json\") as f:\n",
    "                    finished_ids = json.load(f)\n",
    "                with open(f\"{save_dir}/results.json\") as f:\n",
    "                    self.results = json.load(f)\n",
    "                for finished_id in finished_ids:\n",
    "                    self.finished_ids.append(finished_id)\n",
    "                    id = self.request_ids.index(finished_id)\n",
    "                    self.request_ids.pop(id)\n",
    "                    self.requests.pop(id)\n",
    "\n",
    "        await asyncio.gather(\n",
    "            *[self.process_requests(temperature = temperature, max_tokens = max_tokens, restart = restart, save_dir=save_dir) for _ in range(self.max_request)]\n",
    "        )\n",
    "            \n",
    "        return self.results\n",
    "\n",
    "\n",
    "    async def process_requests(self, max_tokens = 3000, temperature=0.4, save_dir = \"progress_log\", restart = False):\n",
    "\n",
    "        while len(self.requests) != 0:\n",
    "            request_dict = self.requests.pop(0)\n",
    "            request_id = self.request_ids.pop(0)\n",
    "\n",
    "            prompt = request_dict[\"prompt\"]\n",
    "\n",
    "            final_output = None\n",
    "            results_generator = engine.generate(prompt, SamplingParams(temperature=temperature, max_tokens=max_tokens), request_id)\n",
    "            async for request_output in results_generator:\n",
    "                # print(request_output) => for streaming\n",
    "                final_output = request_output\n",
    "\n",
    "            output = final_output.outputs[0].text\n",
    "            \n",
    "            request_dict[\"output\"] = output\n",
    "            self.results.append(request_dict)\n",
    "            self.finished_ids.append(request_id)\n",
    "\n",
    "            with open(f\"{save_dir}/results.json\", \"w\") as f:\n",
    "                json.dump(self.results, f)\n",
    "            with open(f\"{save_dir}/finished_ids.json\", \"w\") as f:\n",
    "                json.dump(self.finished_ids, f)\n",
    "    \n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62f73bfe-471f-4ffe-aeb4-6776ba70983f",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Mistral API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc8cbf2b-2495-4c68-ae10-0e91bf647900",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initial Setting for Mistral API call\n",
    "# Attach your API key and Run the following script. Be sure to delete the API key for the case someone else see this script.\n",
    "\n",
    "import os\n",
    "from mistralai import Mistral\n",
    "\n",
    "os.environ[\"MISTRAL_API_KEY\"] = \"ACAGbDxsXG5K8kTaGGGPJ4Lj1gJ0aYdl\"\n",
    "\n",
    "api_key = os.environ[\"MISTRAL_API_KEY\"]\n",
    "#mistral_model = \"mistral-large-latest\"\n",
    "mistral_model = \"ministral-8b-latest\"\n",
    "client = Mistral(api_key=api_key)\n",
    "\n",
    "async def process_api_requests(requests, request_id, max_tokens = 2000, temperature=0.4, save_dir = \"api_progress_log\", restart = False, get_result = None, delete_save_file = True):\n",
    "    # requests: [{\"prompt\":, ...} ... ]\n",
    "\n",
    "    results = []\n",
    "    \n",
    "    save_path = f\"./{save_dir}/{request_id}.json\"\n",
    "    if os.path.exists(save_dir):\n",
    "        if os.path.exists(save_path):\n",
    "            if restart:\n",
    "                with open(save_path) as f:\n",
    "                    log_dict = json.load(f)\n",
    "                results = log_dict[\"results\"]\n",
    "            else:\n",
    "                os.remove(save_path)\n",
    "    else:\n",
    "        os.makedirs(save_dir)\n",
    "    \n",
    "    print()\n",
    "    print(f\"request {request_id} started\")\n",
    "\n",
    "    for i, request_dict in enumerate(requests):\n",
    "\n",
    "        # For the case restart = True\n",
    "        if len(results) > i:\n",
    "            continue\n",
    "        \n",
    "        prompt = request_dict[\"prompt\"]\n",
    "\n",
    "        start = time.time()\n",
    "        \n",
    "        # start the generation\n",
    "        chat_response = client.chat.complete(\n",
    "            model= mistral_model,\n",
    "            messages = [\n",
    "                {\n",
    "                    \"role\": \"user\",\n",
    "                    \"content\": prompt,\n",
    "                },\n",
    "            ]\n",
    "        )\n",
    "    \n",
    "        output = chat_response.choices[0].message.content\n",
    "\n",
    "        end = time.time()\n",
    "\n",
    "        request_dict[\"output\"] = output\n",
    "\n",
    "        if get_result:\n",
    "            result = get_result(request_dict, save_dir)\n",
    "        else:\n",
    "            result = request_dict\n",
    "            \n",
    "        results.append(result)\n",
    "    \n",
    "        print()\n",
    "        print(f\"request_id: {request_id}, {i+1}th request finished\")\n",
    "        print(f\"num token  prompt:{get_num_tokens(prompt)}, output:{get_num_tokens(output)}\")\n",
    "        print(f\"calculation time: {end-start}\")\n",
    "        #print(f\"output text: {output}\")\n",
    "        print()\n",
    "\n",
    "        with open(save_path, \"w\") as f:\n",
    "            json.dump({\"request_id\":request_id, \"requests\":requests, \"results\":results}, f)\n",
    "\n",
    "    print()\n",
    "    print(f\"request {request_id} all finished\")\n",
    "\n",
    "    if delete_save_file:\n",
    "        if os.path.exists(save_path):\n",
    "            os.remove(save_path)\n",
    "            print(f\"log file for request {request_id} is deleted\")\n",
    "\n",
    "    # results: [{\"prompt\":, \"output\":} ... ]\n",
    "    return results\n",
    "\n",
    "def get_num_tokens(text, add_special_tokens=False):\n",
    "    input_ids = tokenizer.encode(text, add_special_tokens=add_special_tokens)\n",
    "    return len(input_ids)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d59bfdf-078f-46c9-bb9b-180a18667076",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## vllm.entrypoints.LLM + release memory after process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4899da1-a7e1-4e65-a5f3-bb453a2c720f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use Model from local environment\n",
    "\n",
    "import asyncio\n",
    "from transformers import AutoTokenizer\n",
    "from vllm import LLM, SamplingParams\n",
    "import time, warnings, copy\n",
    "\n",
    "import gc, torch, contextlib\n",
    "from vllm.distributed.parallel_state import destroy_model_parallel, destroy_distributed_environment\n",
    "\n",
    "#model_name = \"Qwen/QwQ-32B-Preview\"\n",
    "#model_name = \"MBMMurad/QwQ-32B-preview-AWQ-AIMO-earlysharing\"\n",
    "#model_name = \"mistralai/Ministral-8B-Instruct-2410\"\n",
    "#model_name = \"deepseek-ai/deepseek-math-7b-instruct\"\n",
    "#model_name = \"Qwen/Qwen2.5-72B-Instruct-AWQ\"\n",
    "#model_name = \"bartowski/Qwen2.5-Math-72B-Instruct-GGUF\" *\n",
    "\n",
    "#qwq_model = \"./qwq_awq_model\"\n",
    "#mistral_model = \"./mistral8b_model\"\n",
    "default_model_name = \"./mistral8b_model\"\n",
    "tensor_parallel_size = 4\n",
    "\n",
    "class AllRequests:\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.requests = []\n",
    "        \n",
    "    def add(self, request):\n",
    "        self.requests.append(request)\n",
    "\n",
    "    def process(self, model=default_model_name, max_tokens = 3000, temperature=0.4, save_dir = \"progress_log\", restart = False, get_result = None, delete_save_file = False, destroy_model = True):\n",
    "        \n",
    "        results=[]\n",
    "        save_path = f\"{save_dir}/{0}.json\"\n",
    "        if os.path.exists(save_dir):\n",
    "            if os.path.exists(save_path):\n",
    "                if restart:\n",
    "                    with open(save_path) as f:\n",
    "                        log_dict = json.load(f)\n",
    "                    results = log_dict[\"results\"]\n",
    "                else:\n",
    "                    os.remove(save_path)\n",
    "        else:\n",
    "            os.makedirs(save_dir)\n",
    "\n",
    "        prompts = []\n",
    "    \n",
    "        if restart:\n",
    "            if len(results) == len(self.requests):\n",
    "                return results\n",
    "            processed_requests = self.requests[len(results):]\n",
    "        else:\n",
    "            processed_requests = self.requests\n",
    "    \n",
    "        for request_dict in processed_requests:\n",
    "            prompts.append(request_dict[\"prompt\"])\n",
    "\n",
    "\n",
    "        llm = LLM(model=model, tensor_parallel_size=tensor_parallel_size, distributed_executor_backend=\"mp\", disable_custom_all_reduce=True, trust_remote_code=True, enforce_eager=True)\n",
    "        sampling_params = SamplingParams(max_tokens=max_tokens, temperature=temperature, top_p=0.95)\n",
    "        \n",
    "        outputs = llm.generate(prompts, sampling_params)\n",
    "        \n",
    "        for i, output in enumerate(outputs):\n",
    "            prompt = output.prompt\n",
    "            generated_text = output.outputs[0].text\n",
    "            result_dict = processed_requests[i]\n",
    "            \n",
    "            result_dict[\"output\"] = generated_text\n",
    "    \n",
    "            if get_result:\n",
    "                result_dict = get_result(result_dict, save_dir)\n",
    "    \n",
    "            result = copy.deepcopy(result_dict) # without this, all results become same\n",
    "            results.append(result)\n",
    "\n",
    "        with open(save_path, \"w\") as f:\n",
    "            json.dump({\"requests\":self.requests, \"results\":results}, f)\n",
    "\n",
    "        if destroy_model:\n",
    "            destroy_model_parallel()\n",
    "            destroy_distributed_environment()\n",
    "            del llm.llm_engine.model_executor\n",
    "            del llm\n",
    "            gc.collect()\n",
    "            torch.cuda.empty_cache()\n",
    "            with contextlib.suppress(AssertionError):\n",
    "                torch.distributed.destroy_process_group()\n",
    "            \n",
    "        return results\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e5b141e-8a1c-43fb-abf9-dbe32ae9fb75",
   "metadata": {},
   "source": [
    "### old"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20dd9f47-6358-4eb6-b605-6f533faead8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use Model from local dir using vllm.entrypoints.LLM class releasing memory for model at last\n",
    "\n",
    "import asyncio\n",
    "from transformers import AutoTokenizer\n",
    "from vllm import LLM, AsyncLLMEngine, AsyncEngineArgs, SamplingParams\n",
    "import time, warnings, copy\n",
    "import torch\n",
    "\n",
    "#from huggingface_hub import login\n",
    "#login(token=\"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\")\n",
    "\n",
    "#default_model_name = \"Qwen/QwQ-32B-Preview\"\n",
    "#default_model_name = \"MBMMurad/QwQ-32B-preview-AWQ-AIMO-earlysharing\"\n",
    "#default_model_name = \"mistralai/Ministral-8B-Instruct-2410\"\n",
    "#default_model_name = \"deepseek-ai/deepseek-math-7b-instruct\"\n",
    "#default_model_name = \"Qwen/Qwen2.5-72B-Instruct-AWQ\"\n",
    "#default_model_name = \"bartowski/Qwen2.5-Math-72B-Instruct-GGUF\" *\n",
    "\n",
    "#default_model_name = \"./qwq_awq_model\"\n",
    "default_model_name = \"./mistral8b_model\"\n",
    "\n",
    "import gc, torch, contextlib\n",
    "from vllm.distributed.parallel_state import destroy_model_parallel, destroy_distributed_environment\n",
    "\n",
    "\n",
    "def process_requests(requests, request_id, model=default_model_name, max_tokens = 3000, temperature=0.4, save_dir = \"progress_log\", restart = False, get_result = None, delete_save_file = False):\n",
    "\n",
    "    results = []\n",
    "    \n",
    "    save_path = f\"./{save_dir}/{request_id}.json\"\n",
    "    if os.path.exists(save_dir):\n",
    "        if os.path.exists(save_path):\n",
    "            if restart:\n",
    "                with open(save_path) as f:\n",
    "                    log_dict = json.load(f)\n",
    "                results = log_dict[\"results\"]\n",
    "            else:\n",
    "                os.remove(save_path)\n",
    "    else:\n",
    "        os.makedirs(save_dir)\n",
    "\n",
    "    prompts = []\n",
    "    \n",
    "    if restart:\n",
    "        if len(results) == len(requests):\n",
    "            return results\n",
    "        processed_requests = requests[len(results):]\n",
    "    else:\n",
    "        processed_requests = requests\n",
    "\n",
    "    for request_dict in processed_requests:\n",
    "        prompts.append(request_dict[\"prompt\"])\n",
    "    \n",
    "    llm = LLM(model=model, tensor_parallel_size=2, distributed_executor_backend=\"mp\", disable_custom_all_reduce=True,)\n",
    "    sampling_params = SamplingParams(max_tokens=max_tokens, temperature=temperature, top_p=0.95)\n",
    "    \n",
    "    outputs = llm.generate(prompts, sampling_params)\n",
    "    \n",
    "    for i, output in enumerate(outputs):\n",
    "        prompt = output.prompt\n",
    "        generated_text = output.outputs[0].text\n",
    "        result_dict = processed_requests[i]\n",
    "        \n",
    "        result_dict[\"output\"] = generated_text\n",
    "\n",
    "        if get_result:\n",
    "            result_dict = get_result(result_dict, save_dir)\n",
    "\n",
    "        result = copy.deepcopy(result_dict) # without this, all results become same\n",
    "        results.append(result)\n",
    "\n",
    "    with open(save_path, \"w\") as f:\n",
    "        json.dump({\"request_id\":request_id, \"requests\":requests, \"results\":results}, f)\n",
    "\n",
    "    if delete_save_file:\n",
    "        if os.path.exists(save_path):\n",
    "            os.remove(save_path)\n",
    "            print(f\"log file for request {request_id} is deleted\")\n",
    "\n",
    "    destroy_model_parallel()\n",
    "    destroy_distributed_environment()\n",
    "    del llm.llm_engine.model_executor  #.driver_worker\n",
    "    del llm # Isn't necessary for releasing memory, but why not\n",
    "    gc.collect()\n",
    "    torch.cuda.empty_cache()\n",
    "    with contextlib.suppress(AssertionError):\n",
    "        torch.distributed.destroy_process_group()\n",
    "\n",
    "    print(f\"{torch.cuda.memory_allocated() / 1000000000} GB allocated\")\n",
    "\n",
    "    # results: [{\"prompt\":, \"output\":} ... ]\n",
    "    return results\n",
    "\n",
    "\n",
    "# Not used now. This is useful when someone wants requests in progress\n",
    "def get_all_requests(dir = \"./progress_log\", delete_files = True):\n",
    "    all_requests = {}\n",
    "    file_names = get_all_file_names(dir)\n",
    "    for file_name in file_names:\n",
    "        with open(f\"{dir}/{file_name}\") as f:\n",
    "            log_dict = json.load(f)\n",
    "        request_id = log_dict[\"request_id\"]\n",
    "        requests = log_dict[\"requests\"]\n",
    "        all_requests[str(request_id)] = requests\n",
    "\n",
    "    if delete_files:\n",
    "        delete_all_files_in_directory(dir)\n",
    "\n",
    "    return all_requests\n",
    "\n",
    "def get_num_tokens(text, add_special_tokens=False):\n",
    "    input_ids = tokenizer.encode(text, add_special_tokens=add_special_tokens)\n",
    "    return len(input_ids)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "689e739f-4236-4b0d-8287-3c40c0592e92",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# Collect Problems (problems, correct_answers, solutions)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13fbe3e6-e188-45f7-8f3e-2eb9e9bc2dc7",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## LiveCodeBench"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2646bc80-e25a-4f87-b95c-22fcae447a4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "lcb_codegen = load_dataset(\"livecodebench/code_generation_lite\", version_tag=\"release_v5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43ff750f-3529-4141-989a-6a86a30753e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "lcb_codegen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1bf872c-e13a-4caa-9ef8-ab48e6181ed8",
   "metadata": {},
   "outputs": [],
   "source": [
    "lcb_codegen[\"test\"]['question_content'][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4b829dd-9455-4c24-ba63-961d2d53299e",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## BigCodeBench"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58ade73d-5ffa-4222-bdfe-8bb9f0d4b4c3",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### Download"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9aaa34bd-cc1c-4e81-93dc-efe9c48f3365",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "num_sample = 100\n",
    "dataset_name = 'bigcode/bigcodebench'\n",
    "save_path = \"bigcodebench3.json\"\n",
    "\n",
    "import pandas as pd\n",
    "from datasets import load_dataset\n",
    "import random\n",
    "\n",
    "# Load a dataset from Hugging Face\n",
    "dataset = load_dataset(dataset_name)\n",
    "\n",
    "# Convert the dataset to a pandas DataFrame\n",
    "# Assuming you want to use the 'train' split of the dataset\n",
    "df = pd.DataFrame(dataset['v0.1.0_hf'])\n",
    "\n",
    "# Convert the DataFrame to a list of dictionaries\n",
    "data_list = df.to_dict(orient='records')\n",
    "#data_list = data_list[:num_sample]\n",
    "data_list = random.sample(data_list, num_sample)\n",
    "\n",
    "# Print the first few records to verify\n",
    "#print(data_list[:5])\n",
    "\n",
    "# Prepare list\n",
    "task_id = []\n",
    "complete_prompt = []\n",
    "instruct_prompt = []\n",
    "canonical_solution = []\n",
    "code_prompt = []\n",
    "test = []\n",
    "doc_struct = []\n",
    "\n",
    "for i, data_dict in enumerate(data_list):\n",
    "    task_id.append(data_dict[\"task_id\"])\n",
    "    complete_prompt.append(data_dict[\"complete_prompt\"])\n",
    "    instruct_prompt.append(data_dict[\"instruct_prompt\"])\n",
    "    canonical_solution.append(data_dict[\"canonical_solution\"])\n",
    "    code_prompt.append(data_dict[\"code_prompt\"])\n",
    "    test.append(data_dict[\"test\"])\n",
    "    doc_struct.append(data_dict[\"doc_struct\"])\n",
    "\n",
    "import json\n",
    "with open(save_path, \"w\") as f:\n",
    "    json.dump({\"task_id\":task_id, \"complete_prompt\":complete_prompt, \"instruct_prompt\":instruct_prompt, \"canonical_solution\":canonical_solution, \"code_prompt\":code_prompt, \"test\":test, \"doc_struct\":doc_struct,}, f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "433af124-f677-4b9e-bbe5-fcaf6e9c8f4a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#num_sample = 2000\n",
    "dataset_name = 'bigcode/bigcodebench'\n",
    "save_path = \"bigcodebench3.json\"\n",
    "\n",
    "import pandas as pd\n",
    "from datasets import load_dataset\n",
    "import random\n",
    "\n",
    "# Load a dataset from Hugging Face\n",
    "dataset = load_dataset(dataset_name)\n",
    "\n",
    "# Convert the dataset to a pandas DataFrame\n",
    "# Assuming you want to use the 'train' split of the dataset\n",
    "df = pd.DataFrame(dataset['v0.1.3'])\n",
    "\n",
    "# Convert the DataFrame to a list of dictionaries\n",
    "data_list = df.to_dict(orient='records')\n",
    "#data_list = data_list[:num_sample]\n",
    "#data_list = random.sample(data_list, num_sample)\n",
    "\n",
    "# Print the first few records to verify\n",
    "#print(data_list[:5])\n",
    "\n",
    "# Prepare list\n",
    "task_id = []\n",
    "complete_prompt = []\n",
    "instruct_prompt = []\n",
    "canonical_solution = []\n",
    "code_prompt = []\n",
    "test = []\n",
    "doc_struct = []\n",
    "\n",
    "for i, data_dict in enumerate(data_list):\n",
    "    task_id.append(data_dict[\"task_id\"])\n",
    "    complete_prompt.append(data_dict[\"complete_prompt\"])\n",
    "    instruct_prompt.append(data_dict[\"instruct_prompt\"])\n",
    "    canonical_solution.append(data_dict[\"canonical_solution\"])\n",
    "    code_prompt.append(data_dict[\"code_prompt\"])\n",
    "    test.append(data_dict[\"test\"])\n",
    "    doc_struct.append(data_dict[\"doc_struct\"])\n",
    "\n",
    "import json\n",
    "with open(save_path, \"w\") as f:\n",
    "    json.dump({\"task_id\":task_id, \"complete_prompt\":complete_prompt, \"instruct_prompt\":instruct_prompt, \"canonical_solution\":canonical_solution, \"code_prompt\":code_prompt, \"test\":test, \"doc_struct\":doc_struct,}, f)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26e835b4-3dae-45ca-b164-b60c23bf9a45",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### Load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6fcf830-4b4c-43cf-adc2-55de5741ee0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "save_path = \"bigcodebench3.json\"\n",
    "with open(save_path) as f:\n",
    "    data_dict = json.load(f)\n",
    "\n",
    "task_id=data_dict[\"task_id\"]\n",
    "complete_prompt=data_dict[\"complete_prompt\"]\n",
    "instruct_prompt=data_dict[\"instruct_prompt\"]\n",
    "canonical_solution=data_dict[\"canonical_solution\"]\n",
    "code_prompt=data_dict[\"code_prompt\"]\n",
    "test=data_dict[\"test\"]\n",
    "doc_struct=data_dict[\"doc_struct\"]\n",
    "\n",
    "num_problems = len(task_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ec03d21-b76e-4668-b4c0-38ac4d2d41a9",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "984ebfcd-2a26-4399-875f-4fbc3b1d3839",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(instruct_prompt[0])\n",
    "print()\n",
    "print(canonical_solution[0])\n",
    "print()\n",
    "print(test[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "868a9f0a-5586-4a9b-971d-a54e746cc23c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools\n",
    "from random import shuffle\n",
    "def task_func(numbers=list(range(1, 3))):\n",
    "    permutations = list(itertools.permutations(numbers))\n",
    "    sum_diffs = 0\n",
    "\n",
    "    for perm in permutations:\n",
    "        perm = list(perm)\n",
    "        shuffle(perm)\n",
    "        diffs = [abs(perm[i] - perm[i+1]) for i in range(len(perm)-1)]\n",
    "        sum_diffs += sum(diffs)\n",
    "\n",
    "    avg_sum_diffs = sum_diffs / len(permutations)\n",
    "    \n",
    "    return avg_sum_diffs\n",
    "\n",
    "import unittest\n",
    "from unittest.mock import patch\n",
    "from random import seed, shuffle\n",
    "import itertools\n",
    "class TestCases(unittest.TestCase):\n",
    "    def test_default_numbers(self):\n",
    "        # Test with default number range (1 to 10) to check that the result is a positive float.\n",
    "        result = task_func()\n",
    "        self.assertIsInstance(result, float)\n",
    "        self.assertGreater(result, 0)\n",
    "    def test_custom_list(self):\n",
    "        # Test with a custom list of small positive integers to ensure proper handling and positive result.\n",
    "        result = task_func([1, 2, 3])\n",
    "        self.assertIsInstance(result, float)\n",
    "        self.assertGreater(result, 0)\n",
    "    def test_negative_numbers(self):\n",
    "        # Test with negative numbers to verify the function handles and returns a positive result.\n",
    "        result = task_func([-3, -2, -1])\n",
    "        self.assertIsInstance(result, float)\n",
    "        self.assertGreater(result, 0)\n",
    "    def test_single_element(self):\n",
    "        # Test with a single element list to confirm the return is zero since no pairs exist.\n",
    "        result = task_func([5])\n",
    "        self.assertIsInstance(result, float)\n",
    "        self.assertEqual(result, 0)\n",
    "    def test_empty_list(self):\n",
    "        # Test with an empty list to ensure the function handles it gracefully and returns zero.\n",
    "        result = task_func([])\n",
    "        self.assertIsInstance(result, float)\n",
    "        self.assertEqual(result, 0)\n",
    "    def test_identical_elements(self):\n",
    "        # Test with a list of identical elements to confirm that differences are zero and the average is zero.\n",
    "        result = task_func([2, 2, 2])\n",
    "        self.assertIsInstance(result, float)\n",
    "        self.assertEqual(result, 0)\n",
    "    def test_mixed_numbers(self):\n",
    "        # Test with a list of mixed positive and negative numbers to check correct average of differences.\n",
    "        result = task_func([-10, 10, -5])\n",
    "        self.assertIsInstance(result, float)\n",
    "        self.assertGreater(result, 0)\n",
    "    def test_specific_value_with_seed(self):\n",
    "        # Set seed for reproducibility and check the computed value\n",
    "        with patch('random.shuffle', side_effect=lambda x: seed(42) or shuffle(x)):\n",
    "            result = task_func([1, 2, 3])\n",
    "            self.assertAlmostEqual(result, 2.5, delta=0.5)  # This expected value should be calculated beforehand\n",
    "    def test_large_list_with_seed(self):\n",
    "        # Set seed and test with a larger list for specific computed value\n",
    "        with patch('random.shuffle', side_effect=lambda x: seed(99) or shuffle(x)):\n",
    "            result = task_func(list(range(1, 11)))\n",
    "            self.assertAlmostEqual(result, 33.0, delta=0.5)  # This expected value should be calculated beforehand\n",
    "    def test_random_behavior(self):\n",
    "        # Test to ensure different seeds produce different outputs, demonstrating randomness\n",
    "        with patch('random.shuffle', side_effect=lambda x: seed(1) or shuffle(x)):\n",
    "            result1 = task_func([1, 2, 3])\n",
    "        with patch('random.shuffle', side_effect=lambda x: seed(1) or shuffle(x)):\n",
    "            result2 = task_func([1, 2, 4])\n",
    "        self.assertNotEqual(result1, result2)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea34d683-c61e-4d5b-97a1-c1cfadbf23da",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "t = TestCases()\n",
    "t.test_default_numbers()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78db2dc4-7a60-41b5-820c-bd399b98a4b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import inspect\n",
    "def check_code():\n",
    "    instance = TestCases()\n",
    "    methods = inspect.getmembers(cls, predicate=inspect.isfunction)\n",
    "    try:\n",
    "        for name, method in methods:\n",
    "            if name.startswith('test_'):\n",
    "                method(instance)\n",
    "    except Exception as e:\n",
    "        print(\"An error occurred:\", e)\n",
    "        return False\n",
    "    return True\n",
    "\n",
    "is_code_correct__ = check_code()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4f1c6e7-ba4c-45e4-851e-3254c9526ce9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools\n",
    "from random import shuffle\n",
    "\n",
    "def task_func(numbers=list(range(1, 11))):\n",
    "    # Generate all permutations of the list\n",
    "    permutations = list(itertools.permutations(numbers))\n",
    "\n",
    "    # Initialize a list to store the sum of absolute differences for each permutation\n",
    "    sums_of_differences = []\n",
    "\n",
    "    # Iterate over each permutation\n",
    "    for perm in permutations:\n",
    "        # Shuffle the permutation\n",
    "        shuffled_perm = list(perm)\n",
    "        shuffle(shuffled_perm)\n",
    "\n",
    "        # Calculate the sum of absolute differences between consecutive numbers\n",
    "        sum_diff = sum(abs(shuffled_perm[i] - shuffled_perm[i + 1]) for i in range(len(shuffled_perm) - 1))\n",
    "\n",
    "        # Append the sum to the list\n",
    "        sums_of_differences.append(sum_diff)\n",
    "\n",
    "    # Calculate the average of the sums of absolute differences\n",
    "    average_sum_diff = sum(sums_of_differences) / len(sums_of_differences)\n",
    "\n",
    "    return average_sum_diff\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d748bc2-cdba-4825-a901-4a3e7fd6e040",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Kaggle Reference Problems"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f8d3eef-da26-4ccb-955d-6482e81a86d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "problems = [\n",
    "    \"Three airline companies operate flights from Dodola island. Each company has a different schedule of departures. The first company departs every 100 days, the second every 120 days and the third every 150 days. What is the greatest positive integer $d$ for which it is true that there will be $d$ consecutive days without a flight from Dodola island, regardless of the departure times of the various airlines?\",\n",
    "    \"Fred and George take part in a tennis tournament with $4046$ other players. In each round, the players are paired into $2024$ matches. How many ways are there to arrange the first round such that Fred and George do not have to play each other? (Two arrangements for the first round are \\textit{different} if there is a player with a different opponent in the two arrangements.)\",\n",
    "    \"Triangle $ABC$ has side length $AB = 120$ and circumradius $R = 100$. Let $D$ be the foot of the perpendicular from $C$ to the line $AB$. What is the greatest possible length of segment $CD$?\",\n",
    "    \"Find the three-digit number $n$ such that writing any other three-digit number $10^{2024}$ times in a row and $10^{2024}+2$ times in a row results in two numbers divisible by $n$.\",\n",
    "    \"We call a sequence $a_1, a_2, \\ldots$ of non-negative integers \\textit{delightful} if there exists a positive integer $N$ such that for all $n > N$, $a_n = 0$, and for all $i \\geq 1$, $a_i$ counts the number of multiples of $i$ in $a_1, a_2, \\ldots, a_N$. How many delightful sequences of non-negative integers are there?\",\n",
    "    \"Let $ABC$ be a triangle with $BC=108$, $CA=126$, and $AB=39$. Point $X$ lies on segment $AC$ such that $BX$ bisects $\\angle CBA$. Let $\\omega$ be the circumcircle of triangle $ABX$. Let $Y$ be a point on $\\omega$ different from $X$ such that $CX=CY$. Line $XY$ meets $BC$ at $E$. The length of the segment $BE$ can be written as $\\frac{m}{n}$, where $m$ and $n$ are coprime positive integers. Find $m+n$.\",\n",
    "    \"For a positive integer $n$, let $S(n)$ denote the sum of the digits of $n$ in base 10. Compute $S(S(1)+S(2)+\\cdots+S(N))$ with $N=10^{100}-2$.\",\n",
    "    \"\"\"For positive integers $x_1,\\ldots, x_n$ define $G(x_1, \\ldots, x_n)$ to be the sum of their $\\frac{n(n-1)}{2}$ pairwise greatest common divisors. We say that an integer $n \\geq 2$ is \\emph{artificial} if there exist $n$ different positive integers $a_1, ..., a_n$ such that \n",
    "\\[a_1 + \\cdots + a_n = G(a_1, \\ldots, a_n) +1.\\]\n",
    "Find the sum of all artificial integers $m$ in the range $2 \\leq m \\leq 40$.\"\"\",\n",
    "    \"The Fibonacci numbers are defined as follows: $F_0 = 0$, $F_1 = 1$, and $F_{n+1} = F_n + F_{n-1}$ for $n \\geq 1$. There are $N$ positive integers $n$ strictly less than $10^{101}$ such that $n^2 + (n+1)^2$ is a multiple of 5 but $F_{n-1}^2 + F_n^2$ is not. How many prime factors does $N$ have, counted with multiplicity?\",\n",
    "    \"Alice writes all positive integers from $1$ to $n$ on the board for some positive integer $n \\geq 11$. Bob then erases ten of them. The mean of the remaining numbers is $3000/37$. The sum of the numbers Bob erased is $S$. What is the remainder when $n \\times S$ is divided by $997$?\",\n",
    "]\n",
    "\n",
    "correct_answers = [79, 250, 180, 143, 3, 751, 891, 810, 201, 902]\n",
    "\n",
    "solutions = [\n",
    "    \"\"\"The airlines are called A100, A120 and A150, labelled by the frequency of their departures. We first prove that there is a period of 99 days after an A100 departure during which no A150 plane takes off.\\\\\n",
    "Consider a period of 301 days which starts with an A150 departure on Day 0, followed by a departure on Day 150 and on Day 300. Let the first A100 departure in this period be on Day $x$.\n",
    "\n",
    "There are two possibilities: (i) $0 \\leq x \\leq 50$ or (ii) $51 \\leq x \\leq 99$. In case (i), there is a quiet period of 99 days after the first $A 100$ departure. In case (ii), the second A100 departure will be on Day $100+x$ where $151 \\leq 100+x \\leq 199$ so there will be a period of 99 consecutive days after the second A100 departure with no A150 departure.\n",
    "\n",
    "We will now prove that there are 79 consecutive days on which no departure of any airline happens, including the A120 planes. We restart time and define Day 0 to be when an $A 100$ flight departs and there is no A150 flight before Day 100. This situation will repeat later because $300=3 \\times 100=$ $2 \\times 150$. The fourth A100 flight will take off on Day 300 and there will be no subsequent departure of an $A 150$ plane before Day 400.\n",
    "\n",
    "Suppose the first departure of an A120 plane is on Day $y$. If $y \\leq 20$ or $y \\geq 80$, we have found the 79 consecutive days by looking after or before this A120 departure.\\\\\n",
    "If $20 \\leq y \\leq 60$, then there will be an A120 departure on day $240+y$ where $260 \\leq 240+y \\leq 300$ so there will be no A120 departure strictly between Day 300 and Day 380 and so we will find the required 79 consecutive quiet days between those dates.\n",
    "\n",
    "Finally, if $61 \\leq y \\leq 80$, there will be an A120 departure on Day $y+240$ where $301 \\leq y+240 \\leq 320$ and there will be no subsequent departures before Day 400 and again we find the required 79 consecutive quiet days.\\\\\n",
    "We now show that this bound can be attained. Suppose that an A100 departs on Day 0, an A120 departs on Day 80 and an A150 departs on Day 120. The departure days are then:\n",
    "\n",
    "$$\n",
    "0,80,100,120,200 \\& 200,270,300,320,400,420,440,500,560,570\n",
    "$$\n",
    "\n",
    "modulo 600 (i.e. it repeats every 600 days).\\\\\n",
    "The longest run of consecutive days without flights is 79 days (and this is obtained three times in this 600 day cycle).\n",
    "\"\"\",\n",
    "\n",
    "    \"\"\"Consider an tournament with $2 m$ players. The number of possible first round pairings can be calculated by labelling the matches 1 to $m$. Label the players in order 1 to $2 m$ (this can be done in $(2 m)$ ! ways), and assign players $(2 i-1)$ and $2 i$ to match number $i$.\\\\\n",
    "How many times does any given particular first round pairing arise from the method specified above? Swapping the labels of any pair $2 i-1,2 i$ does not change the pairings, nor does permuting the order of the labels of the matches. Doing anything else will result in a different first round pairing, so $(2 m)!$ is overcounting the number of first round pairings by a factor of $2^{m}(m!)$.\\\\\n",
    "Therefore the number of first round pairings is\n",
    "\n",
    "$$\n",
    "\\frac{(2 m)!}{2^{m} m!}\n",
    "$$\n",
    "\n",
    "Now consider the odd and even factors in the product defining $(2 m)$ ! separately. We see that\n",
    "\n",
    "$$\n",
    "(2 m)!=[(2 m) \\cdot 2(m-1) \\cdots 2] \\times[(2 m-1) \\cdot(2 m-3) \\cdots 3 \\cdot 1] .\n",
    "$$\n",
    "\n",
    "The product of the odd numbers $(2 m-1) \\cdot(2 m-3) \\cdots 3 \\cdot 1$ is often written as $(2 m-1)$ !!. Pulling factors of 2 out of the even factors we obtain\n",
    "\n",
    "$$\n",
    "(2 m)!=2^{m} \\cdot m!\\cdot(2 m-1)!!\n",
    "$$\n",
    "\n",
    "Therefore the number of different first round pairings is\n",
    "\n",
    "$$\n",
    "\\frac{(2 m)!}{2^{m} \\cdot m!}=(2 m-1)!!\n",
    "$$\n",
    "\n",
    "The number of first round pairings of the $4048=2 n+2$ players where Fred and George do not play each other is the total number of pairings minus the number of pairings in which they play each other. This is $(2 n+1)!!-(2 n-1)!!=2 n(2 n-1)!!=4046 \\cdot 4045!!$. This number is clearly divisible by 125 because of the large number of factors of 5 in $4045!!$. We would like to know the remainder when $4046 \\cdot 4045$ !! is divided by 8 , because then we could apply the Chinese Remainder Theorem and deduce the remainder on division by 1000 . Note that $4045 \\times 4043 \\times 4041 \\equiv 5 \\times 3 \\times 1 \\equiv-1 \\mathrm{mod}$ 8. The sequence of odd positive integers has period $4 \\operatorname{modulo} 8$, and $4039 \\equiv 7 \\bmod 8$. Notice that $1 \\times 3 \\times 5 \\times 7 \\equiv 1 \\bmod 8$. Running the sequence of odd numbers from 1 to 4039 cycles through a whole number of periods modulo 8 and so $4039!!\\equiv 1 \\bmod 8$. Now $4046 \\cdot 4045!!\\equiv 6 \\times(-1) \\times 1 \\equiv 2$ $\\bmod 8$.\\\\\n",
    "By the Chinese remainder theorem there is a unique integer $x$ in the range $0 \\leq x \\leq 999$ which satisfies our conditions modulo 8 and modulo 125, and by inspection that is 250 and so\n",
    "\n",
    "$$\n",
    "4046 \\cdot 4045!!\\equiv 250 \\bmod 1000\n",
    "$$\n",
    "\n",
    "and we report 250 .\"\"\",\n",
    "    \n",
    "    \"\"\"Let $O$ be the circumcentre of triangle $A B C$. Then\n",
    "\n",
    "$$\n",
    "\\operatorname{dist}(O, A B)=\\sqrt{R^{2}-(A B / 2)^{2}}=\\sqrt{100^{2}-60^{2}}=80\n",
    "$$\n",
    "\n",
    "by Pythagoras.\\\\\n",
    "\\includegraphics[max width=\\textwidth, center]{2024_12_04_73c4dcc43c6e7936620eg-04}\n",
    "\n",
    "Since $C$ must be on the circle with centre $O$ and radius $O A$, the largest possible altitude $h_{c}$ is attained when $C$ is the mid-point of the larger $\\operatorname{arc} A B$ of the circumcircle (i.e. on the perpendicular bisector of $A B$ ) in which case we have $h_{c}=\\operatorname{dist}(O, A B)+R=80+100=180$.\"\"\",\n",
    "    \n",
    "    \"\"\"Let $M=10^{1024}$. Let $a$ be any three-digit number. Writing $M$ copies of $a$ in a row results in a number $X$ where\n",
    "\n",
    "$$\n",
    "X=a \\times 100100100 \\ldots 1001001\n",
    "$$\n",
    "\n",
    "and there are $M$ copies of the digit one in the long number. If instead we wrote $M+2$ copies of $a$ in a row, the resulting number would be $10^{6} X+1001 a$. We use the notation $(u, v)$ to denote the greatest common divisor of two integers $u$ and $v$ which are not both 0 .\\\\\n",
    "We apply Euclid's algorithm so\n",
    "\n",
    "$$\n",
    "\\left(\\left(10^{6} X+1001 a\\right), X\\right)=(1001 a, X)\n",
    "$$\n",
    "\n",
    "It is therefore a necessary condition that our three-digit number $n$ should divide $(1001 a, X)$ for all three-digit numbers $a$. By considering $a=100$ and $a=101$, we see that any candidate for $n$ must divide $1001 \\times 101-1001 \\times 100=1001$. Moreover, if $n$ is a divisor of 1001 , then $n$ will divide $X$ because 1001 divides $10010010010 \\ldots 01001001$ which is\n",
    "\n",
    "$$\n",
    "1001 \\times 10000010000010 \\ldots 01000001\n",
    "$$\n",
    "\n",
    "The second factor involves $M / 2$ copies of the digit one. Such an $n$ will also divide $10^{6} X+1001 a$.\\\\\n",
    "Thus it is a necessary and sufficient condition for $n$ to satisfy the conditions of the problem that $n$ be a three-digit divisor of $1001(=7 \\times 11 \\times 13)$. There is a unique such number: 143 .\"\"\",\n",
    "    \n",
    "    \"\"\"We claim the only such sequences are $(1,0,0, \\ldots),(2,1,0,0, \\ldots)$ and $(2,2,0,0, \\ldots)$. Note that $a_{1}=N$ because 1 divides each of the first $N$ terms. Hence if $N=1$, we necessarily have the sequence $(1,0, \\ldots)$, and that obeys the conditions.\\\\\n",
    "Observe that if $1 \\leq i \\leq N$, then $a_{i}$ is at most $N$ because $a_{i}$ counts the size of a subset of $\\{1,2, \\ldots, N\\}$. For each $1 \\leq i \\leq N$, if (for contradiction) $a_{i}=0$, then $i$ divides $a_{i}$ and so $a_{i} \\neq 0$, which is absurd. Therefore the first $N$ terms of the sequence are non-zero (including $a_{N}$ ), and all subsequent terms are 0 because if $i>N$, then $i$ cannot divide any of the first $N$ terms which are all non-zero and at most $N$. This means that a sequence satisfying the condition for some $N$ will only satisfy the condition for that particular $N$.\\\\\n",
    "Next assume that $N>1$. The term $a_{N-1}$ is positive and so there is an index $j$ in the range $1 \\leq j \\leq N$ such that $N-1$ divides $a_{j}$. However, for $1 \\leq i \\leq N$ each $a_{i}$ is at $\\operatorname{most} N$, so $N-1 \\leq a_{j} \\leq N$. In the case that $N=2$ this yields the second and third delightful sequences mentioned above, and proves that there are no others.\\\\\n",
    "It remains to study the case $N>2$. In that case, $N-1$ does not divide $N$ so, taking $j$ as above, we must have $a_{j}=N-1 \\quad(\\neq 1)$. If (for contradiction) $a_{N} \\geq 2$ there is an index $k$ with $1<k \\leq N$ such that $a_{k}=N$, so $k$ divides all non-zero terms of the sequence, and in particular $k$ divides both $N-1$ and $N$ and so must be 1 , which is absurd. Finally suppose (for contradiction) that $a_{N}=1$. In that case, $j$ (recall that $a_{j}=N-1>1$ so $2 \\leq j \\leq N-1$ ) does not divide $a_{N}$ but $j$ does divide all previous terms of the sequence. Therefore $j$ divides $a_{1}$ which is $N$ and $a_{j}$ which is $N-1$ so $j=1$. Therefore, $a_{1}$ is both $N-1$ and $N$, which is absurd.\n",
    "\n",
    "Thus, there are no delightful sequences with $N>2$. There are exactly the 3 delightful sequences mentioned above so we report the answer 3.\"\"\",\n",
    "    \n",
    "    \"\"\"We have the key claim: $\\omega$ is tangent to $B C$.\\\\\n",
    "Proof of Claim: The angle bisector theorem gives $\\frac{C X}{C A}=\\frac{C B}{C B+B A}$ which rearranges to\n",
    "\n",
    "$$\n",
    "C X=\\frac{A C \\cdot B C}{A B+B C}=\\frac{126 \\times 108}{147}=\\frac{6 \\times 108}{7}=\\frac{648}{7}\n",
    "$$\n",
    "\n",
    "Now calculate: $C B^{2}=108^{2}=\\frac{648}{7} \\times 126=C X \\cdot C A$ and we establish the required tangency by the converse of power of point (the tangent-secant theorem) applied to $C$ and $\\omega$.\\\\\n",
    "\\includegraphics[max width=\\textwidth, center]{2024_12_04_73c4dcc43c6e7936620eg-03}\n",
    "\n",
    "Having established this tangency, we now perform a short calculation.\\\\\n",
    "Let $\\Gamma$ denote the circle with centre $C$ and radius $C X$ which passes through $Y$. The point $E$ is on the radical axis $X Y$ of $\\omega$ and $\\Gamma$. It follows that $E$ has equal powers with respect to both circles $\\omega$ and $\\Gamma$.\n",
    "\n",
    "Let $x=B E$ so $E C=108-x$. The power of $E$ with respect to $\\omega$ is $x^{2}$ (because of the tangency at $B$ ) and the power of $E$ with respect to $\\Gamma$ is $E C^{2}-X C^{2}$ because $C$ is the centre of $\\Gamma$ and $X C$ is its radius.\n",
    "\n",
    "We have\n",
    "\n",
    "$$\n",
    "x^{2}=(108-x)^{2}-\\left(\\frac{648^{2}}{7}\\right)^{2}\n",
    "$$\n",
    "\n",
    "The quadratic terms cancel so $216 x=108^{2}-\\frac{648^{2}}{49}=\\frac{49 \\times 108^{2}-648^{2}}{49}$ and this gives the solution $x=\\frac{702}{49}$. Now, 702 and 49 are coprime so $m+n=751$ is the required answer.\"\"\",\n",
    "    \n",
    "    \"\"\"For each integer $k$ in the range $0 \\leq k \\leq 10^{100}-1$ we have\n",
    "\n",
    "$$\n",
    "k+\\left(10^{100}-k-1\\right)=10^{100}-1\n",
    "$$\n",
    "\n",
    "which in decimal notation is a string of 100 nines. Deeming all numbers $k$ in the range to be decimal strings of 100 digits (including initial padding zeros when necessary), we see that for each $j$ in the range $1 \\leq j \\leq 100$, the $j$-th digit of $k$ and $j$-th digit of $10^{100}-1-k$ add up to 9 .\\\\\n",
    "Therefore for each integer $k$ in the range $0 \\leq k \\leq 10^{100}-1$ we have $S(k)+S\\left(10^{100}-1-k\\right)=900$. Recall that $N=10^{100}-2$ and observe that $S(0)=0$. Then\n",
    "\n",
    "$$\n",
    "2\\left(S(0)+S(1)+S(2)+\\cdots+S\\left(10^{100}-1\\right)\\right)=900 \\times 10^{100}\n",
    "$$\n",
    "\n",
    "so\n",
    "\n",
    "$$\n",
    "S(1)+S(2)+\\cdots+S(N)=450 \\times\\left(10^{100}-1\\right)-S\\left(10^{100}-1\\right)=450 \\times 10^{100}-900\n",
    "$$\n",
    "\n",
    "In decimal notation, $450 \\times 10^{100}$ is the string 45 followed by 101 zeros. Subtracting 900 gives the string 44 followed by 98 nines and then the digits 100.\n",
    "\n",
    "The digit sum of this number is $4+4+98 \\times 9+1=99 \\times 9=891$. We report 891 .\"\"\",\n",
    "    \n",
    "    \"\"\"We will show that the smallest artificial number is 5 , and that if $a$ is an artificial number, then so too is $a+1$. This will solve the problem.\n",
    "\n",
    "First, we eliminate small cases. If $n=2$ and the different positive integers are $a, b$ with $a<b$. Then, $\\operatorname{gcd}(a, b) \\leq a$ so $\\operatorname{gcd}(a, b)+1=G(a, b)+1 \\leq a+1 \\leq b<a+b$.\\\\\n",
    "Now suppose that $n=3$ and the different positive integers are $a<b<c$. Using round brackets to denote gcds, we have\n",
    "\n",
    "$$\n",
    "(a, b)+(b, c)+(c, a) \\leq 2 a+b \\leq a+b+c-2\n",
    "$$\n",
    "\n",
    "because $a \\leq c-2$. Therefore $G(a, b, c)+1<a+b+c$.\\\\\n",
    "Next we tackle the case $n=4$, and let the positive integers be $a<b<c<d$. Now\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "G(a, b, c, d)+1 & =[(a, b)+(b, c)+(c, d)]+[(a, c)+(a, d)]+(b, d)+1 \\\\\n",
    "& =[(a, b-a)+(b, c-b)+(c, d-c)]+[(a, c)+(a, d)]+(b, d)+1 \\\\\n",
    "& \\leq(b-a+c-b+d-c)+2 a+b+1 \\\\\n",
    "& =a+b+d+1 .\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "Now $c \\geq 3$ so\n",
    "\n",
    "$$\n",
    "G(a, b, c, d)+1 \\leq a+b+c+d+(1-c) \\leq a+b+c+d-2\n",
    "$$\n",
    "\n",
    "Having eliminated small cases, we show that 5 is artificial. Consider the numbers 1,2,3,4 and 6 . Now\n",
    "\n",
    "$$\n",
    "G(1,2,3,4,6)+1=1+1+1+1+1+2+2+1+3+2+1=16=(1+2+3+4+6)\n",
    "$$\n",
    "\n",
    "Therefore 5 is artificial.\\\\\n",
    "Now suppose that $m$ is an artificial positive integer witnessed by different positive integers $a_{1}, a_{2}, \\ldots a_{m}$. For $1 \\leq i \\leq m$, let $b_{i}=m a_{i}$ and let $b_{m+1}=1$. We will show that these $m+1$ different positive integers $b_{i}$ are witnesses to $m+1$ being artificial.\n",
    "\n",
    "If $i, j \\leq m$, then $\\operatorname{gcd}\\left(b_{i}, b_{j}\\right)=m \\cdot \\operatorname{gcd}\\left(a_{i}, a_{j}\\right)$ and $\\left(b_{i}, b_{m+1}\\right)=1$ for $1 \\leq i \\leq m$. Therefore\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "G\\left(b_{1}, b_{2}, \\ldots b_{m+1}\\right) & =G\\left(b_{1}, b_{2}, \\ldots, b_{m}\\right)+m \\\\\n",
    "& =m \\cdot G\\left(a_{1}, a_{2}, \\ldots, a_{m}\\right)+m \\\\\n",
    "& =m\\left(a_{1}+a_{2}+\\cdots+a_{m}-1\\right)+m \\\\\n",
    "& =b_{1}+b_{2}+\\cdots+b_{m} \\\\\n",
    "& =\\left(b_{1}+b_{2}+\\cdots+b_{m+1}\\right)-1 .\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "Therefore $m$ is artificial for all $m \\geq 5$ by induction. We must report\n",
    "\n",
    "$$\n",
    "5+6+\\cdots+40=\\frac{45 \\times 36}{2}=810\n",
    "$$\"\"\",\n",
    "    \n",
    "    \"\"\"Checking modulo 5 , we find that $n^{2}+(n+1)^{2} \\equiv 0 \\bmod 5$ if, and only if, $n$ is 1 or 3 modulo 5. On the other hand, the Fibonacci numbers modulo 5 form a sequence of period 20 and their squares form a sequence of period 10 . By inspection $F_{n-1}^{2}+F_{n}^{2} \\equiv 0 \\bmod 5$ if, and only if, $n$ is $3 \\bmod 5$. Therefore we are being asked to find the number of positive integers $m$ in the range $1 \\leq m \\leq 10^{101}$ such that $m$ is 1 modulo 5 . This is one fifth of the numbers in the range since $10^{101}$ is divisible by 5 , so\n",
    "\n",
    "$$\n",
    "N=10^{101} / 5=2 \\times 10^{100}=2^{101} \\cdot 5^{100}\n",
    "$$\n",
    "\n",
    "We therefore report $100+101=201$.\"\"\",\n",
    "    \n",
    "    \"\"\"Let $T$ be the sum of the numbers, after Bob has erased 10 of them. We can bound $T$ by considering what happens if Bob happens to erase the smallest ten numbers, or the largest ten numbers.\\\\\n",
    "The average of the set $\\{1,2, \\ldots, n-10\\}$ is $\\frac{n-9}{2}$ (the average of the first and last terms of a finite arithmetic progression). Similarly, average of the set $\\{11,12, \\ldots, n\\}$ is $\\frac{n+11}{2}$. The average of the remaining numbers, $\\frac{3000}{37}=\\frac{T}{n-10}$, must be bounded by these quantities, so\n",
    "\n",
    "$$\n",
    "\\frac{n-9}{2} \\leq \\frac{T}{n-10}=\\frac{3000}{37} \\leq \\frac{n+11}{2}\n",
    "$$\n",
    "\n",
    "It follows that $n-10$ must be a multiple of 37 so that it can cancel down to 37 . Another way to say that is $n \\equiv 10 \\bmod 37$. Multiplying the inequalities by 2 we obtain\n",
    "\n",
    "$$\n",
    "n-9 \\leq \\frac{6000}{37} \\leq n+11\n",
    "$$\n",
    "\n",
    "or rather\n",
    "\n",
    "$$\n",
    "\\frac{6000}{37}-11 \\leq n \\leq \\frac{6000}{37}+9\n",
    "$$\n",
    "\n",
    "Now $6000 / 37=162+\\frac{6}{37}$ so\n",
    "\n",
    "$$\n",
    "152 \\leq n \\leq 171\n",
    "$$\n",
    "\n",
    "The only integer in this range which is 10 modulo 37 is 158 , so $n=158$ and\n",
    "\n",
    "$$\n",
    "T=(n-10) \\times \\frac{3000}{37}=\\frac{148 \\times 3000}{37}=4 \\times 3000=12000\n",
    "$$\n",
    "\n",
    "Let the sum of the numbers erased by Bob be $S$, so\n",
    "\n",
    "$$\n",
    "S=\\left(\\sum_{i=1}^{158} i\\right)-12000=\\frac{158 \\times 159}{2}-12000=12561-12000=561\n",
    "$$\n",
    "\n",
    "Bob can achieve this by erasing $\\{51,52, \\ldots, 59,66\\}$.\\\\\n",
    "The number that we must report is $n \\times S$ modulo 997 , which is $158 \\times 561$ modulo 997 , which is 902 .\"\"\",\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48e20df7-86e6-4931-9629-384ea8989a5c",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## AIME Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23c527f5-0e67-46ad-be69-b41977cf7cd3",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### Download"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e55508e-2742-4241-8293-59df56b8708d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "num_sample = 10000\n",
    "dataset_name = 'Dahoas/aimo-validation-aime'\n",
    "save_path = \"aime.json\"\n",
    "\n",
    "import pandas as pd\n",
    "from datasets import load_dataset\n",
    "\n",
    "# Load a dataset from Hugging Face\n",
    "dataset = load_dataset(dataset_name)\n",
    "\n",
    "# Convert the dataset to a pandas DataFrame\n",
    "# Assuming you want to use the 'train' split of the dataset\n",
    "df = pd.DataFrame(dataset['train'])\n",
    "\n",
    "# Convert the DataFrame to a list of dictionaries\n",
    "data_list = df.to_dict(orient='records')\n",
    "data_list = data_list[:num_sample]\n",
    "\n",
    "# Print the first few records to verify\n",
    "#print(data_list[:5])\n",
    "\n",
    "# Prepare list\n",
    "problems = []\n",
    "correct_answers = []\n",
    "solutions = []\n",
    "for i, data_dict in enumerate(data_list):\n",
    "    problems.append(data_dict[\"problem\"])\n",
    "    correct_answers.append(int(data_dict[\"answer\"]))\n",
    "    solutions.append(data_dict[\"solution\"].split(\"\\n~\")[0])\n",
    "\n",
    "import json\n",
    "with open(save_path, \"w\") as f:\n",
    "    json.dump({\"problems\":problems, \"correct_answers\":correct_answers, \"solutions\":solutions}, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec58072b-3bd5-4dd9-abcb-bdb12610a95a",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### Load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "552b7630-bbc0-4344-ac4f-bf342f4f0bda",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "save_path = \"aime.json\"\n",
    "with open(save_path) as f:\n",
    "    data = json.load(f)\n",
    "\n",
    "problems = data[\"problems\"]\n",
    "correct_answers = data[\"correct_answers\"]\n",
    "solutions = data[\"solutions\"]\n",
    "num_problems = len(problems)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5c64e41-96e5-4d35-ad66-1d94715054aa",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3b5cc36-2b25-4855-9588-48850754ac3b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "id = 3\n",
    "print(data_list[id][\"problem\"])\n",
    "print()\n",
    "print(data_list[id][\"solution\"].split(\"\\n~\")[0])\n",
    "print()\n",
    "print(data_list[id][\"answer\"])\n",
    "print(type(int(data_list[id][\"answer\"])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a4e088f-5b4c-45e1-aeb8-b7b3ace4a2a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(data_list))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "179abaa1-27ab-475c-810b-e8e03a555867",
   "metadata": {},
   "source": [
    "# Solve Problems"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8dd7dc69-5063-4106-a7b1-6542f2afb57a",
   "metadata": {},
   "source": [
    "## SWEBench"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "26abe5ec-de7e-4582-a19f-827299850206",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 03-30 12:29:16 config.py:478] This model supports multiple tasks: {'reward', 'embed', 'classify', 'generate', 'score'}. Defaulting to 'generate'.\n",
      "INFO 03-30 12:29:16 llm_engine.py:249] Initializing an LLM engine (v0.6.5) with config: model='/workspace/qwen3b', speculative_config=None, tokenizer='/workspace/qwen3b', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=32768, download_dir=None, load_format=auto, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto, quantization_param_path=None, device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='xgrammar'), observability_config=ObservabilityConfig(otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=0, served_model_name=/workspace/qwen3b, num_scheduler_steps=1, multi_step_stream_outputs=True, enable_prefix_caching=False, chunked_prefill_enabled=False, use_async_output_proc=True, mm_cache_preprocessor=False, mm_processor_kwargs=None, pooler_config=None, compilation_config={\"splitting_ops\":[\"vllm.unified_attention\",\"vllm.unified_attention_with_output\"],\"candidate_compile_sizes\":[],\"compile_sizes\":[],\"capture_sizes\":[256,248,240,232,224,216,208,200,192,184,176,168,160,152,144,136,128,120,112,104,96,88,80,72,64,56,48,40,32,24,16,8,4,2,1],\"max_capture_size\":256}, use_cached_outputs=False, \n",
      "INFO 03-30 12:29:17 selector.py:120] Using Flash Attention backend.\n",
      "INFO 03-30 12:29:18 model_runner.py:1092] Starting to load model /workspace/qwen3b...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7e562445b6de48a5b08a7a2b736bb9ba",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading safetensors checkpoint shards:   0% Completed | 0/2 [00:00<?, ?it/s]\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 03-30 12:29:56 model_runner.py:1097] Loading model weights took 5.7915 GB\n",
      "INFO 03-30 12:29:58 worker.py:241] Memory profiling takes 2.40 seconds\n",
      "INFO 03-30 12:29:58 worker.py:241] the current vLLM instance can use total_gpu_memory (44.34GiB) x gpu_memory_utilization (0.90) = 39.91GiB\n",
      "INFO 03-30 12:29:58 worker.py:241] model weights take 5.79GiB; non_torch_memory takes 0.14GiB; PyTorch activation peak memory takes 2.52GiB; the rest of the memory reserved for KV Cache is 31.45GiB.\n",
      "INFO 03-30 12:29:59 gpu_executor.py:76] # GPU blocks: 57251, # CPU blocks: 7281\n",
      "INFO 03-30 12:29:59 gpu_executor.py:80] Maximum concurrency for 32768 tokens per request: 27.95x\n",
      "INFO 03-30 12:30:02 model_runner.py:1413] Capturing cudagraphs for decoding. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI.\n",
      "INFO 03-30 12:30:02 model_runner.py:1417] If out-of-memory error occurs during cudagraph capture, consider decreasing `gpu_memory_utilization` or switching to eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.\n",
      "INFO 03-30 12:30:18 model_runner.py:1527] Graph capturing finished in 16 secs, took 0.23 GiB\n",
      "INFO 03-30 12:30:18 llm_engine.py:446] init engine (profile, create kv cache, warmup model) took 22.27 seconds\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7145928dd96a425c86b9bca24decb61f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "modules.json:   0%|          | 0.00/229 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "09f8df4e32aa4d17a1de88d3630c27be",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config_sentence_transformers.json:   0%|          | 0.00/266 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "968a3c1f8d9c4906b6f920699e634782",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "README.md:   0%|          | 0.00/114k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "09eee898e2db497e81a642691143c45c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "sentence_bert_config.json:   0%|          | 0.00/53.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8cee3311627541e6b003d1a08308e65f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/677 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "41e3a739d52e477b854d5ae5555a3993",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/670M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1780f464ae2e4fb7a1d3a79dfb95c106",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/1.24k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c8c2d7bdf7534359ae4f97c5fb3c3036",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a1bc87a18f7741f9acf005d3e9d81c0f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/711k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e1482543ba1548c78fdb446147ca846c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/695 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0c819a778a97483d930089b7a88f04ef",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/297 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "SEIMEI.expert_classes:  [<class 'Answer.Answer'>, <class 'CheckInf.CheckInf'>, <class 'MetaSurvey.MetaSurvey'>, <class 'CollectCodeFileToModify.CollectCodeFileToModify'>]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from SEIMEI import SEIMEI\n",
    "import asyncio\n",
    "\n",
    "processed_path = \"./processed\"  # input path same as save_path you used in Preparation\n",
    "database_path = \"./database\"\n",
    "#se_restrictions = [\"MetaSurvey2\"]  # search engine only hits classes in this list normally (except when adding expert_restriction in kwargs)\n",
    "expert_config = [\n",
    "    {\n",
    "        \"dir_path\" : \"./Experts/SWE/\", # can be either folder or file\n",
    "        \"class_names\" : [\"Answer\", \"CheckInf\", \"MetaSurvey\", \"CollectCodeFileToModify\"]\n",
    "    }\n",
    "]\n",
    "\n",
    "seimei = SEIMEI(\n",
    "    processed_path = processed_path,\n",
    "    database_path = database_path, \n",
    "    expert_config = expert_config,\n",
    "    max_inference_time = 1000,\n",
    "    tensor_parallel_size = 1,\n",
    "    max_request = 20,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77d50479-f22c-4308-a171-e6d812a00780",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Expert <class 'SEIMEI.Experts'> started\n",
      "\n",
      "\n",
      "Expert <class 'SEIMEI.SpecificExperts'> started\n",
      "\n",
      "\n",
      "Expert <class 'SEIMEI.Search'> started\n",
      "\n",
      "\n",
      "Expert <class 'SEIMEI.PermanentExperts'> started\n",
      "\n",
      "\n",
      "Expert <class 'MetaSurvey.MetaSurvey'> started\n",
      "\n",
      "<class 'SEIMEI.Experts'>\n",
      "{'query': 'How to implement a new equilibrium state called Miller equilibrium into gyro-kinetic vlasov simulation?', 'doc_path': '/gkv-code'}\n",
      "\n",
      "Expert <class 'QuickSummary.QuickSummary'> started\n",
      "\n",
      "QuickSummary prompt num token:  5166\n",
      "QuickSummary prompt num token:  835\n",
      "QuickSummary prompt num token:  4327\n",
      "QuickSummary prompt num token:  4815\n",
      "QuickSummary prompt num token:  2699\n",
      "QuickSummary prompt num token:  185\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (150445 > 131072). Running this sequence through the model will result in indexing errors\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "QuickSummary prompt num token:  8625\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (187652 > 131072). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (187652 > 131072). Running this sequence through the model will result in indexing errors\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "QuickSummary prompt num token:  7799\n",
      "QuickSummary prompt num token:  8977\n",
      "QuickSummary prompt num token:  7707\n",
      "QuickSummary prompt num token:  5280\n",
      "QuickSummary prompt num token:  7341\n",
      "QuickSummary prompt num token:  5509\n",
      "QuickSummary prompt num token:  4273\n",
      "QuickSummary prompt num token:  4608\n",
      "QuickSummary prompt num token:  4708\n",
      "QuickSummary prompt num token:  3369\n",
      "QuickSummary prompt num token:  4927\n",
      "QuickSummary prompt num token:  6132\n",
      "QuickSummary prompt num token:  4330\n",
      "QuickSummary prompt num token:  7788\n",
      "QuickSummary prompt num token:  10237\n",
      "QuickSummary prompt num token:  10238\n",
      "QuickSummary prompt num token:  10239\n",
      "QuickSummary prompt num token:  10238\n",
      "QuickSummary prompt num token:  10237\n",
      "QuickSummary prompt num token:  10239\n",
      "QuickSummary prompt num token:  9087\n",
      "QuickSummary prompt num token:  2196\n",
      "\n",
      "Expert <class 'SEIMEI.PermanentExpert'> started\n",
      "\n",
      "\n",
      "Expert <class 'CheckInf.CheckInf'> started\n",
      "\n",
      "\n",
      "\n",
      "Expert <class 'CheckInf.CheckInf'> ended\n",
      "\n",
      "result: None\n",
      "\n",
      "\n",
      "\n",
      "Expert <class 'SEIMEI.PermanentExpert'> started\n",
      "\n",
      "\n",
      "Expert <class 'CollectCodeFileToModify.CollectCodeFileToModify'> started\n",
      "\n",
      "\n",
      "\n",
      "Expert <class 'CollectCodeFileToModify.CollectCodeFileToModify'> ended\n",
      "\n",
      "result: None\n",
      "\n",
      "\n",
      "INFO 03-30 12:31:14 async_llm_engine.py:211] Added request 0.\n",
      "INFO 03-30 12:31:14 async_llm_engine.py:211] Added request 1.\n",
      "INFO 03-30 12:31:14 async_llm_engine.py:211] Added request 2.\n",
      "INFO 03-30 12:31:14 async_llm_engine.py:211] Added request 3.\n",
      "INFO 03-30 12:31:14 async_llm_engine.py:211] Added request 4.\n",
      "INFO 03-30 12:31:14 async_llm_engine.py:211] Added request 5.\n",
      "INFO 03-30 12:31:14 async_llm_engine.py:211] Added request 6.\n",
      "INFO 03-30 12:31:14 async_llm_engine.py:211] Added request 7.\n",
      "INFO 03-30 12:31:14 async_llm_engine.py:211] Added request 8.\n",
      "INFO 03-30 12:31:14 async_llm_engine.py:211] Added request 9.\n",
      "INFO 03-30 12:31:14 async_llm_engine.py:211] Added request 10.\n",
      "INFO 03-30 12:31:14 async_llm_engine.py:211] Added request 11.\n",
      "INFO 03-30 12:31:14 async_llm_engine.py:211] Added request 12.\n",
      "INFO 03-30 12:31:14 async_llm_engine.py:211] Added request 13.\n",
      "INFO 03-30 12:31:14 async_llm_engine.py:211] Added request 14.\n",
      "INFO 03-30 12:31:14 async_llm_engine.py:211] Added request 15.\n",
      "INFO 03-30 12:31:14 async_llm_engine.py:211] Added request 16.\n",
      "INFO 03-30 12:31:14 async_llm_engine.py:211] Added request 17.\n",
      "INFO 03-30 12:31:14 async_llm_engine.py:211] Added request 18.\n",
      "INFO 03-30 12:31:14 async_llm_engine.py:211] Added request 19.\n",
      "INFO 03-30 12:31:16 metrics.py:467] Avg prompt throughput: 470.7 tokens/s, Avg generation throughput: 0.1 tokens/s, Running: 11 reqs, Swapped: 0 reqs, Pending: 9 reqs, GPU KV cache usage: 6.2%, CPU KV cache usage: 0.0%.\n",
      "INFO 03-30 12:31:21 metrics.py:467] Avg prompt throughput: 14935.1 tokens/s, Avg generation throughput: 46.4 tokens/s, Running: 20 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 11.1%, CPU KV cache usage: 0.0%.\n",
      "\n",
      "Expert <class 'CheckInf.CheckInf'> started\n",
      "\n",
      "\n",
      "\n",
      "Expert <class 'CheckInf.CheckInf'> ended\n",
      "\n",
      "result: None\n",
      "\n",
      "\n",
      "INFO 03-30 12:31:26 metrics.py:467] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 776.6 tokens/s, Running: 20 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 11.6%, CPU KV cache usage: 0.0%.\n",
      "INFO 03-30 12:31:30 async_llm_engine.py:179] Finished request 14.\n",
      "INFO 03-30 12:31:31 metrics.py:467] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 771.5 tokens/s, Running: 19 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 11.4%, CPU KV cache usage: 0.0%.\n",
      "INFO 03-30 12:31:31 async_llm_engine.py:211] Added request 28.\n",
      "INFO 03-30 12:31:33 async_llm_engine.py:179] Finished request 7.\n",
      "\n",
      "Expert <class 'CollectCodeFileToModify.CollectCodeFileToModify'> started\n",
      "\n",
      "\n",
      "\n",
      "Expert <class 'CollectCodeFileToModify.CollectCodeFileToModify'> ended\n",
      "\n",
      "result: None\n",
      "\n",
      "\n",
      "\n",
      "Expert <class 'CheckInf.CheckInf'> started\n",
      "\n",
      "\n",
      "\n",
      "Expert <class 'CheckInf.CheckInf'> ended\n",
      "\n",
      "result: None\n",
      "\n",
      "\n",
      "INFO 03-30 12:31:34 async_llm_engine.py:211] Added request 27.\n",
      "INFO 03-30 12:31:36 metrics.py:467] Avg prompt throughput: 2254.1 tokens/s, Avg generation throughput: 626.5 tokens/s, Running: 20 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 12.1%, CPU KV cache usage: 0.0%.\n",
      "INFO 03-30 12:31:38 async_llm_engine.py:179] Finished request 18.\n",
      "INFO 03-30 12:31:39 async_llm_engine.py:211] Added request 26.\n",
      "INFO 03-30 12:31:40 async_llm_engine.py:179] Finished request 0.\n",
      "INFO 03-30 12:31:40 async_llm_engine.py:179] Finished request 16.\n",
      "INFO 03-30 12:31:41 metrics.py:467] Avg prompt throughput: 2047.4 tokens/s, Avg generation throughput: 608.5 tokens/s, Running: 18 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 11.7%, CPU KV cache usage: 0.0%.\n",
      "INFO 03-30 12:31:41 async_llm_engine.py:211] Added request 24.\n",
      "INFO 03-30 12:31:41 async_llm_engine.py:211] Added request 25.\n",
      "\n",
      "Expert <class 'CheckInf.CheckInf'> started\n",
      "\n",
      "\n",
      "\n",
      "Expert <class 'CheckInf.CheckInf'> ended\n",
      "\n",
      "result: None\n",
      "\n",
      "\n",
      "INFO 03-30 12:31:44 async_llm_engine.py:179] Finished request 28.\n",
      "INFO 03-30 12:31:45 async_llm_engine.py:211] Added request 23.\n",
      "INFO 03-30 12:31:46 metrics.py:467] Avg prompt throughput: 6118.3 tokens/s, Avg generation throughput: 389.6 tokens/s, Running: 20 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 15.0%, CPU KV cache usage: 0.0%.\n",
      "INFO 03-30 12:31:47 async_llm_engine.py:179] Finished request 3.\n",
      "INFO 03-30 12:31:47 async_llm_engine.py:179] Finished request 4.\n",
      "INFO 03-30 12:31:48 async_llm_engine.py:211] Added request 21.\n",
      "INFO 03-30 12:31:48 async_llm_engine.py:211] Added request 22.\n",
      "INFO 03-30 12:31:50 async_llm_engine.py:179] Finished request 6.\n",
      "INFO 03-30 12:31:51 metrics.py:467] Avg prompt throughput: 4094.7 tokens/s, Avg generation throughput: 485.0 tokens/s, Running: 19 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 15.5%, CPU KV cache usage: 0.0%.\n",
      "INFO 03-30 12:31:51 async_llm_engine.py:179] Finished request 17.\n",
      "INFO 03-30 12:31:51 async_llm_engine.py:211] Added request 20.\n",
      "\n",
      "Expert <class 'CollectCodeFileToModify.CollectCodeFileToModify'> started\n",
      "\n",
      "\n",
      "\n",
      "Expert <class 'CollectCodeFileToModify.CollectCodeFileToModify'> ended\n",
      "\n",
      "result: None\n",
      "\n",
      "\n",
      "\n",
      "Expert <class 'CheckInf.CheckInf'> started\n",
      "\n",
      "\n",
      "\n",
      "Expert <class 'CheckInf.CheckInf'> ended\n",
      "\n",
      "result: None\n",
      "\n",
      "\n",
      "INFO 03-30 12:31:56 metrics.py:467] Avg prompt throughput: 1557.4 tokens/s, Avg generation throughput: 592.5 tokens/s, Running: 19 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 16.0%, CPU KV cache usage: 0.0%.\n",
      "INFO 03-30 12:31:56 async_llm_engine.py:179] Finished request 1.\n",
      "INFO 03-30 12:32:00 async_llm_engine.py:179] Finished request 27.\n",
      "INFO 03-30 12:32:00 async_llm_engine.py:179] Finished request 13.\n",
      "INFO 03-30 12:32:01 async_llm_engine.py:179] Finished request 25.\n",
      "INFO 03-30 12:32:01 metrics.py:467] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 633.3 tokens/s, Running: 15 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 13.3%, CPU KV cache usage: 0.0%.\n",
      "INFO 03-30 12:32:02 async_llm_engine.py:179] Finished request 26.\n",
      "INFO 03-30 12:32:03 async_llm_engine.py:179] Finished request 24.\n",
      "\n",
      "Expert <class 'CheckInf.CheckInf'> started\n",
      "\n",
      "\n",
      "\n",
      "Expert <class 'CheckInf.CheckInf'> ended\n",
      "\n",
      "result: None\n",
      "\n",
      "\n",
      "INFO 03-30 12:32:05 async_llm_engine.py:179] Finished request 21.\n",
      "INFO 03-30 12:32:06 metrics.py:467] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 549.4 tokens/s, Running: 12 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 10.0%, CPU KV cache usage: 0.0%.\n",
      "INFO 03-30 12:32:07 async_llm_engine.py:179] Finished request 22.\n",
      "INFO 03-30 12:32:11 metrics.py:467] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 472.2 tokens/s, Running: 11 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 9.1%, CPU KV cache usage: 0.0%.\n",
      "INFO 03-30 12:32:11 async_llm_engine.py:179] Finished request 8.\n",
      "INFO 03-30 12:32:14 async_llm_engine.py:179] Finished request 20.\n",
      "\n",
      "Expert <class 'CollectCodeFileToModify.CollectCodeFileToModify'> started\n",
      "\n",
      "\n",
      "\n",
      "Expert <class 'CollectCodeFileToModify.CollectCodeFileToModify'> ended\n",
      "\n",
      "result: None\n",
      "\n",
      "\n",
      "\n",
      "Expert <class 'CheckInf.CheckInf'> started\n",
      "\n",
      "\n",
      "\n",
      "Expert <class 'CheckInf.CheckInf'> ended\n",
      "\n",
      "result: None\n",
      "\n",
      "\n",
      "INFO 03-30 12:32:16 metrics.py:467] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 439.4 tokens/s, Running: 9 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 7.2%, CPU KV cache usage: 0.0%.\n",
      "INFO 03-30 12:32:21 metrics.py:467] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 416.2 tokens/s, Running: 9 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 7.5%, CPU KV cache usage: 0.0%.\n",
      "INFO 03-30 12:32:21 async_llm_engine.py:179] Finished request 19.\n",
      "\n",
      "Expert <class 'CheckInf.CheckInf'> started\n",
      "\n",
      "\n",
      "\n",
      "Expert <class 'CheckInf.CheckInf'> ended\n",
      "\n",
      "result: None\n",
      "\n",
      "\n",
      "INFO 03-30 12:32:26 metrics.py:467] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 389.8 tokens/s, Running: 8 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 7.0%, CPU KV cache usage: 0.0%.\n",
      "INFO 03-30 12:32:31 metrics.py:467] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 382.7 tokens/s, Running: 8 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 7.2%, CPU KV cache usage: 0.0%.\n",
      "\n",
      "Expert <class 'CollectCodeFileToModify.CollectCodeFileToModify'> started\n",
      "\n",
      "\n",
      "\n",
      "Expert <class 'CollectCodeFileToModify.CollectCodeFileToModify'> ended\n",
      "\n",
      "result: None\n",
      "\n",
      "\n",
      "\n",
      "Expert <class 'CheckInf.CheckInf'> started\n",
      "\n",
      "\n",
      "\n",
      "Expert <class 'CheckInf.CheckInf'> ended\n",
      "\n",
      "result: None\n",
      "\n",
      "\n",
      "INFO 03-30 12:32:36 metrics.py:467] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 381.2 tokens/s, Running: 8 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 7.4%, CPU KV cache usage: 0.0%.\n",
      "INFO 03-30 12:32:41 metrics.py:467] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 378.4 tokens/s, Running: 8 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 7.6%, CPU KV cache usage: 0.0%.\n",
      "\n",
      "Expert <class 'CheckInf.CheckInf'> started\n",
      "\n",
      "\n",
      "\n",
      "Expert <class 'CheckInf.CheckInf'> ended\n",
      "\n",
      "result: None\n",
      "\n",
      "\n",
      "INFO 03-30 12:32:46 metrics.py:467] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 374.6 tokens/s, Running: 8 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 7.8%, CPU KV cache usage: 0.0%.\n",
      "INFO 03-30 12:32:51 metrics.py:467] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 372.2 tokens/s, Running: 8 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 8.0%, CPU KV cache usage: 0.0%.\n",
      "\n",
      "Expert <class 'CollectCodeFileToModify.CollectCodeFileToModify'> started\n",
      "\n",
      "\n",
      "\n",
      "Expert <class 'CollectCodeFileToModify.CollectCodeFileToModify'> ended\n",
      "\n",
      "result: None\n",
      "\n",
      "\n",
      "\n",
      "Expert <class 'CheckInf.CheckInf'> started\n",
      "\n",
      "\n",
      "\n",
      "Expert <class 'CheckInf.CheckInf'> ended\n",
      "\n",
      "result: None\n",
      "\n",
      "\n",
      "INFO 03-30 12:32:56 metrics.py:467] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 368.3 tokens/s, Running: 8 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 8.2%, CPU KV cache usage: 0.0%.\n",
      "INFO 03-30 12:33:01 metrics.py:467] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 367.6 tokens/s, Running: 8 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 8.4%, CPU KV cache usage: 0.0%.\n",
      "\n",
      "Expert <class 'CheckInf.CheckInf'> started\n",
      "\n",
      "\n",
      "\n",
      "Expert <class 'CheckInf.CheckInf'> ended\n",
      "\n",
      "result: None\n",
      "\n",
      "\n",
      "INFO 03-30 12:33:06 metrics.py:467] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 365.0 tokens/s, Running: 8 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 8.6%, CPU KV cache usage: 0.0%.\n",
      "INFO 03-30 12:33:11 metrics.py:467] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 362.5 tokens/s, Running: 8 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 8.8%, CPU KV cache usage: 0.0%.\n",
      "INFO 03-30 12:33:13 async_llm_engine.py:179] Finished request 2.\n",
      "\n",
      "Expert <class 'CollectCodeFileToModify.CollectCodeFileToModify'> started\n",
      "\n",
      "\n",
      "\n",
      "Expert <class 'CollectCodeFileToModify.CollectCodeFileToModify'> ended\n",
      "\n",
      "result: None\n",
      "\n",
      "\n",
      "\n",
      "Expert <class 'CheckInf.CheckInf'> started\n",
      "\n",
      "\n",
      "\n",
      "Expert <class 'CheckInf.CheckInf'> ended\n",
      "\n",
      "result: None\n",
      "\n",
      "\n",
      "INFO 03-30 12:33:16 metrics.py:467] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 339.8 tokens/s, Running: 7 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 8.0%, CPU KV cache usage: 0.0%.\n",
      "INFO 03-30 12:33:21 metrics.py:467] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 322.8 tokens/s, Running: 7 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 8.2%, CPU KV cache usage: 0.0%.\n",
      "\n",
      "Expert <class 'CheckInf.CheckInf'> started\n",
      "\n",
      "\n",
      "\n",
      "Expert <class 'CheckInf.CheckInf'> ended\n",
      "\n",
      "result: None\n",
      "\n",
      "\n",
      "INFO 03-30 12:33:26 metrics.py:467] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 319.9 tokens/s, Running: 7 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 8.4%, CPU KV cache usage: 0.0%.\n",
      "INFO 03-30 12:33:31 metrics.py:467] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 317.8 tokens/s, Running: 7 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 8.5%, CPU KV cache usage: 0.0%.\n",
      "INFO 03-30 12:33:34 async_llm_engine.py:179] Finished request 10.\n",
      "INFO 03-30 12:33:34 async_llm_engine.py:179] Finished request 12.\n",
      "\n",
      "Expert <class 'CollectCodeFileToModify.CollectCodeFileToModify'> started\n",
      "\n",
      "\n",
      "\n",
      "Expert <class 'CollectCodeFileToModify.CollectCodeFileToModify'> ended\n",
      "\n",
      "result: None\n",
      "\n",
      "\n",
      "\n",
      "Expert <class 'CheckInf.CheckInf'> started\n",
      "\n",
      "\n",
      "\n",
      "Expert <class 'CheckInf.CheckInf'> ended\n",
      "\n",
      "result: None\n",
      "\n",
      "\n",
      "INFO 03-30 12:33:36 metrics.py:467] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 283.0 tokens/s, Running: 5 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 6.3%, CPU KV cache usage: 0.0%.\n",
      "INFO 03-30 12:33:37 async_llm_engine.py:179] Finished request 9.\n",
      "INFO 03-30 12:33:41 metrics.py:467] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 220.3 tokens/s, Running: 4 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 4.9%, CPU KV cache usage: 0.0%.\n",
      "\n",
      "Expert <class 'CheckInf.CheckInf'> started\n",
      "\n",
      "\n",
      "\n",
      "Expert <class 'CheckInf.CheckInf'> ended\n",
      "\n",
      "result: None\n",
      "\n",
      "\n",
      "INFO 03-30 12:33:46 metrics.py:467] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 211.0 tokens/s, Running: 4 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 5.1%, CPU KV cache usage: 0.0%.\n",
      "INFO 03-30 12:33:47 async_llm_engine.py:179] Finished request 15.\n",
      "INFO 03-30 12:33:51 metrics.py:467] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 174.4 tokens/s, Running: 3 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 4.0%, CPU KV cache usage: 0.0%.\n",
      "\n",
      "Expert <class 'CollectCodeFileToModify.CollectCodeFileToModify'> started\n",
      "\n",
      "\n",
      "\n",
      "Expert <class 'CollectCodeFileToModify.CollectCodeFileToModify'> ended\n",
      "\n",
      "result: None\n",
      "\n",
      "\n",
      "\n",
      "Expert <class 'CheckInf.CheckInf'> started\n",
      "\n",
      "\n",
      "\n",
      "Expert <class 'CheckInf.CheckInf'> ended\n",
      "\n",
      "result: None\n",
      "\n",
      "\n",
      "INFO 03-30 12:33:56 metrics.py:467] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 164.9 tokens/s, Running: 3 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 4.1%, CPU KV cache usage: 0.0%.\n",
      "INFO 03-30 12:33:57 async_llm_engine.py:179] Finished request 23.\n",
      "INFO 03-30 12:34:01 metrics.py:467] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 129.1 tokens/s, Running: 2 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 2.4%, CPU KV cache usage: 0.0%.\n",
      "\n",
      "Expert <class 'CheckInf.CheckInf'> started\n",
      "\n",
      "\n",
      "\n",
      "Expert <class 'CheckInf.CheckInf'> ended\n",
      "\n",
      "result: None\n",
      "\n",
      "\n",
      "INFO 03-30 12:34:06 metrics.py:467] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 117.7 tokens/s, Running: 2 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 2.4%, CPU KV cache usage: 0.0%.\n",
      "INFO 03-30 12:34:11 metrics.py:467] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 117.2 tokens/s, Running: 2 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 2.5%, CPU KV cache usage: 0.0%.\n",
      "\n",
      "Expert <class 'CollectCodeFileToModify.CollectCodeFileToModify'> started\n",
      "\n",
      "\n",
      "\n",
      "Expert <class 'CollectCodeFileToModify.CollectCodeFileToModify'> ended\n",
      "\n",
      "result: None\n",
      "\n",
      "\n",
      "\n",
      "Expert <class 'CheckInf.CheckInf'> started\n",
      "\n",
      "\n",
      "\n",
      "Expert <class 'CheckInf.CheckInf'> ended\n",
      "\n",
      "result: None\n",
      "\n",
      "\n",
      "INFO 03-30 12:34:16 metrics.py:467] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 117.4 tokens/s, Running: 2 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 2.5%, CPU KV cache usage: 0.0%.\n",
      "INFO 03-30 12:34:21 metrics.py:467] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 117.3 tokens/s, Running: 2 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 2.6%, CPU KV cache usage: 0.0%.\n",
      "\n",
      "Expert <class 'CheckInf.CheckInf'> started\n",
      "\n",
      "\n",
      "\n",
      "Expert <class 'CheckInf.CheckInf'> ended\n",
      "\n",
      "result: None\n",
      "\n",
      "\n",
      "INFO 03-30 12:34:26 metrics.py:467] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 117.0 tokens/s, Running: 2 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 2.7%, CPU KV cache usage: 0.0%.\n",
      "INFO 03-30 12:34:31 metrics.py:467] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 116.2 tokens/s, Running: 2 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 2.7%, CPU KV cache usage: 0.0%.\n",
      "\n",
      "Expert <class 'CollectCodeFileToModify.CollectCodeFileToModify'> started\n",
      "\n",
      "\n",
      "\n",
      "Expert <class 'CollectCodeFileToModify.CollectCodeFileToModify'> ended\n",
      "\n",
      "result: None\n",
      "\n",
      "\n",
      "\n",
      "Expert <class 'CheckInf.CheckInf'> started\n",
      "\n",
      "\n",
      "\n",
      "Expert <class 'CheckInf.CheckInf'> ended\n",
      "\n",
      "result: None\n",
      "\n",
      "\n",
      "INFO 03-30 12:34:36 metrics.py:467] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 115.9 tokens/s, Running: 2 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 2.8%, CPU KV cache usage: 0.0%.\n",
      "INFO 03-30 12:34:41 metrics.py:467] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 115.1 tokens/s, Running: 2 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 2.9%, CPU KV cache usage: 0.0%.\n",
      "\n",
      "Expert <class 'CheckInf.CheckInf'> started\n",
      "\n",
      "\n",
      "\n",
      "Expert <class 'CheckInf.CheckInf'> ended\n",
      "\n",
      "result: None\n",
      "\n",
      "\n",
      "INFO 03-30 12:34:46 metrics.py:467] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 115.0 tokens/s, Running: 2 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 2.9%, CPU KV cache usage: 0.0%.\n",
      "INFO 03-30 12:34:51 metrics.py:467] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 114.6 tokens/s, Running: 2 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 3.0%, CPU KV cache usage: 0.0%.\n",
      "INFO 03-30 12:34:53 async_llm_engine.py:179] Finished request 5.\n",
      "INFO 03-30 12:34:53 async_llm_engine.py:179] Finished request 11.\n",
      "survey_paths_list:  [{1: './database/gkv-code/README.md', 2: './database/gkv-code/README_for_namelist.txt', 3: './database/gkv-code/Version_memo.txt', 4: './database/gkv-code/benchmarks/Benchmarks_Note.md'}, {2: './database/gkv-code/extra_tools/backup/performance_analysis/calc_scaling.sh', 3: './database/gkv-code/extra_tools/backup/performance_analysis/fujitsu_fapppx.sh', 4: './database/gkv-code/extra_tools/backup/performance_analysis/fujitsu_fipppx.sh'}, {0: './database/gkv-code/extra_tools/backup/performance_analysis/plot_scaling.gn', 1: './database/gkv-code/extra_tools/backup/performance_analysis/v08create_rankmap.f90', 2: './database/gkv-code/extra_tools/backup/performance_analysis/v08create_rankmap.py', 3: './database/gkv-code/extra_tools/backup/scripts/chparam.sh', 4: './database/gkv-code/extra_tools/backup/scripts/cp_for_ch_res.sh'}, {0: './database/gkv-code/extra_tools/backup/scripts/reduction/reduction.sh', 1: './database/gkv-code/extra_tools/backup/scripts/reduction/reduction_1d.awk', 2: './database/gkv-code/extra_tools/backup/scripts/reduction/reduction_xy2x.awk', 3: './database/gkv-code/extra_tools/backup/scripts/reduction/reduction_xy2y.awk', 4: './database/gkv-code/extra_tools/backup/scripts/reduction/timeaverage.awk'}, {0: './database/gkv-code/extra_tools/backup/scripts/rename.sh', 1: './database/gkv-code/extra_tools/backup/scripts/scan_Nref.sh', 2: './database/gkv-code/extra_tools/backup/scripts/scan_ky.sh', 3: './database/gkv-code/extra_tools/backup/scripts/scan_shatbeta.sh', 4: './database/gkv-code/extra_tools/backup/scripts/time_average.awk'}, {}, {0: './database/gkv-code/lib/Bessel0_Zeros.f90', 1: './database/gkv-code/lib/gkvp_math_MATRIX.f90', 2: './database/gkv-code/lib/gkvp_math_MKLNAG.f90', 3: './database/gkv-code/lib/gkvp_math_SSL2.f90', 4: './database/gkv-code/lib/gkvp_math_portable.f90'}, {0: './database/gkv-code/lib/sample_bessel/sample_bessel_SSL2.dat', 1: './database/gkv-code/lib/sample_bessel/sample_bessel_portable.dat', 3: './database/gkv-code/run/Makefile', 4: './database/gkv-code/run/backup/Makefile_flow'}, {0: './database/gkv-code/run/backup/Makefile_fugaku', 1: './database/gkv-code/run/backup/Makefile_fugaku_old', 2: './database/gkv-code/run/backup/Makefile_ito', 3: './database/gkv-code/run/backup/Makefile_jfrs', 4: './database/gkv-code/run/backup/Makefile_p-srv'}, {0: './database/gkv-code/run/backup/Makefile_ps_sx', 1: './database/gkv-code/run/backup/Makefile_ubuntu', 2: './database/gkv-code/run/backup/Makefile_ubuntu_old', 3: './database/gkv-code/run/backup/benchmarks/gkvp_namelist_ITGae-lin', 4: './database/gkv-code/run/backup/benchmarks/gkvp_namelist_ITGke-lin'}, {0: './database/gkv-code/run/backup/benchmarks/gkvp_namelist_SmallGridIO', 1: './database/gkv-code/run/backup/gkvp_namelist_1sp', 2: './database/gkv-code/run/backup/gkvp_namelist_2sp', 3: './database/gkv-code/run/backup/gkvp_namelist_3sp', 4: './database/gkv-code/run/backup/gkvp_namelist_4sp'}, {0: './database/gkv-code/run/backup/old/Makefile_bx', 1: './database/gkv-code/run/backup/old/Makefile_fx10', 2: './database/gkv-code/run/backup/old/Makefile_helios', 3: './database/gkv-code/run/backup/old/Makefile_k', 4: './database/gkv-code/run/backup/old/Makefile_mac'}, {0: './database/gkv-code/run/backup/old/Makefile_nu_fx100', 1: './database/gkv-code/run/backup/old/Makefile_oakleaf', 2: './database/gkv-code/run/backup/old/Makefile_ofp', 3: './database/gkv-code/run/backup/old/Makefile_ps_fx100', 4: './database/gkv-code/run/backup/old/compile.sh'}, {0: './database/gkv-code/run/backup/old/gkvp_namelist_k', 1: './database/gkv-code/run/backup/old/go.sh', 2: './database/gkv-code/run/backup/old/shoot_helios', 3: './database/gkv-code/run/backup/old/shoot_k', 4: './database/gkv-code/run/backup/old/shoot_mac'}, {0: './database/gkv-code/run/backup/old/shoot_nu_fx100', 1: './database/gkv-code/run/backup/old/shoot_oakleaf', 2: './database/gkv-code/run/backup/old/shoot_ofp', 3: './database/gkv-code/run/backup/old/shoot_old', 4: './database/gkv-code/run/backup/old/shoot_ps_fx100'}, {0: './database/gkv-code/run/backup/old/sub.q_bx', 1: './database/gkv-code/run/backup/old/sub.q_cnt_k', 2: './database/gkv-code/run/backup/old/sub.q_fx10', 3: './database/gkv-code/run/backup/old/sub.q_helios', 4: './database/gkv-code/run/backup/old/sub.q_init_k'}, {0: './database/gkv-code/run/backup/old/sub.q_mac', 1: './database/gkv-code/run/backup/old/sub.q_nu_fx100', 2: './database/gkv-code/run/backup/old/sub.q_oakleaf', 3: './database/gkv-code/run/backup/old/sub.q_ofp', 4: './database/gkv-code/run/backup/old/sub.q_ps_fx100'}, {0: './database/gkv-code/run/backup/shoot_flow', 1: './database/gkv-code/run/backup/shoot_fugaku', 2: './database/gkv-code/run/backup/shoot_ito', 3: './database/gkv-code/run/backup/shoot_jfrs', 4: './database/gkv-code/run/backup/shoot_jfrs_CrayPAT'}, {0: './database/gkv-code/run/backup/shoot_p-srv', 1: './database/gkv-code/run/backup/shoot_ps_sx', 2: './database/gkv-code/run/backup/shoot_ubuntu', 3: './database/gkv-code/run/backup/sub.q_flow', 4: './database/gkv-code/run/backup/sub.q_fugaku'}, {0: './database/gkv-code/run/backup/sub.q_ito', 1: './database/gkv-code/run/backup/sub.q_jfrs', 2: './database/gkv-code/run/backup/sub.q_p-srv', 3: './database/gkv-code/run/backup/sub.q_ps_sx', 4: './database/gkv-code/run/backup/sub.q_ubuntu'}, {0: './database/gkv-code/run/gkvp_namelist', 1: './database/gkv-code/run/shoot', 2: './database/gkv-code/run/sub.q', 3: './database/gkv-code/src/.ipynb_checkpoints/gkvp_advnc-checkpoint.f90', 4: './database/gkv-code/src/.ipynb_checkpoints/gkvp_bndry-checkpoint.f90'}, {0: './database/gkv-code/src/.ipynb_checkpoints/gkvp_clock-checkpoint.f90', 1: './database/gkv-code/src/.ipynb_checkpoints/gkvp_freq-checkpoint.f90', 2: './database/gkv-code/src/.ipynb_checkpoints/gkvp_main-checkpoint.f90', 3: './database/gkv-code/src/gkvp_advnc.f90', 4: './database/gkv-code/src/gkvp_bndry.f90'}, {0: './database/gkv-code/src/gkvp_clock.f90', 1: './database/gkv-code/src/gkvp_colli.f90', 2: './database/gkv-code/src/gkvp_colliimp.f90', 3: './database/gkv-code/src/gkvp_dtc.f90', 4: './database/gkv-code/src/gkvp_exb.f90'}, {0: './database/gkv-code/src/gkvp_f0.56_advnc_tune_nec1.f90', 1: './database/gkv-code/src/gkvp_f0.56_bndry_tune_nec1.f90', 2: './database/gkv-code/src/gkvp_f0.56_colli_tune_nifs.f90', 3: './database/gkv-code/src/gkvp_f0.56_exb_tune2r_0813.f90', 4: './database/gkv-code/src/gkvp_f0.56_fft_fftw_tune2r_0813.f90'}, {0: './database/gkv-code/src/gkvp_f0.56_zfilter_tune_nec1.f90', 1: './database/gkv-code/src/gkvp_fft_fftw.f90', 2: './database/gkv-code/src/gkvp_fileio_fortran.f90', 3: './database/gkv-code/src/gkvp_fileio_netcdf.f90', 4: './database/gkv-code/src/gkvp_fld.f90'}, {0: './database/gkv-code/src/gkvp_freq.f90', 1: './database/gkv-code/src/gkvp_geom.f90', 2: './database/gkv-code/src/gkvp_header.f90', 3: './database/gkv-code/src/gkvp_igs.f90', 4: './database/gkv-code/src/gkvp_intgrl.f90'}, {0: './database/gkv-code/src/gkvp_main.f90', 1: './database/gkv-code/src/gkvp_mpienv.f90', 2: './database/gkv-code/src/gkvp_out.f90', 3: './database/gkv-code/src/gkvp_ring.f90', 4: './database/gkv-code/src/gkvp_set.f90'}, {0: './database/gkv-code/src/gkvp_shearflow.f90', 1: './database/gkv-code/src/gkvp_tips.f90', 2: './database/gkv-code/src/gkvp_trans.f90', 3: './database/gkv-code/src/gkvp_vmecbzx.f90', 4: './database/gkv-code/src/gkvp_vmecin.f90'}, {0: './database/gkv-code/src/gkvp_zfilter.f90'}]\n",
      "len(survey_paths_list):  29\n",
      "i:  0\n",
      "i:  1\n",
      "i:  2\n",
      "i:  3\n",
      "i:  4\n",
      "i:  5\n",
      "i:  6\n",
      "i:  7\n",
      "i:  8\n",
      "i:  9\n",
      "i:  10\n",
      "i:  11\n",
      "i:  12\n",
      "i:  13\n",
      "i:  14\n",
      "i:  15\n",
      "i:  16\n",
      "i:  17\n",
      "i:  18\n",
      "i:  19\n",
      "i:  20\n",
      "i:  21\n",
      "i:  22\n",
      "i:  23\n",
      "i:  24\n",
      "i:  25\n",
      "i:  26\n",
      "i:  27\n",
      "i:  28\n",
      "\n",
      "\n",
      "Expert <class 'QuickSummary.QuickSummary'> ended\n",
      "\n",
      "result: [{'file id': '1', 'summary': 'GyroKinetic Vlasov simulation code: GKV, including details on code development, performance, and usage.', 'file_path': './database/gkv-code/README.md'}, {'file id': '2', 'summary': 'Changelog and update log for the GKV code, detailing modifications and improvements.', 'file_path': './database/gkv-code/README_for_namelist.txt'}, {'file id': '3', 'summary': 'Changelog and update log for the GKV code, focusing on specific updates and modifications.', 'file_path': './database/gkv-code/Version_memo.txt'}, {'file id': '4', 'summary': 'Directory containing benchmark results for the GKV code.', 'file_path': './database/gkv-code/benchmarks/Benchmarks_Note.md'}, {'file id': 2, 'summary': 'Processes log files to generate a scaling.dat file for performance analysis of MPI applications.', 'file_path': './database/gkv-code/extra_tools/backup/performance_analysis/calc_scaling.sh'}, {'file id': '0', 'summary': 'Analyzes strong scaling behavior of a parallel simulation on a specific system configuration.', 'file_path': './database/gkv-code/extra_tools/backup/performance_analysis/plot_scaling.gn'}, {'file id': '1', 'summary': 'Automates the process of mapping processes to a 3D torus network for a parallel simulation.', 'file_path': './database/gkv-code/extra_tools/backup/performance_analysis/v08create_rankmap.f90'}, {'file id': '2', 'summary': 'Automates the process of mapping processes to a 3D torus network for a parallel simulation using Python.', 'file_path': './database/gkv-code/extra_tools/backup/performance_analysis/v08create_rankmap.py'}, {'file id': '3', 'summary': 'Automates the process of changing run directory and parameters for a parallel simulation.', 'file_path': './database/gkv-code/extra_tools/backup/scripts/chparam.sh'}, {'file id': '4', 'summary': 'Automates the process of renaming and copying files in a directory for data processing or backup.', 'file_path': './database/gkv-code/extra_tools/backup/scripts/cp_for_ch_res.sh'}, {'file id': 0, 'summary': 'Script for setting up simulation parameters and performing data reduction and averaging on 2D data files.', 'file_path': './database/gkv-code/extra_tools/backup/scripts/reduction/reduction.sh'}, {'file id': 1, 'summary': '`awk` script to sum up 1D data from a file.', 'file_path': './database/gkv-code/extra_tools/backup/scripts/reduction/reduction_1d.awk'}, {'file id': 2, 'summary': '`awk` script to sum up 2D data along the second column from a file.', 'file_path': './database/gkv-code/extra_tools/backup/scripts/reduction/reduction_xy2x.awk'}, {'file id': 3, 'summary': '`awk` script to sum up 2D data along the first column from a file.', 'file_path': './database/gkv-code/extra_tools/backup/scripts/reduction/reduction_xy2y.awk'}, {'file id': 4, 'summary': '`awk` script to average data over multiple input files.', 'file_path': './database/gkv-code/extra_tools/backup/scripts/reduction/timeaverage.awk'}, {'file id': 0, 'summary': 'Shell script for renaming and updating files in multiple directories.', 'file_path': './database/gkv-code/extra_tools/backup/scripts/rename.sh'}, {'file id': 1, 'summary': 'Shell script for parameter scanning and simulation execution for Nref and ky.', 'file_path': './database/gkv-code/extra_tools/backup/scripts/scan_Nref.sh'}, {'file id': 2, 'summary': 'Shell script for parameter scanning and simulation execution for ky and kymin.', 'file_path': './database/gkv-code/extra_tools/backup/scripts/scan_ky.sh'}, {'file id': 3, 'summary': 'Shell script for parameter scanning and simulation execution for beta and shat.', 'file_path': './database/gkv-code/extra_tools/backup/scripts/scan_shatbeta.sh'}, {'file id': 4, 'summary': 'Shell script for calculating time-averaged values of 1D data.', 'file_path': './database/gkv-code/extra_tools/backup/scripts/time_average.awk'}, {'file id': 0, 'summary': 'Contains information about the zeros of the 0th-order Bessel function', 'file_path': './database/gkv-code/lib/Bessel0_Zeros.f90'}, {'file id': 1, 'summary': 'Contains mathematical functions using the MATRIX/MPP library', 'file_path': './database/gkv-code/lib/gkvp_math_MATRIX.f90'}, {'file id': 2, 'summary': 'Contains mathematical functions using the SSLII library', 'file_path': './database/gkv-code/lib/gkvp_math_MKLNAG.f90'}, {'file id': 3, 'summary': 'Contains mathematical functions using the SSLII library', 'file_path': './database/gkv-code/lib/gkvp_math_SSL2.f90'}, {'file id': 4, 'summary': 'Contains mathematical functions using the MATRIX/MPP library, including additional elliptic integrals', 'file_path': './database/gkv-code/lib/gkvp_math_portable.f90'}, {'file id': 0, 'summary': 'Tests Bessel functions using MPI and writes results to a file.', 'file_path': './database/gkv-code/lib/sample_bessel/sample_bessel_SSL2.dat'}, {'file id': 1, 'summary': 'Tests Bessel functions using MPI and writes results to a file.', 'file_path': './database/gkv-code/lib/sample_bessel/sample_bessel_portable.dat'}, {'file id': 3, 'summary': 'Compiles a scientific application using the Fujitsu Fortran Compiler with optimization and parallel processing.', 'file_path': './database/gkv-code/run/Makefile'}, {'file id': 4, 'summary': 'Compiles a scientific application using the Fujitsu Fortran Compiler with optimization and parallel processing.', 'file_path': './database/gkv-code/run/backup/Makefile_flow'}, {'file id': '0', 'summary': \"This file head is for a Fortran compiler (likely Fujitsu) to compile a program named 'gkvp.exe'. It includes settings for optimization, OpenMP, and other compiler options, and specifies the source files and libraries needed to build the program.\", 'file_path': './database/gkv-code/run/backup/Makefile_fugaku'}, {'file id': 1, 'summary': 'Compiles and links Fortran source files to create the gkvp executable using Open-MPI and gfortran.', 'file_path': './database/gkv-code/run/backup/Makefile_ubuntu'}, {'file id': 2, 'summary': 'Compiles and links Fortran source files to create the gkvp executable using Open-MPI and gfortran.', 'file_path': './database/gkv-code/run/backup/Makefile_ubuntu_old'}, {'file id': 0, 'summary': 'Simulation of magnetized plasma using GKV-plus code with nonlinear calculations and s-alpha equilibrium model.', 'file_path': './database/gkv-code/run/backup/benchmarks/gkvp_namelist_SmallGridIO'}, {'file id': 1, 'summary': 'Simulation of magnetized plasma using GKV-plus code with nonlinear calculations and eqdsk equilibrium model.', 'file_path': './database/gkv-code/run/backup/gkvp_namelist_1sp'}, {'file id': 2, 'summary': 'Simulation of magnetized plasma using GKV-plus code with linear frequency calculations and s-alpha equilibrium model.', 'file_path': './database/gkv-code/run/backup/gkvp_namelist_2sp'}, {'file id': 3, 'summary': 'Simulation of magnetized plasma using GKV-plus code with nonlinear calculations and eqdsk equilibrium model.', 'file_path': './database/gkv-code/run/backup/gkvp_namelist_3sp'}, {'file id': 4, 'summary': 'Simulation of magnetized plasma using GKV-plus code with linear frequency calculations and s-alpha equilibrium model.', 'file_path': './database/gkv-code/run/backup/gkvp_namelist_4sp'}, {'file id': '0', 'summary': 'Compiles a set of Fortran source files into the gkvp_mpifft.exe executable using MPI and FFTW for parallel and fast Fourier transform operations.', 'file_path': './database/gkv-code/run/backup/old/Makefile_bx'}, {'file id': '1', 'summary': 'Compiles a set of Fortran source files into the gkvp_mpifft.exe executable using MPI and FFTW for parallel and fast Fourier transform operations.', 'file_path': './database/gkv-code/run/backup/old/Makefile_fx10'}, {'file id': '2', 'summary': 'Compiles a set of Fortran source files into the gkvp_mpifft.exe executable using MPI, Intel MKL, and FFTW for parallel and fast Fourier transform operations.', 'file_path': './database/gkv-code/run/backup/old/Makefile_helios'}, {'file id': '3', 'summary': 'Compiles a set of Fortran source files into the gkvp_mpifft.exe executable using MPI and FFTW for parallel and fast Fourier transform operations.', 'file_path': './database/gkv-code/run/backup/old/Makefile_k'}, {'file id': '4', 'summary': 'Compiles a set of Fortran source files into the gkvp_mpifft.exe executable using MPI and FFTW for parallel and fast Fourier transform operations.', 'file_path': './database/gkv-code/run/backup/old/Makefile_mac'}, {'file id': 0, 'summary': 'Compiles a Fortran program for a numerical application using Intel Fortran Compiler with FFTW library.', 'file_path': './database/gkv-code/run/backup/old/Makefile_nu_fx100'}, {'file id': 1, 'summary': 'Compiles a Fortran program for a numerical application using Intel Fortran Compiler with FFTW library.', 'file_path': './database/gkv-code/run/backup/old/Makefile_oakleaf'}, {'file id': 2, 'summary': 'Compiles a Fortran program for a numerical application using Intel Fortran Compiler with FFTW library.', 'file_path': './database/gkv-code/run/backup/old/Makefile_ofp'}, {'file id': 3, 'summary': 'Compiles a Fortran program for a numerical application using Fujitsu Fortran Compiler with FFTW library.', 'file_path': './database/gkv-code/run/backup/old/Makefile_ps_fx100'}, {'file id': 4, 'summary': \"A shell script that runs the 'make clean' command to clean up previous build artifacts.\", 'file_path': './database/gkv-code/run/backup/old/compile.sh'}, {'file id': '0', 'summary': 'Namelist file for a computational simulation defining various parameters and settings.', 'file_path': './database/gkv-code/run/backup/old/gkvp_namelist_k'}, {'file id': '1', 'summary': 'Shell script for setting up the environment and submitting jobs for a computational simulation on an HPC system.', 'file_path': './database/gkv-code/run/backup/old/go.sh'}, {'file id': '2', 'summary': 'Shell script for setting up the environment and submitting jobs for a step-by-step computational simulation on an HPC system.', 'file_path': './database/gkv-code/run/backup/old/shoot_helios'}, {'file id': '3', 'summary': 'Shell script for setting up the environment and submitting jobs for a step-by-step computational simulation on an HPC system, handling continuation of runs after a specific job ID.', 'file_path': './database/gkv-code/run/backup/old/shoot_k'}, {'file id': '0', 'summary': 'Script for submitting sequential jobs in an HPC environment, setting up directories and environment variables for job execution.', 'file_path': './database/gkv-code/run/backup/old/shoot_nu_fx100'}, {'file id': '1', 'summary': 'Script for submitting sequential jobs in an HPC environment, setting up directories and environment variables for job execution.', 'file_path': './database/gkv-code/run/backup/old/shoot_oakleaf'}, {'file id': '2', 'summary': 'Script for submitting sequential jobs in an HPC environment, setting up directories and environment variables for job execution.', 'file_path': './database/gkv-code/run/backup/old/shoot_ofp'}, {'file id': '3', 'summary': 'Script for submitting sequential jobs in an HPC environment, setting up directories and environment variables for job execution.', 'file_path': './database/gkv-code/run/backup/old/shoot_old'}, {'file id': '4', 'summary': 'Script for submitting sequential jobs in an HPC environment, setting up directories and environment variables for job execution.', 'file_path': './database/gkv-code/run/backup/old/shoot_ps_fx100'}, {'file id': '0', 'summary': 'Job submission script for a parallel MPI job on a high-performance computing cluster, setting up process and thread numbers, queue, and other parameters.', 'file_path': './database/gkv-code/run/backup/old/sub.q_bx'}, {'file id': '0', 'summary': 'Shell script setting environment variables for OpenMP threading and parallelization.', 'file_path': './database/gkv-code/run/backup/old/sub.q_mac'}, {'file id': '1', 'summary': 'C shell script setting environment variables for OpenMP threading and parallelization.', 'file_path': './database/gkv-code/run/backup/old/sub.q_nu_fx100'}, {'file id': '2', 'summary': 'C shell script setting environment variables for OpenMP threading and parallelization.', 'file_path': './database/gkv-code/run/backup/old/sub.q_oakleaf'}, {'file id': '3', 'summary': 'C shell script setting environment variables for OpenMP threading and parallelization.', 'file_path': './database/gkv-code/run/backup/old/sub.q_ofp'}, {'file id': '0', 'summary': 'Job submission script for a computational simulation, handling directory creation, file copying, and job submission with continuation and tracing options.', 'file_path': './database/gkv-code/run/backup/shoot_flow'}, {'file id': '1', 'summary': 'Job submission script for a computational simulation, handling directory creation, file copying, and job submission with continuation and tracing options.', 'file_path': './database/gkv-code/run/backup/shoot_fugaku'}, {'file id': '2', 'summary': 'Job submission script for a computational simulation, handling directory creation, file copying, and job submission with continuation and tracing options.', 'file_path': './database/gkv-code/run/backup/shoot_ito'}, {'file id': '3', 'summary': 'Job submission script for a computational simulation, handling directory creation, file copying, and job submission with continuation and tracing options.', 'file_path': './database/gkv-code/run/backup/shoot_jfrs'}, {'file id': '4', 'summary': 'Job submission script for a computational simulation, handling directory creation, file copying, and job submission with continuation and tracing options.', 'file_path': './database/gkv-code/run/backup/shoot_jfrs_CrayPAT'}, {'file id': '0', 'summary': 'A script for submitting jobs in a step-by-step manner using qsub on Fujitsu PRIMEHPC FX1000.', 'file_path': './database/gkv-code/run/backup/shoot_p-srv'}, {'file id': '1', 'summary': 'A script for submitting jobs in a step-by-step manner using qsub on Fujitsu PRIMEHPC FX1000, supporting continuation of simulation runs.', 'file_path': './database/gkv-code/run/backup/shoot_ps_sx'}, {'file id': '2', 'summary': 'A script for submitting jobs in a step-by-step manner using qsub on Fujitsu PRIMEHPC FX1000, not supporting continuation of simulation runs.', 'file_path': './database/gkv-code/run/backup/shoot_ubuntu'}, {'file id': '3', 'summary': 'A script for running a job using mpiexec on Fujitsu PRIMEHPC FX1000.', 'file_path': './database/gkv-code/run/backup/sub.q_flow'}, {'file id': '4', 'summary': 'A script for running a job using mpiexec on Fujitsu Fugaku.', 'file_path': './database/gkv-code/run/backup/sub.q_fugaku'}, {'file id': '0', 'summary': 'Batch job submission script for an HPC system (ITO supercomputer)', 'file_path': './database/gkv-code/run/backup/sub.q_ito'}, {'file id': '1', 'summary': 'Batch job submission script for an HPC system (JFRS-1)', 'file_path': './database/gkv-code/run/backup/sub.q_jfrs'}, {'file id': '2', 'summary': 'Batch job submission script for an HPC system (NEC SX-Aurora TSUBASA)', 'file_path': './database/gkv-code/run/backup/sub.q_p-srv'}, {'file id': '3', 'summary': 'Batch job submission script for an HPC system (ITO supercomputer)', 'file_path': './database/gkv-code/run/backup/sub.q_ps_sx'}, {'file id': 0, 'summary': 'Namelist file for the GKV-plus code, containing parameters for linear frequency calculation, triad interactions, equilibrium, and time management.', 'file_path': './database/gkv-code/run/gkvp_namelist'}, {'file id': 1, 'summary': 'Shell script for job submission and directory management in a computational physics simulation.', 'file_path': './database/gkv-code/run/shoot'}, {'file id': 2, 'summary': 'Shell script for job submission and running a program with specific job class and MPI settings in a computational physics simulation.', 'file_path': './database/gkv-code/run/sub.q'}, {'file id': 3, 'summary': 'Module for time integration of the GK equation using Runge-Kutta-Gill method, including boundary condition subroutines.', 'file_path': './database/gkv-code/src/.ipynb_checkpoints/gkvp_advnc-checkpoint.f90'}, {'file id': 4, 'summary': 'Module for imposing modified periodic boundary conditions in the z-direction for the distribution function in a computational physics simulation.', 'file_path': './database/gkv-code/src/.ipynb_checkpoints/gkvp_bndry-checkpoint.f90'}, {'file id': 0, 'summary': 'Plasma simulation module for collision terms, including parameter setting and collision frequency calculation.', 'file_path': './database/gkv-code/src/gkvp_clock.f90'}, {'file id': 1, 'summary': 'Plasma simulation module for collision terms using an implicit solver, including parameter setting and collision frequency calculation.', 'file_path': './database/gkv-code/src/gkvp_colli.f90'}, {'file id': 2, 'summary': 'Plasma simulation module for time step size control, including parameter setting and collision term calculation.', 'file_path': './database/gkv-code/src/gkvp_colliimp.f90'}, {'file id': 3, 'summary': 'Plasma simulation module for calculating the E x B term, including term calculation and its effects.', 'file_path': './database/gkv-code/src/gkvp_dtc.f90'}, {'file id': 0, 'summary': 'Contains routines for time integration of the GK equation using RKG method with collisional and collisionless terms.', 'file_path': './database/gkv-code/src/gkvp_f0.56_advnc_tune_nec1.f90'}, {'file id': 1, 'summary': 'Provides routines for handling boundary conditions and periodic boundary conditions for the distribution function in the z-direction.', 'file_path': './database/gkv-code/src/gkvp_f0.56_bndry_tune_nec1.f90'}, {'file id': 2, 'summary': 'Contains routines for setting parameters of the GK collision term.', 'file_path': './database/gkv-code/src/gkvp_f0.56_colli_tune_nifs.f90'}, {'file id': 3, 'summary': 'Includes routines for calculating the E x B term, involving nonlinear terms and FFT calculations.', 'file_path': './database/gkv-code/src/gkvp_f0.56_exb_tune2r_0813.f90'}, {'file id': 4, 'summary': 'Provides routines for initializing FFT and setting up plans for E x B term calculations.', 'file_path': './database/gkv-code/src/gkvp_f0.56_fft_fftw_tune2r_0813.f90'}, {'file id': '0', 'summary': 'File 0 contains a subroutine for performing a z-derivative operation on complex data arrays, likely used in numerical simulations or data processing.', 'file_path': './database/gkv-code/src/gkvp_f0.56_zfilter_tune_nec1.f90'}, {'file id': '0', 'summary': 'Module for calculating growth rates and frequencies in a flux-tube model, including setting initial conditions, resetting the module, and writing frequency data.', 'file_path': './database/gkv-code/src/gkvp_freq.f90'}, {'file id': 0, 'summary': 'This file is the main entry point of the GKV+ nonlinear gyrokinetic Vlasov code, responsible for initializing and orchestrating the simulation.', 'file_path': './database/gkv-code/src/gkvp_main.f90'}, {'file id': 0, 'summary': 'Module for calculating the effect of shear flow on magnetic field components in the kx direction using MPI.', 'file_path': './database/gkv-code/src/gkvp_shearflow.f90'}, {'file id': 1, 'summary': 'Module for post-processing diagnostics including reality check, flushing output, and rescaling fields for linear runs.', 'file_path': './database/gkv-code/src/gkvp_tips.f90'}, {'file id': 2, 'summary': 'Module for calculating the entropy balance equation and triad transfer from magnetic field components.', 'file_path': './database/gkv-code/src/gkvp_trans.f90'}, {'file id': 3, 'summary': 'Module for reading and processing magnetic field and metric coefficients from a VMEC equilibrium file using BZX code.', 'file_path': './database/gkv-code/src/gkvp_vmecbzx.f90'}, {'file id': 4, 'summary': 'Module for reading and processing magnetic field and metric coefficients from a VMEC equilibrium file.', 'file_path': './database/gkv-code/src/gkvp_vmecin.f90'}, {'file id': '0', 'summary': 'Module for filtering data in the zz direction to reduce high-kz numerical oscillations.', 'file_path': './database/gkv-code/src/gkvp_zfilter.f90'}]\n",
      "\n",
      "\n",
      "\n",
      "Expert <class 'MetaSurvey2CheckInfo.MetaSurvey2CheckInfo'> started\n",
      "\n",
      "\n",
      "\n",
      "Expert <class 'MetaSurvey2CheckInfo.MetaSurvey2CheckInfo'> ended\n",
      "\n",
      "result: {'info_dicts': []}\n",
      "\n",
      "\n",
      "INFO 03-30 12:34:53 async_llm_engine.py:211] Added request 29.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"/workspace/SEIMEI6-2/Experts/SWE/QuickSummary.py\", line 115, in inference\n",
      "    id = int(id_text)\n",
      "ValueError: invalid literal for int() with base 10: 'file id: 0'\n",
      "Traceback (most recent call last):\n",
      "  File \"/workspace/SEIMEI6-2/Experts/SWE/QuickSummary.py\", line 115, in inference\n",
      "    id = int(id_text)\n",
      "ValueError: invalid literal for int() with base 10: 'file id: 1'\n",
      "Traceback (most recent call last):\n",
      "  File \"/workspace/SEIMEI6-2/Experts/SWE/QuickSummary.py\", line 115, in inference\n",
      "    id = int(id_text)\n",
      "ValueError: invalid literal for int() with base 10: 'file id: 2'\n",
      "Traceback (most recent call last):\n",
      "  File \"/workspace/SEIMEI6-2/Experts/SWE/QuickSummary.py\", line 115, in inference\n",
      "    id = int(id_text)\n",
      "ValueError: invalid literal for int() with base 10: 'file id: 3'\n",
      "Traceback (most recent call last):\n",
      "  File \"/workspace/SEIMEI6-2/Experts/SWE/QuickSummary.py\", line 117, in inference\n",
      "    data[\"file_path\"] = survey_paths_list[i][id]\n",
      "KeyError: 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Expert <class 'CollectCodeFileToModify.CollectCodeFileToModify'> started\n",
      "\n",
      "\n",
      "\n",
      "Expert <class 'CollectCodeFileToModify.CollectCodeFileToModify'> ended\n",
      "\n",
      "result: None\n",
      "\n",
      "\n",
      "\n",
      "Expert <class 'CheckInf.CheckInf'> started\n",
      "\n",
      "\n",
      "\n",
      "Expert <class 'CheckInf.CheckInf'> ended\n",
      "\n",
      "result: None\n",
      "\n",
      "\n",
      "INFO 03-30 12:34:56 metrics.py:467] Avg prompt throughput: 1249.8 tokens/s, Avg generation throughput: 72.8 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.7%, CPU KV cache usage: 0.0%.\n",
      "INFO 03-30 12:35:01 metrics.py:467] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 66.9 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.7%, CPU KV cache usage: 0.0%.\n",
      "\n",
      "Expert <class 'CheckInf.CheckInf'> started\n",
      "\n",
      "\n",
      "\n",
      "Expert <class 'CheckInf.CheckInf'> ended\n",
      "\n",
      "result: None\n",
      "\n",
      "\n",
      "INFO 03-30 12:35:06 metrics.py:467] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 66.7 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.8%, CPU KV cache usage: 0.0%.\n",
      "INFO 03-30 12:35:11 metrics.py:467] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 66.5 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.8%, CPU KV cache usage: 0.0%.\n",
      "\n",
      "Expert <class 'CollectCodeFileToModify.CollectCodeFileToModify'> started\n",
      "\n",
      "\n",
      "\n",
      "Expert <class 'CollectCodeFileToModify.CollectCodeFileToModify'> ended\n",
      "\n",
      "result: None\n",
      "\n",
      "\n",
      "\n",
      "Expert <class 'CheckInf.CheckInf'> started\n",
      "\n",
      "\n",
      "\n",
      "Expert <class 'CheckInf.CheckInf'> ended\n",
      "\n",
      "result: None\n",
      "\n",
      "\n",
      "INFO 03-30 12:35:16 metrics.py:467] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 66.4 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.9%, CPU KV cache usage: 0.0%.\n",
      "INFO 03-30 12:35:21 metrics.py:467] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 66.5 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.9%, CPU KV cache usage: 0.0%.\n",
      "\n",
      "Expert <class 'CheckInf.CheckInf'> started\n",
      "\n",
      "\n",
      "\n",
      "Expert <class 'CheckInf.CheckInf'> ended\n",
      "\n",
      "result: None\n",
      "\n",
      "\n",
      "INFO 03-30 12:35:26 metrics.py:467] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 65.5 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.9%, CPU KV cache usage: 0.0%.\n",
      "INFO 03-30 12:35:31 metrics.py:467] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 65.0 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 1.0%, CPU KV cache usage: 0.0%.\n",
      "\n",
      "Expert <class 'CollectCodeFileToModify.CollectCodeFileToModify'> started\n",
      "\n",
      "\n",
      "\n",
      "Expert <class 'CollectCodeFileToModify.CollectCodeFileToModify'> ended\n",
      "\n",
      "result: None\n",
      "\n",
      "\n",
      "\n",
      "Expert <class 'CheckInf.CheckInf'> started\n",
      "\n",
      "\n",
      "\n",
      "Expert <class 'CheckInf.CheckInf'> ended\n",
      "\n",
      "result: None\n",
      "\n",
      "\n",
      "INFO 03-30 12:35:36 metrics.py:467] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 65.2 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 1.0%, CPU KV cache usage: 0.0%.\n",
      "INFO 03-30 12:35:41 metrics.py:467] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 64.9 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 1.0%, CPU KV cache usage: 0.0%.\n",
      "\n",
      "Expert <class 'CheckInf.CheckInf'> started\n",
      "\n",
      "\n",
      "\n",
      "Expert <class 'CheckInf.CheckInf'> ended\n",
      "\n",
      "result: None\n",
      "\n",
      "\n",
      "INFO 03-30 12:35:46 metrics.py:467] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 64.6 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 1.1%, CPU KV cache usage: 0.0%.\n",
      "INFO 03-30 12:35:51 metrics.py:467] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 64.7 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 1.1%, CPU KV cache usage: 0.0%.\n",
      "\n",
      "Expert <class 'CollectCodeFileToModify.CollectCodeFileToModify'> started\n",
      "\n",
      "\n",
      "\n",
      "Expert <class 'CollectCodeFileToModify.CollectCodeFileToModify'> ended\n",
      "\n",
      "result: None\n",
      "\n",
      "\n",
      "\n",
      "Expert <class 'CheckInf.CheckInf'> started\n",
      "\n",
      "\n",
      "\n",
      "Expert <class 'CheckInf.CheckInf'> ended\n",
      "\n",
      "result: None\n",
      "\n",
      "\n",
      "INFO 03-30 12:35:56 metrics.py:467] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 64.5 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 1.1%, CPU KV cache usage: 0.0%.\n",
      "INFO 03-30 12:36:01 metrics.py:467] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 64.4 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 1.2%, CPU KV cache usage: 0.0%.\n",
      "\n",
      "Expert <class 'CheckInf.CheckInf'> started\n",
      "\n",
      "\n",
      "\n",
      "Expert <class 'CheckInf.CheckInf'> ended\n",
      "\n",
      "result: None\n",
      "\n",
      "\n",
      "INFO 03-30 12:36:06 metrics.py:467] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 63.9 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 1.2%, CPU KV cache usage: 0.0%.\n",
      "INFO 03-30 12:36:11 metrics.py:467] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 63.7 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 1.2%, CPU KV cache usage: 0.0%.\n",
      "\n",
      "Expert <class 'CollectCodeFileToModify.CollectCodeFileToModify'> started\n",
      "\n",
      "\n",
      "\n",
      "Expert <class 'CollectCodeFileToModify.CollectCodeFileToModify'> ended\n",
      "\n",
      "result: None\n",
      "\n",
      "\n",
      "\n",
      "Expert <class 'CheckInf.CheckInf'> started\n",
      "\n",
      "\n",
      "\n",
      "Expert <class 'CheckInf.CheckInf'> ended\n",
      "\n",
      "result: None\n",
      "\n",
      "\n",
      "INFO 03-30 12:36:16 metrics.py:467] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 63.6 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 1.3%, CPU KV cache usage: 0.0%.\n",
      "INFO 03-30 12:36:21 metrics.py:467] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 63.7 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 1.3%, CPU KV cache usage: 0.0%.\n",
      "\n",
      "Expert <class 'CheckInf.CheckInf'> started\n",
      "\n",
      "\n",
      "\n",
      "Expert <class 'CheckInf.CheckInf'> ended\n",
      "\n",
      "result: None\n",
      "\n",
      "\n",
      "INFO 03-30 12:36:26 metrics.py:467] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 63.6 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 1.3%, CPU KV cache usage: 0.0%.\n",
      "INFO 03-30 12:36:31 metrics.py:467] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 63.5 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 1.4%, CPU KV cache usage: 0.0%.\n",
      "\n",
      "Expert <class 'CollectCodeFileToModify.CollectCodeFileToModify'> started\n",
      "\n",
      "\n",
      "\n",
      "Expert <class 'CollectCodeFileToModify.CollectCodeFileToModify'> ended\n",
      "\n",
      "result: None\n",
      "\n",
      "\n",
      "\n",
      "Expert <class 'CheckInf.CheckInf'> started\n",
      "\n",
      "\n",
      "\n",
      "Expert <class 'CheckInf.CheckInf'> ended\n",
      "\n",
      "result: None\n",
      "\n",
      "\n",
      "INFO 03-30 12:36:36 metrics.py:467] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 63.6 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 1.4%, CPU KV cache usage: 0.0%.\n",
      "INFO 03-30 12:36:41 metrics.py:467] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 63.5 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 1.4%, CPU KV cache usage: 0.0%.\n",
      "\n",
      "Expert <class 'CheckInf.CheckInf'> started\n",
      "\n",
      "\n",
      "\n",
      "Expert <class 'CheckInf.CheckInf'> ended\n",
      "\n",
      "result: None\n",
      "\n",
      "\n",
      "INFO 03-30 12:36:46 metrics.py:467] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 60.4 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 1.5%, CPU KV cache usage: 0.0%.\n",
      "INFO 03-30 12:36:51 metrics.py:467] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 55.9 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 1.5%, CPU KV cache usage: 0.0%.\n",
      "\n",
      "Expert <class 'CollectCodeFileToModify.CollectCodeFileToModify'> started\n",
      "\n",
      "\n",
      "\n",
      "Expert <class 'CollectCodeFileToModify.CollectCodeFileToModify'> ended\n",
      "\n",
      "result: None\n",
      "\n",
      "\n",
      "\n",
      "Expert <class 'CheckInf.CheckInf'> started\n",
      "\n",
      "\n",
      "\n",
      "Expert <class 'CheckInf.CheckInf'> ended\n",
      "\n",
      "result: None\n",
      "\n",
      "\n",
      "INFO 03-30 12:36:56 metrics.py:467] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 55.8 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 1.5%, CPU KV cache usage: 0.0%.\n",
      "INFO 03-30 12:37:01 metrics.py:467] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 55.8 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 1.6%, CPU KV cache usage: 0.0%.\n",
      "\n",
      "Expert <class 'CheckInf.CheckInf'> started\n",
      "\n",
      "\n",
      "\n",
      "Expert <class 'CheckInf.CheckInf'> ended\n",
      "\n",
      "result: None\n",
      "\n",
      "\n",
      "INFO 03-30 12:37:06 metrics.py:467] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 55.7 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 1.6%, CPU KV cache usage: 0.0%.\n",
      "INFO 03-30 12:37:11 metrics.py:467] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 55.7 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 1.6%, CPU KV cache usage: 0.0%.\n",
      "\n",
      "Expert <class 'CollectCodeFileToModify.CollectCodeFileToModify'> started\n",
      "\n",
      "\n",
      "\n",
      "Expert <class 'CollectCodeFileToModify.CollectCodeFileToModify'> ended\n",
      "\n",
      "result: None\n",
      "\n",
      "\n",
      "\n",
      "Expert <class 'CheckInf.CheckInf'> started\n",
      "\n",
      "\n",
      "\n",
      "Expert <class 'CheckInf.CheckInf'> ended\n",
      "\n",
      "result: None\n",
      "\n",
      "\n",
      "INFO 03-30 12:37:16 metrics.py:467] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 55.6 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 1.7%, CPU KV cache usage: 0.0%.\n",
      "INFO 03-30 12:37:21 metrics.py:467] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 55.8 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 1.7%, CPU KV cache usage: 0.0%.\n",
      "\n",
      "Expert <class 'CheckInf.CheckInf'> started\n",
      "\n",
      "\n",
      "\n",
      "Expert <class 'CheckInf.CheckInf'> ended\n",
      "\n",
      "result: None\n",
      "\n",
      "\n",
      "INFO 03-30 12:37:26 metrics.py:467] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 55.7 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 1.7%, CPU KV cache usage: 0.0%.\n",
      "INFO 03-30 12:37:31 metrics.py:467] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 55.6 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 1.8%, CPU KV cache usage: 0.0%.\n",
      "\n",
      "Expert <class 'CollectCodeFileToModify.CollectCodeFileToModify'> started\n",
      "\n",
      "\n",
      "\n",
      "Expert <class 'CollectCodeFileToModify.CollectCodeFileToModify'> ended\n",
      "\n",
      "result: None\n",
      "\n",
      "\n",
      "\n",
      "Expert <class 'CheckInf.CheckInf'> started\n",
      "\n",
      "\n",
      "\n",
      "Expert <class 'CheckInf.CheckInf'> ended\n",
      "\n",
      "result: None\n",
      "\n",
      "\n",
      "INFO 03-30 12:37:34 async_llm_engine.py:179] Finished request 29.\n",
      "json_text:  [\n",
      "    {\n",
      "        \"action\": \"Modify the gkvp_vmecbzx.f90 file to include the new equilibrium state\",\n",
      "        \"file id\": 57\n",
      "    }\n",
      "]\n",
      "\n",
      "Expert <class 'ModifyCodeFile.ModifyCodeFile'> started\n",
      "\n",
      "\n",
      "Expert <class 'Experts.Code.PreprocessFile.PreprocessFile'> started\n",
      "\n",
      "PreprocessFile survey_path:  ./database/gkv-code/run/backup/old/Makefile_helios\n",
      "PreprocessFile chunks:  [\"FC = mpiifort\\nFFLAGS = -ipo -O3 -no-prec-div -xavx -openmp -r8 -shared-intel -mcmodel=large -fpp -warn all\\n#FFLAGS += -warn all -warn declarations -std -CB -check uninit -fpe0\\n\\nPROG = 'gkvp_mpifft.exe'\\n\\nSRC = ../src/\\nMYL = ../lib/\\n\\nMKLROOT = /csc/softs/intel/composer_xe_2011_sp1.7.256/mkl\\nFFTWROOT = /csc/softs/fftw/fftw-3.3/intel-12.0.5.220/bullxmpi-1.1.11.1/default\\nFFTWLIB= -L$(FFTWROOT)/lib -lfftw3\\nNAGDIR = /csc/softs/nag/fsl6i22dc\\nRNGINC =\\nRNGLIB =\\nBESINC =\\nBESLIB = -L$(NAGDIR)/lib -openmp -lnagsmp\\n\\nMATH = gkvp_math_MKLNAG\\nFFT = gkvp_fft_fftw\\n#INC = $(RNGINC) $(BESINC) -I$(MKLROOT)/include -I$(MKLROOT)/include/fftw\\n#LIB = $(RNGLIB) $(BESLIB)  -lmkl_intel_lp64 -lmkl_sequential -lmkl_core -liomp5 -lpthread -lm\\nINC = $(RNGINC) $(BESINC) -I$(FFTWROOT)/include\\nLIB = $(RNGLIB) $(BESLIB) $(FFTWLIB) -lmkl_intel_lp64 -lmkl_sequential -lmkl_core -liomp5 -lpthread -lm\\n\\n\\ngkvp:\\t$(SRC)gkvp_header.f90\\\\\\n\\t$(SRC)gkvp_mpienv.f90\\\\\\n\\t$(MKLROOT)/include/mkl_vsl.f90\\\\\\n\\t$(MYL)$(MATH).f90\\\\\\n\\t$(SRC)gkvp_clock.f90\\\\\\n\\t$(SRC)gkvp_intgrl.f90\\\\\\n\\t$(SRC)gkvp_tips.f90\\\\\\n\\t$(SRC)gkvp_vmecin.f90\\\\\\n\\t$(SRC)gkvp_igs.f90\\\\\\n\\t$(SRC)gkvp_bndry.f90\\\\\\n\\t$(SRC)gkvp_colli.f90\\\\\\n\\t$(SRC)$(FFT).f90\\\\\\n\\t$(SRC)gkvp_fld.f90\\\\\\n\\t$(SRC)gkvp_freq.f90\\\\\\n\\t$(SRC)gkvp_zfilter.f90\\\\\\n\\t$(SRC)gkvp_exb.f90\\\\\\n\\t$(SRC)gkvp_trans.f90\\\\\\n\\t$(SRC)gkvp_advnc.f90\\\\\\n\\t$(SRC)gkvp_dtc.f90\\\\\\n\\t$(SRC)gkvp_out.f90\\\\\\n\\t$(SRC)gkvp_set.f90\\\\\\n\\t$(SRC)gkvp_main.f90\\n\\n\\t$(FC) $(FFLAGS) -c $(SRC)gkvp_header.f90\\n\\t$(FC) $(FFLAGS) -c $(SRC)gkvp_mpienv.f90\\n\\t$(FC) $(FFLAGS) -c $(MKLROOT)/include/mkl_vsl.f90 $(INC)\\n\\t$(FC) $(FFLAGS) -c $(MYL)$(MATH).f90\\n\\t$(FC) $(FFLAGS) -c $(SRC)gkvp_clock.f90\\n\\t$(FC) $(FFLAGS) -c $(SRC)gkvp_intgrl.f90\\n\\t$(FC) $(FFLAGS) -c $(SRC)gkvp_tips.f90\\n\\t$(FC) $(FFLAGS) -c $(SRC)gkvp_vmecin.f90\\n\\t$(FC) $(FFLAGS) -c $(SRC)gkvp_igs.f90\\n\\t$(FC) $(FFLAGS) -c $(SRC)gkvp_bndry.f90\\n\\t$(FC) $(FFLAGS) -c $(SRC)gkvp_colli.f90\\n\\t$(FC) $(FFLAGS) -c $(SRC)$(FFT).f90 $(INC)\\n\\t$(FC) $(FFLAGS) -c $(SRC)gkvp_fld.f90\\n\\t$(FC) $(FFLAGS) -c $(SRC)gkvp_freq.f90\\n\\t$(FC) $(FFLAGS) -c $(SRC)gkvp_zfilter.f90\\n\\t$(FC) $(FFLAGS) -c $(SRC)gkvp_exb.f90\\n\\t$(FC) $(FFLAGS) -c $(SRC)gkvp_trans.f90\\n\\t$(FC) $(FFLAGS) -c $(SRC)gkvp_advnc.f90\\n\\t$(FC) $(FFLAGS) -c $(SRC)gkvp_dtc.f90\\n\\t$(FC) $(FFLAGS) -c $(SRC)gkvp_out.f90\\n\\t$(FC) $(FFLAGS) -c $(SRC)gkvp_set.f90\\n\\t$(FC) $(FFLAGS) -c $(SRC)gkvp_main.f90\\n\\n\\t$(FC) $(FFLAGS)\\t\\\\\\n\\t\\t\\tgkvp_header.o\\\\\\n\\t\\t\\tgkvp_mpienv.o\\\\\\n\\t\\t\\tmkl_vsl.o\\\\\\n\\t\\t\\t$(MATH).o\\\\\\n\\t\\t\\tgkvp_clock.o\\\\\\n\\t\\t\\tgkvp_intgrl.o\\\\\\n\\t\\t\\tgkvp_tips.o\\\\\\n\\t\\t\\tgkvp_vmecin.o\\\\\\n\\t\\t\\tgkvp_igs.o\\\\\\n\\t\\t\\tgkvp_bndry.o\\\\\\n\\t\\t\\tgkvp_colli.o\\\\\\n\\t\\t\\t$(FFT).o\\\\\\n\\t\\t\\tgkvp_fld.o\\\\\\n\\t\\t\\tgkvp_freq.o\\\\\\n\\t\\t\\tgkvp_zfilter.o\\\\\\n\\t\\t\\tgkvp_exb.o\\\\\\n\\t\\t\\tgkvp_trans.o\\\\\\n\\t\\t\\tgkvp_advnc.o\\\\\\n\\t\\t\\tgkvp_dtc.o\\\\\\n\\t\\t\\tgkvp_out.o\\\\\\n\\t\\t\\tgkvp_set.o\\\\\\n                        gkvp_main.o\\\\\\n\\t\\t\\t-o $(PROG) $(LIB)\\n\\n\\t\\\\cp *.o *.mod ../src/\\n\\t\\\\rm -f *.o *.mod\\n\\nclean:\\n\\trm -f ../src/*.lst ../src/*.o ../src/*.mod ./*.exe ./*.err ./*.out\\n\\nclear:\\n\\trm -f ./*.lst ./*.o ./*.mod ./*namelist.* ./sub.q.*\\n\\n\"]\n",
      "\n",
      "\n",
      "Expert <class 'Experts.Code.PreprocessFile.PreprocessFile'> ended\n",
      "\n",
      "result: {'chunks': [\"FC = mpiifort\\nFFLAGS = -ipo -O3 -no-prec-div -xavx -openmp -r8 -shared-intel -mcmodel=large -fpp -warn all\\n#FFLAGS += -warn all -warn declarations -std -CB -check uninit -fpe0\\n\\nPROG = 'gkvp_mpifft.exe'\\n\\nSRC = ../src/\\nMYL = ../lib/\\n\\nMKLROOT = /csc/softs/intel/composer_xe_2011_sp1.7.256/mkl\\nFFTWROOT = /csc/softs/fftw/fftw-3.3/intel-12.0.5.220/bullxmpi-1.1.11.1/default\\nFFTWLIB= -L$(FFTWROOT)/lib -lfftw3\\nNAGDIR = /csc/softs/nag/fsl6i22dc\\nRNGINC =\\nRNGLIB =\\nBESINC =\\nBESLIB = -L$(NAGDIR)/lib -openmp -lnagsmp\\n\\nMATH = gkvp_math_MKLNAG\\nFFT = gkvp_fft_fftw\\n#INC = $(RNGINC) $(BESINC) -I$(MKLROOT)/include -I$(MKLROOT)/include/fftw\\n#LIB = $(RNGLIB) $(BESLIB)  -lmkl_intel_lp64 -lmkl_sequential -lmkl_core -liomp5 -lpthread -lm\\nINC = $(RNGINC) $(BESINC) -I$(FFTWROOT)/include\\nLIB = $(RNGLIB) $(BESLIB) $(FFTWLIB) -lmkl_intel_lp64 -lmkl_sequential -lmkl_core -liomp5 -lpthread -lm\\n\\n\\ngkvp:\\t$(SRC)gkvp_header.f90\\\\\\n\\t$(SRC)gkvp_mpienv.f90\\\\\\n\\t$(MKLROOT)/include/mkl_vsl.f90\\\\\\n\\t$(MYL)$(MATH).f90\\\\\\n\\t$(SRC)gkvp_clock.f90\\\\\\n\\t$(SRC)gkvp_intgrl.f90\\\\\\n\\t$(SRC)gkvp_tips.f90\\\\\\n\\t$(SRC)gkvp_vmecin.f90\\\\\\n\\t$(SRC)gkvp_igs.f90\\\\\\n\\t$(SRC)gkvp_bndry.f90\\\\\\n\\t$(SRC)gkvp_colli.f90\\\\\\n\\t$(SRC)$(FFT).f90\\\\\\n\\t$(SRC)gkvp_fld.f90\\\\\\n\\t$(SRC)gkvp_freq.f90\\\\\\n\\t$(SRC)gkvp_zfilter.f90\\\\\\n\\t$(SRC)gkvp_exb.f90\\\\\\n\\t$(SRC)gkvp_trans.f90\\\\\\n\\t$(SRC)gkvp_advnc.f90\\\\\\n\\t$(SRC)gkvp_dtc.f90\\\\\\n\\t$(SRC)gkvp_out.f90\\\\\\n\\t$(SRC)gkvp_set.f90\\\\\\n\\t$(SRC)gkvp_main.f90\\n\\n\\t$(FC) $(FFLAGS) -c $(SRC)gkvp_header.f90\\n\\t$(FC) $(FFLAGS) -c $(SRC)gkvp_mpienv.f90\\n\\t$(FC) $(FFLAGS) -c $(MKLROOT)/include/mkl_vsl.f90 $(INC)\\n\\t$(FC) $(FFLAGS) -c $(MYL)$(MATH).f90\\n\\t$(FC) $(FFLAGS) -c $(SRC)gkvp_clock.f90\\n\\t$(FC) $(FFLAGS) -c $(SRC)gkvp_intgrl.f90\\n\\t$(FC) $(FFLAGS) -c $(SRC)gkvp_tips.f90\\n\\t$(FC) $(FFLAGS) -c $(SRC)gkvp_vmecin.f90\\n\\t$(FC) $(FFLAGS) -c $(SRC)gkvp_igs.f90\\n\\t$(FC) $(FFLAGS) -c $(SRC)gkvp_bndry.f90\\n\\t$(FC) $(FFLAGS) -c $(SRC)gkvp_colli.f90\\n\\t$(FC) $(FFLAGS) -c $(SRC)$(FFT).f90 $(INC)\\n\\t$(FC) $(FFLAGS) -c $(SRC)gkvp_fld.f90\\n\\t$(FC) $(FFLAGS) -c $(SRC)gkvp_freq.f90\\n\\t$(FC) $(FFLAGS) -c $(SRC)gkvp_zfilter.f90\\n\\t$(FC) $(FFLAGS) -c $(SRC)gkvp_exb.f90\\n\\t$(FC) $(FFLAGS) -c $(SRC)gkvp_trans.f90\\n\\t$(FC) $(FFLAGS) -c $(SRC)gkvp_advnc.f90\\n\\t$(FC) $(FFLAGS) -c $(SRC)gkvp_dtc.f90\\n\\t$(FC) $(FFLAGS) -c $(SRC)gkvp_out.f90\\n\\t$(FC) $(FFLAGS) -c $(SRC)gkvp_set.f90\\n\\t$(FC) $(FFLAGS) -c $(SRC)gkvp_main.f90\\n\\n\\t$(FC) $(FFLAGS)\\t\\\\\\n\\t\\t\\tgkvp_header.o\\\\\\n\\t\\t\\tgkvp_mpienv.o\\\\\\n\\t\\t\\tmkl_vsl.o\\\\\\n\\t\\t\\t$(MATH).o\\\\\\n\\t\\t\\tgkvp_clock.o\\\\\\n\\t\\t\\tgkvp_intgrl.o\\\\\\n\\t\\t\\tgkvp_tips.o\\\\\\n\\t\\t\\tgkvp_vmecin.o\\\\\\n\\t\\t\\tgkvp_igs.o\\\\\\n\\t\\t\\tgkvp_bndry.o\\\\\\n\\t\\t\\tgkvp_colli.o\\\\\\n\\t\\t\\t$(FFT).o\\\\\\n\\t\\t\\tgkvp_fld.o\\\\\\n\\t\\t\\tgkvp_freq.o\\\\\\n\\t\\t\\tgkvp_zfilter.o\\\\\\n\\t\\t\\tgkvp_exb.o\\\\\\n\\t\\t\\tgkvp_trans.o\\\\\\n\\t\\t\\tgkvp_advnc.o\\\\\\n\\t\\t\\tgkvp_dtc.o\\\\\\n\\t\\t\\tgkvp_out.o\\\\\\n\\t\\t\\tgkvp_set.o\\\\\\n                        gkvp_main.o\\\\\\n\\t\\t\\t-o $(PROG) $(LIB)\\n\\n\\t\\\\cp *.o *.mod ../src/\\n\\t\\\\rm -f *.o *.mod\\n\\nclean:\\n\\trm -f ../src/*.lst ../src/*.o ../src/*.mod ./*.exe ./*.err ./*.out\\n\\nclear:\\n\\trm -f ./*.lst ./*.o ./*.mod ./*namelist.* ./sub.q.*\\n\\n\"]}\n",
      "\n",
      "\n",
      "\n",
      "Expert <class 'Experts.Code.ModifyCodeChunk.ModifyCodeChunk'> started\n",
      "\n",
      "INFO 03-30 12:37:34 async_llm_engine.py:211] Added request 30.\n",
      "INFO 03-30 12:37:36 metrics.py:467] Avg prompt throughput: 63.8 tokens/s, Avg generation throughput: 56.2 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%.\n",
      "INFO 03-30 12:37:41 metrics.py:467] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 60.0 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.1%, CPU KV cache usage: 0.0%.\n",
      "\n",
      "Expert <class 'CheckInf.CheckInf'> started\n",
      "\n",
      "\n",
      "\n",
      "Expert <class 'CheckInf.CheckInf'> ended\n",
      "\n",
      "result: None\n",
      "\n",
      "\n",
      "INFO 03-30 12:37:46 metrics.py:467] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 59.9 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.1%, CPU KV cache usage: 0.0%.\n",
      "INFO 03-30 12:37:51 metrics.py:467] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 59.6 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.1%, CPU KV cache usage: 0.0%.\n",
      "\n",
      "Expert <class 'CollectCodeFileToModify.CollectCodeFileToModify'> started\n",
      "\n",
      "\n",
      "\n",
      "Expert <class 'CollectCodeFileToModify.CollectCodeFileToModify'> ended\n",
      "\n",
      "result: None\n",
      "\n",
      "\n",
      "\n",
      "Expert <class 'CheckInf.CheckInf'> started\n",
      "\n",
      "\n",
      "\n",
      "Expert <class 'CheckInf.CheckInf'> ended\n",
      "\n",
      "result: None\n",
      "\n",
      "\n",
      "INFO 03-30 12:37:56 metrics.py:467] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 59.6 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.2%, CPU KV cache usage: 0.0%.\n",
      "INFO 03-30 12:38:01 metrics.py:467] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 59.6 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.2%, CPU KV cache usage: 0.0%.\n",
      "\n",
      "Expert <class 'CheckInf.CheckInf'> started\n",
      "\n",
      "\n",
      "\n",
      "Expert <class 'CheckInf.CheckInf'> ended\n",
      "\n",
      "result: None\n",
      "\n",
      "\n",
      "INFO 03-30 12:38:06 metrics.py:467] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 59.8 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.2%, CPU KV cache usage: 0.0%.\n",
      "INFO 03-30 12:38:11 metrics.py:467] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 59.3 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.3%, CPU KV cache usage: 0.0%.\n",
      "INFO 03-30 12:38:13 async_llm_engine.py:179] Finished request 30.\n",
      "\n",
      "modified_code\n",
      "(modified code here. if there is not enough information, leave here blank)\n",
      "\n",
      "\n",
      "Expert <class 'Experts.Code.ModifyCodeChunk.ModifyCodeChunk'> ended\n",
      "\n",
      "result: {'next_action': 'To modify the `gkvp_vmecbzx.f90` file to include a new equilibrium state, we need the current state of the file. We need to know the specific changes required to include a new equilibrium state. Once we have this information, we can modify the file accordingly.', 'success': True}\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Expert <class 'ModifyCodeFile.ModifyCodeFile'> ended\n",
      "\n",
      "result: None\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Expert <class 'MetaSurvey.MetaSurvey'> ended\n",
      "\n",
      "result: None\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Expert <class 'SEIMEI.Search'> ended\n",
      "\n",
      "result: None\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Expert <class 'SEIMEI.SpecificExperts'> ended\n",
      "\n",
      "result: None\n",
      "\n",
      "\n",
      "\n",
      "Expert <class 'CollectCodeFileToModify.CollectCodeFileToModify'> started\n",
      "\n",
      "\n",
      "\n",
      "Expert <class 'CollectCodeFileToModify.CollectCodeFileToModify'> ended\n",
      "\n",
      "result: None\n",
      "\n",
      "\n",
      "\n",
      "Expert <class 'CheckInf.CheckInf'> started\n",
      "\n",
      "\n",
      "\n",
      "Expert <class 'CheckInf.CheckInf'> ended\n",
      "\n",
      "result: None\n",
      "\n",
      "\n",
      "\n",
      "Expert <class 'CheckInf.CheckInf'> started\n",
      "\n",
      "\n",
      "\n",
      "Expert <class 'CheckInf.CheckInf'> ended\n",
      "\n",
      "result: None\n",
      "\n",
      "\n",
      "\n",
      "Expert <class 'CollectCodeFileToModify.CollectCodeFileToModify'> started\n",
      "\n",
      "\n",
      "\n",
      "Expert <class 'CollectCodeFileToModify.CollectCodeFileToModify'> ended\n",
      "\n",
      "result: None\n",
      "\n",
      "\n",
      "\n",
      "Expert <class 'CheckInf.CheckInf'> started\n",
      "\n",
      "\n",
      "\n",
      "Expert <class 'CheckInf.CheckInf'> ended\n",
      "\n",
      "result: None\n",
      "\n",
      "\n",
      "\n",
      "Expert <class 'CheckInf.CheckInf'> started\n",
      "\n",
      "\n",
      "\n",
      "Expert <class 'CheckInf.CheckInf'> ended\n",
      "\n",
      "result: None\n",
      "\n",
      "\n",
      "\n",
      "Expert <class 'CollectCodeFileToModify.CollectCodeFileToModify'> started\n",
      "\n",
      "\n",
      "\n",
      "Expert <class 'CollectCodeFileToModify.CollectCodeFileToModify'> ended\n",
      "\n",
      "result: None\n",
      "\n",
      "\n",
      "\n",
      "Expert <class 'CheckInf.CheckInf'> started\n",
      "\n",
      "\n",
      "\n",
      "Expert <class 'CheckInf.CheckInf'> ended\n",
      "\n",
      "result: None\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "original_question = \"How to implement a new equilibrium state called Miller equilibrium into gyro-kinetic vlasov simulation?\"\n",
    "queries = [\n",
    "    {\n",
    "        \"query\":original_question,\n",
    "        \"doc_path\":\"/gkv-code\",\n",
    "    }\n",
    "]\n",
    "\n",
    "final_answer = await seimei.get_answer(queries = queries) # return final answer\n",
    "\n",
    "print()\n",
    "print()\n",
    "print(final_answer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab1d1b48-ac5f-434d-b69a-509220c5eef0",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Kaggle/AIMO2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c72363a4-5937-4d4d-b163-1c03e76b33a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "ids = [i for i in range(num_problems)]\n",
    "#ids = [0,1,4,5,6,7]\n",
    "num_sample = 5\n",
    "max_request = 40\n",
    "progress_save_dir = \"math-aime-progress_log3\"\n",
    "save_path = \"aime_log1.json\"\n",
    "explanation = \"deepseek-r1-qwen14b/aime\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46950efd-cf22-406f-8e11-827146a0b3b5",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import json, re, os\n",
    "\n",
    "# Prepare Variables\n",
    "\n",
    "# log : { str(problem_id): { \"prompts\": {str(sample_id):promt,}, \"outputs\":{}, \"final_answers\":{}, \"corrects\":{}}, }\n",
    "log = {str(id):{\"prompts\":{}, \"outputs\":{}, \"final_answers\":{}, \"corrects\":{}} for id in ids}\n",
    "log[\"num_sample\"] = num_sample\n",
    "log[\"num_problems\"] = num_problems\n",
    "log[\"info\"] = explanation\n",
    "\n",
    "from transformers import AutoTokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"./deepseek_r1_qwen14b\", padding_side='left')\n",
    "\n",
    "# all_requests = {str(request_id): requests}\n",
    "# requests = [{\"prompt\": , ... }, ... ]\n",
    "# Ex. all_requests = {\"0\":[{\"prompt\":, \"problem_id\":,}, ...] }\n",
    "#all_requests = {str(i):[] for i in range(max_request)}\n",
    "all_requests = AllRequests(max_request)\n",
    "\n",
    "# Prepare all_requests\n",
    "#request_id = 0\n",
    "for i in range(num_sample):  # To see the rough result quickly, it'd better process problems with different ids first. That's why loop for sample comes before one for ids.\n",
    "    for id in ids:\n",
    "        messages = [\n",
    "            {\"role\": \"user\", \"content\": f\"\"\"Please answer to the problem, and put your final answer within \\\\boxed{{}}. If final answer is a number larger than 1000, take module 1000.\n",
    "\n",
    "Problem: {problems[id]}\"\"\"}\n",
    "        ]\n",
    "\n",
    "        #messages = [\n",
    "        #    {\"role\": \"system\", \"content\": f\"\"\"Please answer to the problem, and put your final answer within \\\\boxed{{}}. If final answer is a number larger than 1000, take module 1000.\"\"\"},\n",
    "        #    {\"role\": \"user\", \"content\": f\"\"\"Problem: {problems[id]}\"\"\"}\n",
    "        #]\n",
    "        \n",
    "        prompt = tokenizer.apply_chat_template(\n",
    "            messages,\n",
    "            tokenize=False,\n",
    "            add_generation_prompt=True\n",
    "        )\n",
    "        \n",
    "        request_dict = {\"problem_id\":id, \"sample_id\":i, \"prompt\":prompt}\n",
    "        all_requests.add(request_dict)\n",
    "        #all_requests[str(request_id)].append(request_dict)\n",
    "        #request_id += 1\n",
    "        #if request_id == max_request:\n",
    "        #    request_id = 0\n",
    "\n",
    "# Define get_result (Optional): If you want to evaluate the output somehow while running process_requests, you can define get_result function and pass it to process_requests method. This will let you read log files easily with some evaluation.\n",
    "def get_result(request_dict, save_dir):\n",
    "    # request_dict: {\"prompt\":, \"output\":}\n",
    "    # save_dir: this is a directory path for progress log. put your evaluation file in here\n",
    "\n",
    "    problem_id = request_dict[\"problem_id\"]\n",
    "    output = request_dict[\"output\"]\n",
    "    eval_file_path = f\"{save_dir}/score.json\"\n",
    "\n",
    "    if os.path.exists(eval_file_path):\n",
    "        with open(eval_file_path) as f:\n",
    "            log = json.load(f)\n",
    "        if str(problem_id) in log[\"num_answered\"]:\n",
    "            num_answered = log[\"num_answered\"][str(problem_id)]\n",
    "            num_correct = log[\"num_correct\"][str(problem_id)]\n",
    "        else:\n",
    "            num_answered = 0\n",
    "            num_correct = 0\n",
    "    else:\n",
    "        log = {\"num_answered\":{}, \"num_correct\":{}}\n",
    "        num_answered = 0\n",
    "        num_correct = 0\n",
    "\n",
    "    is_correct = False\n",
    "    pattern = r'\\\\boxed{(\\d+)}'\n",
    "    matches = re.findall(pattern, output)\n",
    "    if matches == []:\n",
    "        final_answer = None\n",
    "    else:\n",
    "        final_answer = int(matches[0])\n",
    "        if correct_answers[problem_id] == final_answer:\n",
    "            is_correct = True\n",
    "            num_correct += 1\n",
    "\n",
    "    request_dict[\"final_answer\"] = final_answer\n",
    "    request_dict[\"is_correct\"] = is_correct\n",
    "\n",
    "    num_answered += 1\n",
    "    log[\"num_answered\"][str(problem_id)] = num_answered\n",
    "    log[\"num_correct\"][str(problem_id)] = num_correct\n",
    "\n",
    "    with open(eval_file_path, \"w\") as f:\n",
    "        json.dump(log, f)\n",
    "\n",
    "    return request_dict\n",
    "\n",
    "\n",
    "# Process all_requests\n",
    "# If you had trouble in last process and want to continue to get the output, set restart = True\n",
    "#all_results = await asyncio.gather(\n",
    "#    *[process_requests(all_requests[request_id_str], int(request_id_str), max_tokens = 10000, save_dir = progress_save_dir, restart = False, get_result = get_result) for request_id_str in all_requests]\n",
    "#)\n",
    "all_results = await all_requests.process(model = \"deepseek_r1_qwen14b\", max_tokens = 15000, save_dir = progress_save_dir, restart = True, get_result = get_result)\n",
    "\n",
    "# Check if the outputs are correct\n",
    "for results_dict in all_results:\n",
    "\n",
    "    problem_id = results_dict[\"problem_id\"]\n",
    "    sample_id = results_dict[\"sample_id\"]\n",
    "    prompt = results_dict[\"prompt\"]\n",
    "    output = results_dict[\"output\"]\n",
    "\n",
    "    is_correct = False\n",
    "    pattern = r'\\\\boxed{(\\d+)}'\n",
    "    matches = re.findall(pattern, output)\n",
    "    if matches == []:\n",
    "        final_answer = None\n",
    "    else:\n",
    "        final_answer = int(matches[0])\n",
    "        if correct_answers[problem_id] == final_answer:\n",
    "            is_correct = True\n",
    "\n",
    "    log[str(problem_id)][\"prompts\"][str(sample_id)] = prompt\n",
    "    log[str(problem_id)][\"outputs\"][str(sample_id)] = output\n",
    "    log[str(problem_id)][\"final_answers\"][str(sample_id)] = final_answer\n",
    "    log[str(problem_id)][\"corrects\"][str(sample_id)] = is_correct\n",
    "\n",
    "\n",
    "# Calculate Evaluation Scores\n",
    "from collections import Counter\n",
    "mv_score = 0\n",
    "for problem_id in ids:\n",
    "    num_correct = 0\n",
    "    problem_id_str = str(problem_id)\n",
    "    for sample_id_str in log[problem_id_str][\"corrects\"]:\n",
    "        if log[problem_id_str][\"corrects\"][sample_id_str]: num_correct += 1\n",
    "    log[problem_id_str][\"num_correct\"] = num_correct\n",
    "\n",
    "    filtered_numbers = [log[problem_id_str][\"final_answers\"][sample_id_str] for sample_id_str in log[problem_id_str][\"final_answers\"] if log[problem_id_str][\"final_answers\"][sample_id_str] is not None]\n",
    "    if filtered_numbers == []: continue\n",
    "    counter = Counter(filtered_numbers)\n",
    "    most_common_element, count = counter.most_common(1)[0]\n",
    "    if correct_answers[int(problem_id_str)] == most_common_element:\n",
    "        mv_score += 1\n",
    "        \n",
    "log[\"mv_score\"] = mv_score\n",
    "with open(save_path, \"w\") as json_file:\n",
    "    json.dump(log, json_file)\n",
    "\n",
    "print()\n",
    "print(\"-- ALL FINISHED --\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6ef1976-25cb-4299-adb1-607cab5d53a9",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## BigCodeBench"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28e86bfa-5f01-4e76-8620-09a4da5a4bf9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#ids = [i for i in range(10)]\n",
    "#num_problems = 100\n",
    "ids = [i for i in range(num_problems)]\n",
    "num_sample = 1\n",
    "max_request = 15\n",
    "progress_save_dir = \"progress_log_solve4\"\n",
    "save_path = \"code-log7.json\"\n",
    "explanation = \"deepseek_r1_qwen14b/bigcodebench2 for code test with small number of samples\"\n",
    "#num_problems = len(ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02a4061e-5bed-4d4c-942a-d41a3c951dc9",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import json, re, os\n",
    "\n",
    "# Prepare Variables\n",
    "\n",
    "# log : { str(problem_id): { \"prompts\": {str(sample_id):promt,}, \"outputs\":{}, \"final_answers\":{}, \"corrects\":{}}, }\n",
    "log = {str(id):{\"prompts\":{}, \"outputs\":{}, \"final_answers\":{}, \"corrects\":{}, \"output_code\":{}, \"errors\":{}, \"tracebacks\":{}} for id in ids}\n",
    "log[\"num_sample\"] = num_sample\n",
    "log[\"num_problems\"] = num_problems\n",
    "log[\"info\"] = explanation\n",
    "\n",
    "\n",
    "def extract_text_inside_backticks(text, arbitrary_text):\n",
    "    # Define the pattern to match the text inside ``` that follows the arbitrary text\n",
    "    pattern = re.compile(r'```{}\\s*([\\s\\S]*?)\\s*```'.format(re.escape(arbitrary_text)))\n",
    "\n",
    "    # Search for the pattern in the text\n",
    "    match = pattern.search(text)\n",
    "\n",
    "    if match:\n",
    "        return match.group(1).strip()\n",
    "    else:\n",
    "        return None\n",
    "\n",
    "'''\n",
    "def check_output(output, test_code): # this didn't work because exec behaves differently from running it in jupyter notebook\n",
    "    run_code = \"\"\"\n",
    "import inspect\n",
    "def check_code():\n",
    "    try:\n",
    "        sub_obj = TestCases()\n",
    "        for name, attribute in TestCases.__dict__.items():\n",
    "            if not name.startswith('__') and not name.startswith('_') and callable(attribute):\n",
    "                attribute(sub_obj)\n",
    "    except Exception as e:\n",
    "        return False\n",
    "    return True\n",
    "\n",
    "is_code_correct__ = check_code()\"\"\"\n",
    "\n",
    "    output_code = extract_text_inside_backticks(output, \"python\")\n",
    "    if not output_code: output_code = extract_text_inside_backticks(output, \"\")\n",
    "\n",
    "    if not output_code: return False, test_code + run_code\n",
    "\n",
    "    code = output_code + \"\\n\\n\\n\" + test_code + run_code\n",
    "\n",
    "    local_vars = {}\n",
    "    global_vars = {}\n",
    "\n",
    "    try:\n",
    "        exec(test_code, global_vars, local_vars)\n",
    "        is_code_correct = local_vars['is_code_correct__']\n",
    "    except Exception as e:\n",
    "        return False, code\n",
    "\n",
    "    return is_code_correct, code\n",
    "\n",
    "\n",
    "# Define get_result (Optional): If you want to evaluate the output somehow while running process_requests, you can define get_result function and pass it to process_requests method. This will let you read log files easily with some evaluation.\n",
    "def get_result(request_dict, save_dir):\n",
    "    # request_dict: {\"prompt\":, \"output\":}\n",
    "    # save_dir: this is a directory path for progress log. put your evaluation file in here\n",
    "\n",
    "    problem_id = request_dict[\"problem_id\"]\n",
    "    output = request_dict[\"output\"]\n",
    "    test_code = request_dict[\"test_code\"]\n",
    "    \n",
    "    eval_file_path = f\"{save_dir}/score.json\"\n",
    "\n",
    "    if os.path.exists(eval_file_path):\n",
    "        with open(eval_file_path) as f:\n",
    "            log = json.load(f)\n",
    "        if str(problem_id) in log[\"num_answered\"]:\n",
    "            num_answered = log[\"num_answered\"][str(problem_id)]\n",
    "            num_correct = log[\"num_correct\"][str(problem_id)]\n",
    "        else:\n",
    "            num_answered = 0\n",
    "            num_correct = 0\n",
    "    else:\n",
    "        log = {\"num_answered\":{}, \"num_correct\":{}}\n",
    "        num_answered = 0\n",
    "        num_correct = 0\n",
    "\n",
    "    is_code_correct, code = check_output(output, test_code)\n",
    "    if is_code_correct: num_correct += 1\n",
    "\n",
    "    request_dict[\"is_code_correct\"] = is_code_correct\n",
    "\n",
    "    num_answered += 1\n",
    "    log[\"num_answered\"][str(problem_id)] = num_answered\n",
    "    log[\"num_correct\"][str(problem_id)] = num_correct\n",
    "\n",
    "    with open(eval_file_path, \"w\") as f:\n",
    "        json.dump(log, f)\n",
    "\n",
    "    return request_dict\n",
    "'''\n",
    "\n",
    "# all_requests = {str(request_id): requests}\n",
    "# requests = [{\"prompt\": , ... }, ... ]\n",
    "# Ex. all_requests = {\"0\":[{\"prompt\":, \"problem_id\":,}, ...] }\n",
    "#all_requests = {str(i):[] for i in range(max_request)}\n",
    "all_requests = AllRequests(max_request)\n",
    "\n",
    "# Prepare all_requests\n",
    "#request_id = 0\n",
    "for i in range(num_sample):  # To see the rough result quickly, it'd better process problems with different ids first. That's why loop for sample comes before one for ids.\n",
    "    for id in ids:\n",
    "        messages = [\n",
    "            {\"role\": \"user\", \"content\": f\"\"\"{instruct_prompt[id]}\"\"\"}\n",
    "        ]\n",
    "        prompt = tokenizer.apply_chat_template(\n",
    "            messages,\n",
    "            tokenize=False,\n",
    "            add_generation_prompt=True\n",
    "        )\n",
    "        test_code = test[id]\n",
    "        \n",
    "        request_dict = {\"problem_id\":id, \"sample_id\":i, \"prompt\":prompt, \"test_code\":test_code}\n",
    "        all_requests.add(request_dict)\n",
    "        \n",
    "all_results = await all_requests.process(max_tokens = 10000, save_dir = progress_save_dir, restart = True)\n",
    "\n",
    "test_cases = []\n",
    "candidates = [[] for _ in range(num_problems)]\n",
    "test_cases_dict = {}\n",
    "# Check if the outputs are correct\n",
    "for results_dict in all_results:\n",
    "    problem_id = results_dict[\"problem_id\"]\n",
    "    sample_id = results_dict[\"sample_id\"]\n",
    "    prompt = results_dict[\"prompt\"]\n",
    "    output = results_dict[\"output\"]\n",
    "    test_code = results_dict[\"test_code\"]\n",
    "    #is_code_correct, code = check_output(output, test_code)\n",
    "\n",
    "    log[str(problem_id)][\"prompts\"][str(sample_id)] = prompt\n",
    "    log[str(problem_id)][\"outputs\"][str(sample_id)] = output\n",
    "    #log[str(problem_id)][\"corrects\"][str(sample_id)] = is_code_correct\n",
    "\n",
    "    output_code = extract_text_inside_backticks(output, \"python\")\n",
    "    if not output_code: output_code = extract_text_inside_backticks(output, \"\")\n",
    "    if not output_code: output_code = \"\"\n",
    "\n",
    "    log[str(problem_id)][\"output_code\"][str(sample_id)] = output_code\n",
    "\n",
    "    if not problem_id in test_cases_dict:\n",
    "        test_cases_dict[problem_id] = test_code\n",
    "    candidates[problem_id].append(output_code)\n",
    "\n",
    "for id in test_cases_dict:\n",
    "    test_cases.append(test_cases_dict[id])\n",
    "\n",
    "import os, time\n",
    "os.environ[\"HF_ALLOW_CODE_EVAL\"] = \"1\"\n",
    "#from evaluate import load\n",
    "# Load code evaluation metric\n",
    "#code_eval_metric = load(\"code_eval\")\n",
    "\n",
    "# Modified code_eval which has returns traceback of test error \n",
    "from code_eval.code_eval import CodeEval\n",
    "code_eval_metric = CodeEval()\n",
    "# Compute pass@k\n",
    "k_values = [1]\n",
    "print(\"Evaluating generated code...\")\n",
    "start = time.time()\n",
    "pass_at_k, results = code_eval_metric._compute(\n",
    "    references=test_cases,\n",
    "    predictions=candidates,\n",
    "    k=k_values,\n",
    "    num_workers=18,  # Adjust based on your system\n",
    "    timeout=100.0,   # Adjust the timeout as needed\n",
    ")\n",
    "end = time.time()\n",
    "print(\"calculation time(s): \", end-start)\n",
    "\n",
    "# Print the results\n",
    "#for k in k_values:\n",
    "#    print(f\"Pass@{k}: {pass_at_k[f'pass@{k}'] * 100:.2f}%\")\n",
    "    #log[f\"Pass@{k}\"] = pass_at_k[f'pass@{k}']\n",
    "\n",
    "total_num_correct = 0\n",
    "total_num_problem = 0\n",
    "num_correct_dict = {}\n",
    "for problem_id in range(len(results)):\n",
    "    num_correct = 0\n",
    "    for sample_id in range(len(results[problem_id])):\n",
    "        is_correct = results[problem_id][sample_id][1][\"passed\"]\n",
    "        if candidates[problem_id][sample_id]==\"\": is_correct=False  # passed become true when output_code == \"\" for some reason. This should be incorrect\n",
    "        log[str(problem_id)][\"corrects\"][str(sample_id)] = is_correct\n",
    "        if not is_correct:\n",
    "            try: # for normal case\n",
    "                log[str(problem_id)][\"errors\"][str(sample_id)] = results[problem_id][sample_id][1][\"result\"][\"error\"]\n",
    "                log[str(problem_id)][\"tracebacks\"][str(sample_id)] = results[problem_id][sample_id][1][\"result\"][\"traceback\"]\n",
    "            except: # for canse output_code == \"\"\n",
    "                log[str(problem_id)][\"errors\"][str(sample_id)] = \"failed: there is no code included in the answer\"\n",
    "                log[str(problem_id)][\"tracebacks\"][str(sample_id)] = \"failed: there is no code included in the answer\"\n",
    "        if is_correct: num_correct += 1\n",
    "    log[str(problem_id)][\"num_correct\"] = num_correct\n",
    "    num_correct_dict[str(problem_id)] = num_correct\n",
    "    total_num_correct+=num_correct\n",
    "    total_num_problem+=len(results[problem_id])\n",
    "    \n",
    "log[\"num_correct_dict\"] = num_correct_dict\n",
    "log[\"pass1\"] = total_num_correct/total_num_problem\n",
    "with open(save_path, \"w\") as f:\n",
    "    json.dump(log, f)\n",
    "\n",
    "print()\n",
    "print(\"-- ALL FINISHED --\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0c4caee-f64e-4572-b7ce-ce809d32bfdd",
   "metadata": {},
   "outputs": [],
   "source": [
    "total_num_correct = 0\n",
    "total_num_problem = 0\n",
    "num_correct_dict = {}\n",
    "for problem_id in range(len(results)):\n",
    "    num_correct = 0\n",
    "    for sample_id in range(len(results[problem_id])):\n",
    "        is_correct = results[problem_id][sample_id][1][\"passed\"]\n",
    "        if candidates[problem_id][sample_id]==\"\": is_correct=False  # passed become true when output_code == \"\" for some reason. This should be incorrect\n",
    "        log[str(problem_id)][\"corrects\"][str(sample_id)] = is_correct\n",
    "        if not is_correct:\n",
    "            log[str(problem_id)][\"errors\"][str(sample_id)] = results[problem_id][sample_id][1][\"result\"][\"error\"]\n",
    "            log[str(problem_id)][\"tracebacks\"][str(sample_id)] = results[problem_id][sample_id][1][\"result\"][\"traceback\"]\n",
    "\n",
    "        if is_correct: num_correct += 1\n",
    "    log[str(problem_id)][\"num_correct\"] = num_correct\n",
    "    num_correct_dict[str(problem_id)] = num_correct\n",
    "    total_num_correct+=num_correct\n",
    "    total_num_problem+=len(results[problem_id])\n",
    "    \n",
    "log[\"num_correct_dict\"] = num_correct_dict\n",
    "log[\"pass1\"] = total_num_correct/total_num_problem\n",
    "with open(save_path, \"w\") as f:\n",
    "    json.dump(log, f)\n",
    "\n",
    "print()\n",
    "print(\"-- ALL FINISHED --\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7206b23-844c-41c4-b264-5a567d963684",
   "metadata": {},
   "source": [
    "# New Section"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c851b85-8a48-4241-b346-b80c7661d212",
   "metadata": {},
   "source": [
    "# Correction 3 (with many iteration)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b698734-ed67-42f9-91bd-44cf639a866d",
   "metadata": {},
   "source": [
    "## bigcodebench"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddb4f547-1ec9-420d-8e7f-eeb3b9fe73ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_iteration = 10\n",
    "max_request = 50  # max_request to AsyncEngine\n",
    "load_file = \"code-log5.json\"\n",
    "save_file = \"code-log5-corr1.json\"\n",
    "save_dir_base = \"progless_log9\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f29c499-c924-4dca-b3a7-2b6e4bb3fa7a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import os, json, re, traceback\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "with open(load_file) as json_file:\n",
    "    log = json.load(json_file)\n",
    "\n",
    "total_num_problem = log[\"num_problems\"]\n",
    "total_num_sample = log[\"num_sample\"]*log[\"num_problems\"]\n",
    "\n",
    "if not os.path.exists(save_dir_base):\n",
    "    os.makedirs(save_dir_base)\n",
    "    \n",
    "# advice_result_log: {str(iteration):{str(problem_id):{\"num_problem\":, \"num_correct\":}}\n",
    "advice_result_log = {}\n",
    "\n",
    "def add_numbers_to_lines(text):\n",
    "    # Split the text into lines\n",
    "    lines = text.split('\\n\\n')\n",
    "\n",
    "    # Initialize a counter\n",
    "    counter = 1\n",
    "\n",
    "    # Create a list to hold the numbered lines\n",
    "    numbered_lines = []\n",
    "    numbered_texts = []\n",
    "\n",
    "    # Iterate through the lines\n",
    "    for line in lines:\n",
    "        if line.strip():  # Check if the line is not empty\n",
    "            # Add the number and the line to the list\n",
    "            numbered_lines.append((counter, line))\n",
    "            numbered_texts.append(f\"{counter}. {line}\")\n",
    "            # Increment the counter\n",
    "            counter += 1\n",
    "\n",
    "    numbered_text = '\\n\\n'.join(numbered_texts)    \n",
    "\n",
    "    return numbered_lines, numbered_text\n",
    "\n",
    "\n",
    "def get_text_before_number(numbered_lines, number):\n",
    "    # Find the index of the tuple with the given number\n",
    "    for i, (num, line) in enumerate(numbered_lines):\n",
    "        if num == number:\n",
    "            # Return the original text before the given number\n",
    "            return '\\n\\n'.join(line for _, line in numbered_lines[:i])\n",
    "\n",
    "    # If the number is not found, return an empty string\n",
    "    return \"\"\n",
    "\n",
    "\n",
    "def extract_text_inside_backticks(text, arbitrary_text):\n",
    "    # Define the pattern to match the text inside ``` that follows the arbitrary text\n",
    "    pattern = re.compile(r'```{}\\s*([\\s\\S]*?)\\s*```'.format(re.escape(arbitrary_text)))\n",
    "\n",
    "    # Search for the pattern in the text\n",
    "    match = pattern.search(text)\n",
    "\n",
    "    if match:\n",
    "        return match.group(1).strip()\n",
    "    else:\n",
    "        return None\n",
    "\n",
    "\n",
    "# Define get_result (Optional): If you want to evaluate the output somehow while running process_requests, you can define get_result function and pass it to process_requests method. This will let you read log files easily with some evaluation.\n",
    "def get_result(request_dict, save_dir):\n",
    "    # request_dict: {\"prompt\":, \"output\":}\n",
    "    # save_dir: this is a directory path for progress log. put your evaluation file in here\n",
    "\n",
    "    problem_id = int(request_dict[\"log_ids\"][0])\n",
    "    output = request_dict[\"output\"]\n",
    "    eval_file_path = f\"{save_dir}/score.json\"\n",
    "\n",
    "    if os.path.exists(eval_file_path):\n",
    "        with open(eval_file_path) as f:\n",
    "            log = json.load(f)\n",
    "        if str(problem_id) in log[\"num_answered\"]:\n",
    "            num_answered = log[\"num_answered\"][str(problem_id)]\n",
    "            num_correct = log[\"num_correct\"][str(problem_id)]\n",
    "        else:\n",
    "            num_answered = 0\n",
    "            num_correct = 0\n",
    "    else:\n",
    "        log = {\"num_answered\":{}, \"num_correct\":{}}\n",
    "        num_answered = 0\n",
    "        num_correct = 0\n",
    "\n",
    "    is_correct = False\n",
    "    pattern = r'\\\\boxed{(\\d+)}'\n",
    "    matches = re.findall(pattern, output)\n",
    "    if matches == []:\n",
    "        final_answer = None\n",
    "    else:\n",
    "        final_answer = int(matches[0])\n",
    "        if correct_answers[problem_id] == final_answer:\n",
    "            is_correct = True\n",
    "            num_correct += 1\n",
    "\n",
    "    request_dict[\"final_answer\"] = final_answer\n",
    "    request_dict[\"is_correct\"] = is_correct\n",
    "\n",
    "    num_answered += 1\n",
    "    log[\"num_answered\"][str(problem_id)] = num_answered\n",
    "    log[\"num_correct\"][str(problem_id)] = num_correct\n",
    "\n",
    "    with open(eval_file_path, \"w\") as f:\n",
    "        json.dump(log, f)\n",
    "\n",
    "    return request_dict\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def get_log_dict(log, log_ids):\n",
    "    if len(log_ids) == 0:\n",
    "        return log\n",
    "    \n",
    "    problem_id = log_ids.pop(0)\n",
    "    log_dict = log[str(problem_id)]\n",
    "    \n",
    "    for log_id in log_ids:\n",
    "        log_dict = log_dict[\"children\"][str(log_id)]\n",
    "\n",
    "    return log_dict\n",
    "\n",
    "\n",
    "def get_edit_log_dict(log_ids):\n",
    "    global log\n",
    "    \n",
    "    if len(log_ids) == 0:\n",
    "        return log\n",
    "    \n",
    "    problem_id = log_ids.pop(0)\n",
    "    log_dict = log[str(problem_id)]\n",
    "    \n",
    "    for log_id in log_ids:\n",
    "        log_dict = log_dict[\"children\"][str(log_id)]\n",
    "\n",
    "    return log_dict\n",
    "\n",
    "\n",
    "\n",
    "for iter in range(num_iteration):\n",
    "\n",
    "    all_requests1 = AllRequests(max_request)\n",
    "    \n",
    "    # Make all_request for advice by searching log recursively\n",
    "    def search_log(log_dict, log_ids):  # node_id: str\n",
    "        global all_requests, request_id\n",
    "        node_id = log_ids[-1]\n",
    "\n",
    "        if type(log_dict[node_id]) != dict:\n",
    "            return None\n",
    "            \n",
    "        if \"children\" in log_dict[node_id]:\n",
    "            for next_node_id in log_dict[node_id][\"children\"]:\n",
    "                search_log(log_dict[node_id][\"children\"], log_ids + [next_node_id])\n",
    "        elif \"corrects\" in log_dict[node_id]:\n",
    "            if iter!=0:\n",
    "                all_false = True\n",
    "                for sample_id_str in log_dict[node_id][\"corrects\"]:\n",
    "                    if log_dict[node_id][\"corrects\"][sample_id_str]:\n",
    "                        all_false = False\n",
    "                        break\n",
    "                    \n",
    "                if all_false:\n",
    "                    problem_id_str = log_ids[0]\n",
    "                    problem_id = int(problem_id_str)\n",
    "                    pre_prompt = log_dict[node_id][\"prompts\"][\"0\"]\n",
    "                    pre_output = log_dict[node_id][\"outputs\"][\"0\"]\n",
    "                    error = log_dict[node_id][\"errors\"][\"0\"]\n",
    "                    traceback_ = log_dict[node_id][\"tracebacks\"][\"0\"]\n",
    "                    #student_answer = prompt.split(\"<｜Assistant｜>\")[1] + output  #log_dict[node_id][\"outputs\"][sample_id_str]\n",
    "                    #numbered_lines, numbered_answer = add_numbers_to_lines(student_answer)\n",
    "\n",
    "                    messages = [\n",
    "                        {\"role\": \"user\", \"content\": f\"\"\"### Problem:\n",
    "'''\n",
    "{instruct_prompt[problem_id]}\n",
    "'''\n",
    "\n",
    "\n",
    "### Correct Solution:\n",
    "'''\n",
    "{canonical_solution[problem_id]}\n",
    "'''\n",
    "\n",
    "\n",
    "### Student's Incorrect Answer:\n",
    "'''\n",
    "{pre_output}\n",
    "'''\n",
    "\n",
    "\n",
    "### Test Code and Its Error\n",
    "'''\n",
    "```\n",
    "{test[problem_id]}\n",
    "```\n",
    "\n",
    "{traceback_}\n",
    "'''\n",
    "\n",
    "\n",
    "You are an advanced language model tasked with analyzing a student’s answer to a coding problems and make some instructions to lead him to the correct solution. You are given the coding problem, the correct solution of it, a student’s incorrect answer, test code of the answer code and error cause of the student's incorrect answer. Please make some instructions and let him answer correctly following the instructions below.\n",
    "\n",
    "\n",
    "### Instructions:\n",
    "1. **Think Why Student’s Answer was Wrong**: Compare the correct solution and student’s incorrect answer, and analyze why the student’s answer was wrong and think about where it went in a different direction from the correct solution.\n",
    "2. **Think What was the Idea Missing in Student’s Answer**: Think what idea was included in the correct solution but was missing from the students' answer.\n",
    "3. **Imagine Many Thinking Processes Which May Lead to the Idea**: Imagine as many thinking processes as possible which may lead him to think of the missing idea.\n",
    "4. **Give Short and Abstract Instructions**: Expanding your imagination, make as many instructions as possible which may lead him to the missing idea which was not included in the student’s answer. All the instructions should be abstract and general so that it can be applied to other problems too. These are the examples of the instruction; “Explore all the possibilities of it”, “Check if there are enough conditions to solve the problem”, “Imagine what condition will lead you to solve the problem”, “Find some regularities and prove a statement which narrows down the options”, “Summarize your thought and check if it really follows the problem”.\n",
    "5. **Generate Output**: Based on the result so far, return the missing idea and Instructions in backticks like\n",
    "\n",
    "```idea\n",
    "(The missing idea in the student’s answer)\n",
    "```\n",
    "\n",
    "```instructions\n",
    "[\n",
    "    \"Instruction1 (An instruction which leads him to the missing idea)\",\n",
    "    ...\n",
    "]\n",
    "```\n",
    "\n",
    "\n",
    "Let’s think step by step following each step of the instructions.\"\"\"}\n",
    "                    ]\n",
    "                    \n",
    "                    prompt = tokenizer.apply_chat_template(\n",
    "                        messages,\n",
    "                        tokenize=False,\n",
    "                        add_generation_prompt=True\n",
    "                    )\n",
    "                    \n",
    "                    new_log_ids = log_ids+[\"0\"]\n",
    "                    request_dict = {\"log_ids\":new_log_ids, \"prompt\":prompt, \"pre_prompt\":pre_prompt, \"pre_output\":pre_output}\n",
    "                    all_requests1.add(request_dict)\n",
    "                    \n",
    "            else:\n",
    "                for sample_id_str in log_dict[node_id][\"corrects\"]:\n",
    "                    if not log_dict[node_id][\"corrects\"][sample_id_str]:\n",
    "        \n",
    "                        problem_id_str = log_ids[0]\n",
    "                        problem_id = int(problem_id_str)\n",
    "                        pre_prompt = log_dict[node_id][\"prompts\"][sample_id_str]\n",
    "                        pre_output = log_dict[node_id][\"outputs\"][sample_id_str]\n",
    "                        error = log_dict[node_id][\"errors\"][sample_id_str]\n",
    "                        traceback_ = log_dict[node_id][\"tracebacks\"][sample_id_str]\n",
    "                        #student_answer = prompt.split(\"<｜Assistant｜>\")[1] + output  #log_dict[node_id][\"outputs\"][sample_id_str]\n",
    "                        #numbered_lines, numbered_answer = add_numbers_to_lines(student_answer)\n",
    "    \n",
    "                        messages = [\n",
    "                            {\"role\": \"user\", \"content\": f\"\"\"### Problem:\n",
    "'''\n",
    "{instruct_prompt[problem_id]}\n",
    "'''\n",
    "\n",
    "\n",
    "### Correct Solution:\n",
    "'''\n",
    "{canonical_solution[problem_id]}\n",
    "'''\n",
    "\n",
    "\n",
    "### Student's Incorrect Answer:\n",
    "'''\n",
    "{pre_output}\n",
    "'''\n",
    "\n",
    "\n",
    "### Test Code and Its Error\n",
    "'''\n",
    "```\n",
    "{test[problem_id]}\n",
    "```\n",
    "\n",
    "{traceback_}\n",
    "'''\n",
    "\n",
    "\n",
    "You are an advanced language model tasked with analyzing a student’s answer to a coding problems and make some instructions to lead him to the correct solution. You are given the coding problem, the correct solution of it, a student’s incorrect answer, test code of the answer code and error cause of the student's incorrect answer. Please make some instructions and let him answer correctly following the instructions below.\n",
    "\n",
    "\n",
    "### Instructions:\n",
    "1. **Think Why Student’s Answer was Wrong**: Compare the correct solution and student’s incorrect answer, and analyze why the student’s answer was wrong and think about where it went in a different direction from the correct solution.\n",
    "2. **Think What was the Idea Missing in Student’s Answer**: Think what idea was included in the correct solution but was missing from the students' answer.\n",
    "3. **Imagine Many Thinking Processes Which May Lead to the Idea**: Imagine as many thinking processes as possible which may lead him to think of the missing idea.\n",
    "4. **Give Short and Abstract Instructions**: Expanding your imagination, make as many instructions as possible which may lead him to the missing idea which was not included in the student’s answer. All the instructions should be abstract and general so that it can be applied to other problems too. These are the examples of the instruction; “Explore all the possibilities of it”, “Check if there are enough conditions to solve the problem”, “Imagine what condition will lead you to solve the problem”, “Find some regularities and prove a statement which narrows down the options”, “Summarize your thought and check if it really follows the problem”.\n",
    "5. **Generate Output**: Based on the result so far, return the missing idea and Instructions in backticks like\n",
    "\n",
    "```idea\n",
    "(The missing idea in the student’s answer)\n",
    "```\n",
    "\n",
    "```instructions\n",
    "[\n",
    "    \"Instruction1 (An instruction which leads him to the missing idea)\",\n",
    "    ...\n",
    "]\n",
    "```\n",
    "\n",
    "\n",
    "Let’s think step by step following each step of the instructions.\"\"\"}\n",
    "                        ]\n",
    "                        \n",
    "                        prompt = tokenizer.apply_chat_template(\n",
    "                            messages,\n",
    "                            tokenize=False,\n",
    "                            add_generation_prompt=True\n",
    "                        )\n",
    "                        \n",
    "                        new_log_ids = log_ids+[sample_id_str]\n",
    "                        request_dict = {\"log_ids\":new_log_ids, \"prompt\":prompt, \"pre_prompt\":pre_prompt, \"pre_output\":pre_output}\n",
    "                        all_requests1.add(request_dict)\n",
    "    \n",
    "    for problem_id_str in log:\n",
    "        search_log(log, [problem_id_str])\n",
    "    \n",
    "    \n",
    "    # Process all_requests\n",
    "    # If you had some error in last process and want to continue to get the output, set restart = True\n",
    "    all_results1 = await all_requests1.process(max_tokens = 15000, restart = True, save_dir=f\"{save_dir_base}/inst-{iter}\")\n",
    "    all_requests2 = AllRequests(max_request)\n",
    "\n",
    "    error_num = 0\n",
    "    num_results = len(all_results1)\n",
    "    for result_dict in all_results1:\n",
    "        log_ids = result_dict[\"log_ids\"]\n",
    "        prompt = result_dict[\"prompt\"]\n",
    "        output = result_dict[\"output\"]\n",
    "        pre_prompt = result_dict[\"pre_prompt\"]\n",
    "        pre_output = result_dict[\"pre_output\"]\n",
    "        instruction_log = prompt + output\n",
    "\n",
    "        missing_idea = extract_text_inside_backticks(output, \"idea\")\n",
    "        instruction_list_text = extract_text_inside_backticks(output, \"instructions\")\n",
    "        \n",
    "        if missing_idea and instruction_list_text:\n",
    "            try:\n",
    "                instruction_list_text = instruction_list_text.replace(\"\\n\",\"\")\n",
    "                instruction_list_text = instruction_list_text.replace(\"\\\\\",\"\")\n",
    "                pattern = r\"'((?:[^']|'(?!\\s*[,\\]]))*)'\"\n",
    "                replacement = r'\"\\1\"'\n",
    "                #instruction_list_text = re.sub(pattern, replacement, instruction_list_text)  # convert ['I'm a cat', 'This is the student's car',] into [\"I'm a cat\", \"This is the student's car\",]\n",
    "                instruction_list = json.loads(instruction_list_text)\n",
    "            except Exception as e:\n",
    "                traceback.print_exc()\n",
    "                error_num += 1\n",
    "                print()\n",
    "                print(\"An error occurred:\", e)\n",
    "                print(\"instruction_list_text: \", instruction_list_text)\n",
    "                print(\"error_num: \", error_num)\n",
    "                continue\n",
    "    \n",
    "            problem_id = int(log_ids[0])\n",
    "            problem = instruct_prompt[problem_id]\n",
    "\n",
    "            prompts_dict = {}\n",
    "            insert_ids=[]\n",
    "            instruction_ids=[]\n",
    "            prompt_id = 0\n",
    "            for instruction_id, instruction in enumerate(instruction_list):\n",
    "                modified_insts = instruction_list[:instruction_id] + instruction_list[(instruction_id+1):]\n",
    "                advices_text = \"\"\n",
    "                for i, inst in enumerate(modified_insts):\n",
    "                    advices_text += f\"\\nAdvice{i+1}: {inst}\"\n",
    "                    \n",
    "                messages = [\n",
    "                    {\"role\": \"user\", \"content\": pre_prompt[29:][:-13]},\n",
    "                    {\"role\": \"assistant\", \"content\": pre_output},\n",
    "                    {\"role\": \"user\", \"content\": f\"\"\"Your answer might contain some errors. Please revise your answer following the advice below;\n",
    "{advices_text}\"\"\"}\n",
    "                ]\n",
    "                    \n",
    "                new_prompt = tokenizer.apply_chat_template(\n",
    "                    messages,\n",
    "                    tokenize=False,\n",
    "                    add_generation_prompt=True\n",
    "                )\n",
    "\n",
    "                prompts_dict[str(prompt_id)] = new_prompt\n",
    "\n",
    "                next_request = {\"log_ids\":log_ids+[str(instruction_id)], \"prompt\":new_prompt, \"instruction_log\":[instruction_log]}\n",
    "                all_requests2.add(next_request)\n",
    "                \n",
    "            edit_log_dict = get_edit_log_dict(log_ids[:-1])\n",
    "            if \"children\" in edit_log_dict:\n",
    "                edit_log_dict[\"children\"][str(log_ids[-1])] = {\"instruction_list\":instruction_list, \"prompts\":prompts_dict, \"instruction_log\":[instruction_log]}\n",
    "            else:\n",
    "                edit_log_dict[\"children\"] = {str(log_ids[-1]):{\"instruction_list\":instruction_list, \"prompts\":prompts_dict, \"instruction_log\":[instruction_log]}}\n",
    "\n",
    "\n",
    "    with open(save_file, \"w\") as f:\n",
    "        json.dump(log, f)\n",
    "\n",
    "    print(\"log saved\")\n",
    "    \n",
    "    # Process all_requests\n",
    "    # If you had trouble in last process and want to continue to get the output, set restart = True\n",
    "    all_results2 = await all_requests2.process(max_tokens = 15000, restart = True, save_dir=f\"{save_dir_base}/solve-{iter}\")\n",
    "\n",
    "    advice_result_log[str(iter)] = {\"num_problem\":{}, \"num_correct\":{},}\n",
    "    test_cases = []\n",
    "    candidates = []\n",
    "    problem_ids = []\n",
    "    sample_ids = []\n",
    "    log_ids_list = []\n",
    "    # Check if the outputs are correct\n",
    "    for results_dict in all_results2:\n",
    "        log_ids = results_dict[\"log_ids\"]\n",
    "        prompt = results_dict[\"prompt\"]\n",
    "        output = results_dict[\"output\"]\n",
    "        problem_id = int(log_ids[0])\n",
    "        sample_id = int(log_ids[-1])\n",
    "        test_code = test[problem_id]\n",
    "\n",
    "        output_code = extract_text_inside_backticks(output, \"python\")\n",
    "        if not output_code: output_code = extract_text_inside_backticks(output, \"\")\n",
    "        if not output_code: output_code = \"\"\n",
    "\n",
    "        edit_log_dict = get_edit_log_dict(log_ids[:-1])\n",
    "\n",
    "        if \"prompts\" in edit_log_dict:\n",
    "            edit_log_dict[\"prompts\"][str(sample_id)] = prompt\n",
    "        else:\n",
    "            edit_log_dict[\"prompts\"] = {str(sample_id):prompt}\n",
    "\n",
    "        if \"outputs\" in edit_log_dict:\n",
    "            edit_log_dict[\"outputs\"][str(sample_id)] = output\n",
    "        else:\n",
    "            edit_log_dict[\"outputs\"] = {str(sample_id):output}\n",
    "\n",
    "        if \"output_codes\" in edit_log_dict:\n",
    "            edit_log_dict[\"output_codes\"][str(sample_id)] = output_code\n",
    "        else:\n",
    "            edit_log_dict[\"output_codes\"] = {str(sample_id):output_code}\n",
    "\n",
    "        test_cases.append(test_code)\n",
    "        candidates.append([output_code])\n",
    "        problem_ids.append(problem_id)\n",
    "        sample_ids.append(sample_id)\n",
    "        log_ids_list.append(log_ids)\n",
    "    \n",
    "    import os, time\n",
    "    os.environ[\"HF_ALLOW_CODE_EVAL\"] = \"1\"\n",
    "    from code_eval.code_eval import CodeEval\n",
    "    code_eval_metric = CodeEval()\n",
    "    # Compute pass@k\n",
    "    k_values = [1]\n",
    "    print(\"Evaluating generated code...\")\n",
    "    start = time.time()\n",
    "    pass_at_k, results = code_eval_metric._compute(\n",
    "        references=test_cases,\n",
    "        predictions=candidates,\n",
    "        k=k_values,\n",
    "        num_workers=10,  # Adjust based on your system\n",
    "        timeout=150.0,   # Adjust the timeout as needed\n",
    "    )\n",
    "    end = time.time()\n",
    "    print(\"calculation time(s): \", end-start)\n",
    "    \n",
    "    for i in range(len(results)):\n",
    "        problem_id = problem_ids[i]\n",
    "        sample_id = sample_ids[i]\n",
    "        unexpected_error = False\n",
    "        if results[problem_id] == []:\n",
    "            is_correct = False  # [] appeared sometimes for unknown reason. I define it as incorrect for now, but it should be fixed.\n",
    "            unexpected_error = True\n",
    "        else: is_correct = results[problem_id][0][1][\"passed\"]\n",
    "        \n",
    "        log_ids = log_ids_list[i]\n",
    "        edit_log_dict = get_edit_log_dict(log_ids[:-1])\n",
    "\n",
    "        if \"corrects\" in edit_log_dict:\n",
    "            edit_log_dict[\"corrects\"][str(sample_id)] = is_correct\n",
    "        else:\n",
    "            edit_log_dict[\"corrects\"] = {str(sample_id):is_correct}\n",
    "\n",
    "        if not is_correct:\n",
    "            if not unexpected_error:\n",
    "                error = results[problem_id][0][1][\"result\"][\"error\"]\n",
    "                traceback_ = results[problem_id][0][1][\"result\"][\"traceback\"]\n",
    "            else:\n",
    "                error = \"\"\n",
    "                traceback_ = \"\"\n",
    "    \n",
    "            if \"errors\" in edit_log_dict:\n",
    "                edit_log_dict[\"errors\"][str(sample_id)] = error\n",
    "            else:\n",
    "                edit_log_dict[\"errors\"] = {str(sample_id):error}\n",
    "    \n",
    "            if \"tracebacks\" in edit_log_dict:\n",
    "                edit_log_dict[\"tracebacks\"][str(sample_id)] = traceback_\n",
    "            else:\n",
    "                edit_log_dict[\"tracebacks\"] = {str(sample_id):traceback_}\n",
    "    \n",
    "        if str(log_ids[0]) in advice_result_log[str(iter)][\"num_correct\"]:\n",
    "            if is_correct:\n",
    "                advice_result_log[str(iter)][\"num_correct\"][str(log_ids[0])] += 1\n",
    "        else:\n",
    "            if is_correct:\n",
    "                advice_result_log[str(iter)][\"num_correct\"][str(log_ids[0])] = 1\n",
    "            else:\n",
    "                advice_result_log[str(iter)][\"num_correct\"][str(log_ids[0])] = 0\n",
    "\n",
    "        if str(log_ids[0]) in advice_result_log[str(iter)][\"num_problem\"]:\n",
    "            advice_result_log[str(iter)][\"num_problem\"][str(log_ids[0])] += 1\n",
    "        else:\n",
    "            advice_result_log[str(iter)][\"num_problem\"][str(log_ids[0])] = 1\n",
    "\n",
    "\n",
    "    num_problem = 0\n",
    "    num_sample = 0\n",
    "    pass1_count = 0\n",
    "    passAll_count = 0\n",
    "    for problem_id_str in advice_result_log[str(iter)][\"num_problem\"]:\n",
    "        num_problem += 1\n",
    "        num_sample += advice_result_log[str(iter)][\"num_problem\"][problem_id_str]\n",
    "    for problem_id_str in advice_result_log[str(iter)][\"num_correct\"]:\n",
    "        pass1_count += advice_result_log[str(iter)][\"num_correct\"][problem_id_str]\n",
    "        if advice_result_log[str(iter)][\"num_correct\"][problem_id_str] > 0:\n",
    "            passAll_count += 1\n",
    "\n",
    "    num_already_correct_problem = total_num_problem - len(all_results1)\n",
    "\n",
    "    print(\"total_num_problem: \", total_num_problem)\n",
    "    print(\"total_num_sample: \", total_num_sample)\n",
    "    print(\"num_already_correct_problem: \", num_already_correct_problem)\n",
    "    print(f\"{passAll_count}/{num_problem} problems have got at least 1 correct sample in this iteration\")\n",
    "    print(f\"{pass1_count}/{num_sample} samples were correct in total\")\n",
    "    \n",
    "    pass1 = pass1_count/num_sample\n",
    "    passAll = passAll_count/num_problem\n",
    "\n",
    "    advice_result_log[str(iter)][\"pass@1\"] = pass1\n",
    "    advice_result_log[str(iter)][\"passAll\"] = passAll\n",
    "    print(\"pass1 in this iteration: \", pass1)\n",
    "    print(\"passAll in this iteration: \", passAll)\n",
    "    \n",
    "    \n",
    "    log[\"advice_result_log\"] = advice_result_log\n",
    "    with open(save_file, \"w\") as json_file:\n",
    "        json.dump(log, json_file)\n",
    "\n",
    "print()\n",
    "print(\"-- ALL FINISHED --\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a16cdfa-98f2-411b-a781-6dbfd6fa74e1",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## AIME"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16a69418-ca3b-4cb8-89b6-84f59b4f071f",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_iteration = 10\n",
    "max_request = 40  # max_request to AsyncEngine\n",
    "load_file = \"aime_log1.json\"\n",
    "save_file = \"aime_log1_advice1.json\"\n",
    "save_dir_base = f\"{save_file[:-5]}-progless_log\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae235eca-5e9f-43cf-ae17-d6876719c01a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import os, json, re, traceback\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "with open(load_file) as json_file:\n",
    "    log = json.load(json_file)\n",
    "\n",
    "total_num_problem = log[\"num_problems\"]\n",
    "total_num_sample = log[\"num_sample\"]*log[\"num_problems\"]\n",
    "\n",
    "if not os.path.exists(save_dir_base):\n",
    "    os.makedirs(save_dir_base)\n",
    "    \n",
    "# advice_result_log: {str(iteration):{str(problem_id):{\"num_problem\":, \"num_correct\":}}\n",
    "advice_result_log = {}\n",
    "\n",
    "def add_numbers_to_lines(text):\n",
    "    # Split the text into lines\n",
    "    lines = text.split('\\n\\n')\n",
    "\n",
    "    # Initialize a counter\n",
    "    counter = 1\n",
    "\n",
    "    # Create a list to hold the numbered lines\n",
    "    numbered_lines = []\n",
    "    numbered_texts = []\n",
    "\n",
    "    # Iterate through the lines\n",
    "    for line in lines:\n",
    "        if line.strip():  # Check if the line is not empty\n",
    "            # Add the number and the line to the list\n",
    "            numbered_lines.append((counter, line))\n",
    "            numbered_texts.append(f\"{counter}. {line}\")\n",
    "            # Increment the counter\n",
    "            counter += 1\n",
    "\n",
    "    numbered_text = '\\n\\n'.join(numbered_texts)    \n",
    "\n",
    "    return numbered_lines, numbered_text\n",
    "\n",
    "\n",
    "def get_text_before_number(numbered_lines, number):\n",
    "    # Find the index of the tuple with the given number\n",
    "    for i, (num, line) in enumerate(numbered_lines):\n",
    "        if num == number:\n",
    "            # Return the original text before the given number\n",
    "            return '\\n\\n'.join(line for _, line in numbered_lines[:i])\n",
    "\n",
    "    # If the number is not found, return an empty string\n",
    "    return \"\"\n",
    "\n",
    "\n",
    "def extract_text_inside_backticks(text, arbitrary_text):\n",
    "    # Define the pattern to match the text inside ``` that follows the arbitrary text\n",
    "    pattern = re.compile(r'```{}\\s*([\\s\\S]*?)\\s*```'.format(re.escape(arbitrary_text)))\n",
    "\n",
    "    # Search for the pattern in the text\n",
    "    match = pattern.search(text)\n",
    "\n",
    "    if match:\n",
    "        return match.group(1).strip()\n",
    "    else:\n",
    "        return None\n",
    "\n",
    "\n",
    "# Define get_result (Optional): If you want to evaluate the output somehow while running process_requests, you can define get_result function and pass it to process_requests method. This will let you read log files easily with some evaluation.\n",
    "def get_result(request_dict, save_dir):\n",
    "    # request_dict: {\"prompt\":, \"output\":}\n",
    "    # save_dir: this is a directory path for progress log. put your evaluation file in here\n",
    "\n",
    "    problem_id = int(request_dict[\"log_ids\"][0])\n",
    "    output = request_dict[\"output\"]\n",
    "    eval_file_path = f\"{save_dir}/score.json\"\n",
    "\n",
    "    if os.path.exists(eval_file_path):\n",
    "        with open(eval_file_path) as f:\n",
    "            log = json.load(f)\n",
    "        if str(problem_id) in log[\"num_answered\"]:\n",
    "            num_answered = log[\"num_answered\"][str(problem_id)]\n",
    "            num_correct = log[\"num_correct\"][str(problem_id)]\n",
    "        else:\n",
    "            num_answered = 0\n",
    "            num_correct = 0\n",
    "    else:\n",
    "        log = {\"num_answered\":{}, \"num_correct\":{}}\n",
    "        num_answered = 0\n",
    "        num_correct = 0\n",
    "\n",
    "    is_correct = False\n",
    "    pattern = r'\\\\boxed{(\\d+)}'\n",
    "    matches = re.findall(pattern, output)\n",
    "    if matches == []:\n",
    "        final_answer = None\n",
    "    else:\n",
    "        final_answer = int(matches[0])\n",
    "        if correct_answers[problem_id] == final_answer:\n",
    "            is_correct = True\n",
    "            num_correct += 1\n",
    "\n",
    "    request_dict[\"final_answer\"] = final_answer\n",
    "    request_dict[\"is_correct\"] = is_correct\n",
    "\n",
    "    num_answered += 1\n",
    "    log[\"num_answered\"][str(problem_id)] = num_answered\n",
    "    log[\"num_correct\"][str(problem_id)] = num_correct\n",
    "\n",
    "    with open(eval_file_path, \"w\") as f:\n",
    "        json.dump(log, f)\n",
    "\n",
    "    return request_dict\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def get_log_dict(log, log_ids):\n",
    "    if len(log_ids) == 0:\n",
    "        return log\n",
    "    \n",
    "    problem_id = log_ids.pop(0)\n",
    "    log_dict = log[str(problem_id)]\n",
    "    \n",
    "    for log_id in log_ids:\n",
    "        log_dict = log_dict[\"children\"][str(log_id)]\n",
    "\n",
    "    return log_dict\n",
    "\n",
    "\n",
    "def get_edit_log_dict(log_ids):\n",
    "    global log\n",
    "    \n",
    "    if len(log_ids) == 0:\n",
    "        return log\n",
    "    \n",
    "    problem_id = log_ids.pop(0)\n",
    "    log_dict = log[str(problem_id)]\n",
    "    \n",
    "    for log_id in log_ids:\n",
    "        log_dict = log_dict[\"children\"][str(log_id)]\n",
    "\n",
    "    return log_dict\n",
    "\n",
    "\n",
    "\n",
    "for iter in range(num_iteration):\n",
    "\n",
    "    all_requests1 = AllRequests(max_request)\n",
    "    \n",
    "    # Make all_request for advice by searching log recursively\n",
    "    def search_log(log_dict, log_ids):  # node_id: str\n",
    "        global all_requests, request_id\n",
    "        node_id = log_ids[-1]\n",
    "\n",
    "        if type(log_dict[node_id]) != dict:\n",
    "            return None\n",
    "            \n",
    "        if \"children\" in log_dict[node_id]:\n",
    "            for next_node_id in log_dict[node_id][\"children\"]:\n",
    "                search_log(log_dict[node_id][\"children\"], log_ids + [next_node_id])\n",
    "        elif \"corrects\" in log_dict[node_id]:\n",
    "            if iter>=0:\n",
    "                all_false = True\n",
    "                for sample_id_str in log_dict[node_id][\"corrects\"]:\n",
    "                    if log_dict[node_id][\"corrects\"][sample_id_str]:\n",
    "                        all_false = False\n",
    "                        break\n",
    "                    \n",
    "                if all_false:\n",
    "                    problem_id_str = log_ids[0]\n",
    "                    problem_id = int(problem_id_str)\n",
    "                    pre_prompt = log_dict[node_id][\"prompts\"][\"0\"]\n",
    "                    pre_output = log_dict[node_id][\"outputs\"][\"0\"]\n",
    "\n",
    "                    messages = [\n",
    "                        {\"role\": \"user\", \"content\": f\"\"\"### Problem:\n",
    "'''\n",
    "{problems[problem_id]}\n",
    "'''\n",
    "\n",
    "\n",
    "### Correct Solution:\n",
    "'''\n",
    "{solutions[problem_id]}\n",
    "'''\n",
    "\n",
    "\n",
    "### Student's Incorrect Answer:\n",
    "'''\n",
    "{pre_output}\n",
    "'''\n",
    "\n",
    "\n",
    "You are an advanced language model tasked with analyzing a student’s answer to a mathematical problem and make some instructions to lead him to a correct solution. You are given the math problem, the correct solution of it, and a student’s incorrect answer. Please make some instructions and let him answer correctly following the instructions below.\n",
    "\n",
    "\n",
    "### Instructions:\n",
    "1. **Think Why Student’s Answer was Wrong**: Compare the correct solution and student’s incorrect answer, and analyze why the student’s answer was wrong and think about where it went in a different direction from the correct solution.\n",
    "2. **Think What was the Idea Missing in Student’s Answer**: Think what idea was included in the correct solution but was missing from the students' answer.\n",
    "3. **Imagine Many Thinking Processes Which May Lead to the Idea**: Imagine as many thinking processes as possible which may lead him to think of the missing idea.\n",
    "4. **Give Short and Abstract Instructions**: Expanding your imagination, make as many instructions as possible which may lead him to the missing idea which was not included in the student’s answer. All the instructions should be abstract and general so that it can be applied to other problems too. These are the examples of the instructions; “Explore all the possibilities of it”, “Check if there are enough conditions to solve the problem”, “Imagine what condition will lead you to solve the problem”, “Find some regularities and prove a statement which narrows down the options”, “Summarize your thought and check if it really follows the problem”.\n",
    "5. **Generate Output**: Based on the result so far, return the missing idea and Instructions in backticks like\n",
    "\n",
    "```idea\n",
    "(The missing idea in the student’s answer)\n",
    "```\n",
    "\n",
    "```instructions\n",
    "[\n",
    "    “Instruction1 (An instruction which leads him to the missing idea)\",\n",
    "    ...\n",
    "]\n",
    "```\n",
    "\n",
    "\n",
    "Let’s think step by step following each step of the instructions.\"\"\"}\n",
    "                    ]\n",
    "                    \n",
    "                    prompt = tokenizer.apply_chat_template(\n",
    "                        messages,\n",
    "                        tokenize=False,\n",
    "                        add_generation_prompt=True\n",
    "                    )\n",
    "                    \n",
    "                    new_log_ids = log_ids+[\"0\"]\n",
    "                    request_dict = {\"log_ids\":new_log_ids, \"prompt\":prompt, \"pre_prompt\":pre_prompt, \"pre_output\":pre_output}\n",
    "                    all_requests1.add(request_dict)\n",
    "                    \n",
    "            else:\n",
    "                for sample_id_str in log_dict[node_id][\"corrects\"]:\n",
    "                    if not log_dict[node_id][\"corrects\"][sample_id_str]:\n",
    "        \n",
    "                        problem_id_str = log_ids[0]\n",
    "                        problem_id = int(problem_id_str)\n",
    "                        pre_prompt = log_dict[node_id][\"prompts\"][sample_id_str]\n",
    "                        pre_output = log_dict[node_id][\"outputs\"][sample_id_str]\n",
    "                        \n",
    "                        messages = [\n",
    "                            {\"role\": \"user\", \"content\": f\"\"\"### Problem:\n",
    "'''\n",
    "{problems[problem_id]}\n",
    "'''\n",
    "\n",
    "\n",
    "### Correct Solution:\n",
    "'''\n",
    "{solutions[problem_id]}\n",
    "'''\n",
    "\n",
    "\n",
    "### Student's Incorrect Answer:\n",
    "'''\n",
    "{pre_output}\n",
    "'''\n",
    "\n",
    "\n",
    "You are an advanced language model tasked with analyzing a student’s answer to a mathematical problem and make some instructions to lead him to a correct solution. You are given the math problem, the correct solution of it, and a student’s incorrect answer. Please make some instructions and let him answer correctly following the instructions below.\n",
    "\n",
    "\n",
    "### Instructions:\n",
    "1. **Think Why Student’s Answer was Wrong**: Compare the correct solution and student’s incorrect answer, and analyze why the student’s answer was wrong and think about where it went in a different direction from the correct solution.\n",
    "2. **Think What was the Idea Missing in Student’s Answer**: Think what idea was included in the correct solution but was missing from the students' answer.\n",
    "3. **Imagine Many Thinking Processes Which May Lead to the Idea**: Imagine as many thinking processes as possible which may lead him to think of the missing idea.\n",
    "4. **Give Short and Abstract Instructions**: Expanding your imagination, make as many instructions as possible which may lead him to the missing idea which was not included in the student’s answer. All the instructions should be abstract and general so that it can be applied to other problems too. These are the examples of the instructions; “Explore all the possibilities of it”, “Check if there are enough conditions to solve the problem”, “Imagine what condition will lead you to solve the problem”, “Find some regularities and prove a statement which narrows down the options”, “Summarize your thought and check if it really follows the problem”.\n",
    "5. **Generate Output**: Based on the result so far, return the missing idea and Instructions in backticks like\n",
    "\n",
    "```idea\n",
    "(The missing idea in the student’s answer)\n",
    "```\n",
    "\n",
    "```instructions\n",
    "[\n",
    "    “Instruction1 (An instruction which leads him to the missing idea)\",\n",
    "    ...\n",
    "]\n",
    "```\n",
    "\n",
    "\n",
    "Let’s think step by step following each step of the instructions.\"\"\"}\n",
    "                        ]\n",
    "                        \n",
    "                        prompt = tokenizer.apply_chat_template(\n",
    "                            messages,\n",
    "                            tokenize=False,\n",
    "                            add_generation_prompt=True\n",
    "                        )\n",
    "                        \n",
    "                        new_log_ids = log_ids+[sample_id_str]\n",
    "                        request_dict = {\"log_ids\":new_log_ids, \"prompt\":prompt, \"pre_prompt\":pre_prompt, \"pre_output\":pre_output}\n",
    "                        all_requests1.add(request_dict)\n",
    "    \n",
    "    for problem_id_str in log:\n",
    "        search_log(log, [problem_id_str])\n",
    "    \n",
    "    \n",
    "    # Process all_requests\n",
    "    # If you had some error in last process and want to continue to get the output, set restart = True\n",
    "    all_results1 = await all_requests1.process(max_tokens = 10000, restart = True, save_dir=f\"{save_dir_base}/inst-{iter}\")\n",
    "    all_requests2 = AllRequests(max_request)\n",
    "\n",
    "    error_num = 0\n",
    "    num_results = len(all_results1)\n",
    "    for result_dict in all_results1:\n",
    "        log_ids = result_dict[\"log_ids\"]\n",
    "        prompt = result_dict[\"prompt\"]\n",
    "        output = result_dict[\"output\"]\n",
    "        pre_prompt = result_dict[\"pre_prompt\"]\n",
    "        pre_output = result_dict[\"pre_output\"]\n",
    "        instruction_log = prompt + output\n",
    "\n",
    "        missing_idea = extract_text_inside_backticks(output, \"idea\")\n",
    "        instruction_list_text = extract_text_inside_backticks(output, \"instructions\")\n",
    "        \n",
    "        if missing_idea and instruction_list_text:\n",
    "            try:\n",
    "                instruction_list_text = instruction_list_text.replace(\"\\n\",\"\")\n",
    "                instruction_list_text = instruction_list_text.replace(\"\\\\\",\"\")\n",
    "                pattern = r\"'((?:[^']|'(?!\\s*[,\\]]))*)'\"\n",
    "                replacement = r'\"\\1\"'\n",
    "                #instruction_list_text = re.sub(pattern, replacement, instruction_list_text)  # convert ['I'm a cat', 'This is the student's car',] into [\"I'm a cat\", \"This is the student's car\",]\n",
    "                instruction_list = json.loads(instruction_list_text)\n",
    "            except Exception as e:\n",
    "                traceback.print_exc()\n",
    "                error_num += 1\n",
    "                print()\n",
    "                print(\"An error occurred:\", e)\n",
    "                print(\"instruction_list_text: \", instruction_list_text)\n",
    "                print(\"error_num: \", error_num)\n",
    "                continue\n",
    "    \n",
    "            problem_id = int(log_ids[0])\n",
    "            problem = problems[problem_id]\n",
    "\n",
    "            prompts_dict = {}\n",
    "            insert_ids=[]\n",
    "            instruction_ids=[]\n",
    "            prompt_id = 0\n",
    "            for instruction_id, instruction in enumerate(instruction_list):\n",
    "                modified_insts = instruction_list[:instruction_id] + instruction_list[(instruction_id+1):]\n",
    "                advices_text = \"\"\n",
    "                for i, inst in enumerate(modified_insts):\n",
    "                    advices_text += f\"\\nAdvice{i+1}: {inst}\"\n",
    "                    \n",
    "                messages = [\n",
    "                    {\"role\": \"user\", \"content\": pre_prompt[29:][:-13]},\n",
    "                    {\"role\": \"assistant\", \"content\": pre_output},\n",
    "                    {\"role\": \"user\", \"content\": f\"\"\"Your answer might contain some errors. Please revise your answer following the advice below;\n",
    "{advices_text}\n",
    "\n",
    "Please put your final answer within \\\\boxed{{}}. If final answer is a number larger than 1000, take module 1000.\"\"\"}\n",
    "                ]\n",
    "                    \n",
    "                new_prompt = tokenizer.apply_chat_template(\n",
    "                    messages,\n",
    "                    tokenize=False,\n",
    "                    add_generation_prompt=True\n",
    "                )\n",
    "\n",
    "                prompts_dict[str(prompt_id)] = new_prompt\n",
    "\n",
    "                next_request = {\"log_ids\":log_ids+[str(instruction_id)], \"prompt\":new_prompt}\n",
    "                all_requests2.add(next_request)\n",
    "                \n",
    "            edit_log_dict = get_edit_log_dict(log_ids[:-1])\n",
    "            if \"children\" in edit_log_dict:\n",
    "                edit_log_dict[\"children\"][str(log_ids[-1])] = {\"instruction_list\":instruction_list, \"prompts\":prompts_dict, \"instruction_log\":[instruction_log]}\n",
    "            else:\n",
    "                edit_log_dict[\"children\"] = {str(log_ids[-1]):{\"instruction_list\":instruction_list, \"prompts\":prompts_dict, \"instruction_log\":[instruction_log]}}\n",
    "\n",
    "\n",
    "    with open(save_file, \"w\") as f:\n",
    "        json.dump(log, f)\n",
    "\n",
    "    print(\"log saved\")\n",
    "    \n",
    "    # Process all_requests\n",
    "    # If you had trouble in last process and want to continue to get the output, set restart = True\n",
    "    all_results2 = await all_requests2.process(max_tokens = 15000, restart = True, save_dir=f\"{save_dir_base}/solve-{iter}\")\n",
    "\n",
    "    advice_result_log[str(iter)] = {\"num_problem\":{}, \"num_correct\":{},}\n",
    "    test_cases = []\n",
    "    candidates = []\n",
    "    problem_ids = []\n",
    "    sample_ids = []\n",
    "    log_ids_list = []\n",
    "    # Check if the outputs are correct\n",
    "    for results_dict in all_results2:\n",
    "        log_ids = results_dict[\"log_ids\"]\n",
    "        prompt = results_dict[\"prompt\"]\n",
    "        output = results_dict[\"output\"]\n",
    "        problem_id = int(log_ids[0])\n",
    "        sample_id = int(log_ids[-1])\n",
    "\n",
    "        is_correct = False\n",
    "        pattern = r'\\\\boxed{(\\d+)}'\n",
    "        matches = re.findall(pattern, output)\n",
    "        if matches == []:\n",
    "            final_answer = None\n",
    "        else:\n",
    "            final_answer = int(matches[0])\n",
    "            if correct_answers[problem_id] == final_answer:\n",
    "                is_correct = True\n",
    "\n",
    "        edit_log_dict = get_edit_log_dict(log_ids[:-1])\n",
    "\n",
    "        if \"prompts\" in edit_log_dict:\n",
    "            edit_log_dict[\"prompts\"][str(sample_id)] = prompt\n",
    "        else:\n",
    "            edit_log_dict[\"prompts\"] = {str(sample_id):prompt}\n",
    "\n",
    "        if \"outputs\" in edit_log_dict:\n",
    "            edit_log_dict[\"outputs\"][str(sample_id)] = output\n",
    "        else:\n",
    "            edit_log_dict[\"outputs\"] = {str(sample_id):output}\n",
    "\n",
    "        if \"corrects\" in edit_log_dict:\n",
    "            edit_log_dict[\"corrects\"][str(sample_id)] = is_correct\n",
    "        else:\n",
    "            edit_log_dict[\"corrects\"] = {str(sample_id):is_correct}\n",
    "    \n",
    "        if str(log_ids[0]) in advice_result_log[str(iter)][\"num_correct\"]:\n",
    "            if is_correct:\n",
    "                advice_result_log[str(iter)][\"num_correct\"][str(log_ids[0])] += 1\n",
    "        else:\n",
    "            if is_correct:\n",
    "                advice_result_log[str(iter)][\"num_correct\"][str(log_ids[0])] = 1\n",
    "            else:\n",
    "                advice_result_log[str(iter)][\"num_correct\"][str(log_ids[0])] = 0\n",
    "\n",
    "        if str(log_ids[0]) in advice_result_log[str(iter)][\"num_problem\"]:\n",
    "            advice_result_log[str(iter)][\"num_problem\"][str(log_ids[0])] += 1\n",
    "        else:\n",
    "            advice_result_log[str(iter)][\"num_problem\"][str(log_ids[0])] = 1\n",
    "\n",
    "\n",
    "    num_problem = 0\n",
    "    num_sample = 0\n",
    "    pass1_count = 0\n",
    "    passAll_count = 0\n",
    "    for problem_id_str in advice_result_log[str(iter)][\"num_problem\"]:\n",
    "        num_problem += 1\n",
    "        num_sample += advice_result_log[str(iter)][\"num_problem\"][problem_id_str]\n",
    "    for problem_id_str in advice_result_log[str(iter)][\"num_correct\"]:\n",
    "        pass1_count += advice_result_log[str(iter)][\"num_correct\"][problem_id_str]\n",
    "        if advice_result_log[str(iter)][\"num_correct\"][problem_id_str] > 0:\n",
    "            passAll_count += 1\n",
    "\n",
    "    num_already_correct_problem = total_num_problem - len(all_results1)\n",
    "\n",
    "    print(\"total_num_problem: \", total_num_problem)\n",
    "    print(\"total_num_sample: \", total_num_sample)\n",
    "    print(\"num_already_correct_problem: \", num_already_correct_problem)\n",
    "    print(f\"{passAll_count}/{num_problem} problems have got at least 1 correct sample in this iteration\")\n",
    "    print(f\"{pass1_count}/{num_sample} samples were correct in total\")\n",
    "    \n",
    "    pass1 = pass1_count/num_sample\n",
    "    passAll = passAll_count/num_problem\n",
    "\n",
    "    advice_result_log[str(iter)][\"pass@1\"] = pass1\n",
    "    advice_result_log[str(iter)][\"passAll\"] = passAll\n",
    "    print(\"pass1 in this iteration: \", pass1)\n",
    "    print(\"passAll in this iteration: \", passAll)\n",
    "    \n",
    "    \n",
    "    log[\"advice_result_log\"] = advice_result_log\n",
    "    with open(save_file, \"w\") as json_file:\n",
    "        json.dump(log, json_file)\n",
    "\n",
    "print()\n",
    "print(\"-- ALL FINISHED --\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b228f79b-d904-4c49-a933-c6f281e41759",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# Destruction Check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bf047d4-393d-4c24-be47-f15871b05776",
   "metadata": {},
   "outputs": [],
   "source": [
    "advice_path = \"advice6-1.json\"\n",
    "save_path = \"log6-1-3-fake-advice.json\"\n",
    "ids = [i for i in range(10)]\n",
    "fake_advice_ids = [i for i in range(10)]\n",
    "num_sample = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2d583c8-e1f8-4f14-b6fd-b398ae0707d3",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# with a piece of advice\n",
    "\n",
    "import json, re, os, random\n",
    "\n",
    "async def answer(ids, advices):\n",
    "\n",
    "    # answer with the advices\n",
    "    texts = []\n",
    "    for id in ids:\n",
    "        filtered_list = [num for num in fake_advice_ids if num != id]\n",
    "        fake_advice_id = random.choice(filtered_list)\n",
    "        advice = advices[str(fake_advice_id)]\n",
    "        problem = problems[id]\n",
    "        \n",
    "        messages = [\n",
    "            {\"role\": \"system\", \"content\": f\"\"\"Please answer to user's problem referring to the advice below. If the final answer is a number larger than 1000, take modulo 1000.\n",
    "\n",
    "Advice:\n",
    "{advice}\"\"\"},\n",
    "            {\"role\": \"user\", \"content\": f\"\"\"Problem: {problem}\"\"\"}\n",
    "        ]\n",
    "\n",
    "        text = tokenizer.apply_chat_template(\n",
    "            messages,\n",
    "            tokenize=False,\n",
    "            add_generation_prompt=True\n",
    "        )\n",
    "        texts.append(text)\n",
    "\n",
    "    start = time.time()\n",
    "\n",
    "    results = await asyncio.gather(\n",
    "        *[add_request(text, i, start, max_tokens = 10000) for i, text in enumerate(texts)]\n",
    "    )\n",
    "\n",
    "    import re, json\n",
    "    boxed_answers = {}\n",
    "    results_ = {}\n",
    "    \n",
    "    for i, result in enumerate(results):\n",
    "        results_[str(ids[i])] = result\n",
    "        \n",
    "        # Define the regex pattern to match the number enclosed in \\\\boxed{number}\n",
    "        pattern = r'\\\\boxed{(\\d+)}'\n",
    "        # Find all matches in the text\n",
    "        matches = re.findall(pattern, result)\n",
    "        # Print the extracted numbers\n",
    "        if matches == []:\n",
    "            boxed_answers[str(ids[i])] = None\n",
    "        else:\n",
    "            boxed_answers[str(ids[i])] = int(matches[0])\n",
    "\n",
    "    return results_, boxed_answers\n",
    "\n",
    "\n",
    "with open(advice_path) as json_file:\n",
    "    advices = json.load(json_file)\n",
    "\n",
    "all_boxed_answers = {}\n",
    "all_results = {}\n",
    "num_correct_answer = {}\n",
    "mv_score = {}\n",
    "num_problems = len(ids)\n",
    "\n",
    "for i in range(num_sample):\n",
    "    results, boxed_answers = await answer(ids, advices)\n",
    "    \n",
    "    for id in ids:\n",
    "\n",
    "        if str(id) in num_correct_answer:\n",
    "            score = num_correct_answer[str(id)]\n",
    "        else:\n",
    "            score = 0\n",
    "\n",
    "        if correct_answers[id] == boxed_answers[str(id)]:\n",
    "            score += 1\n",
    "        \n",
    "        num_correct_answer[str(id)] = score\n",
    "            \n",
    "        if str(id) in all_results:\n",
    "            all_results[str(id)].append(results[str(id)])\n",
    "            all_boxed_answers[str(id)].append(boxed_answers[str(id)])\n",
    "        else:\n",
    "            all_results[str(id)] = [results[str(id)]]\n",
    "            all_boxed_answers[str(id)] = [boxed_answers[str(id)]]\n",
    "    \n",
    "    log = {\"info\":\"Answer to the problems with fake advice to check the destruction. Temparature = 0.4\", \"all_results\":all_results, \"all_boxed_answers\":all_boxed_answers, \"num_correct_answer\":num_correct_answer, \"num_sample\":num_sample, \"num_problems\":num_problems}\n",
    "    with open(save_path, \"w\") as json_file:\n",
    "        json.dump(log, json_file)\n",
    "\n",
    "\n",
    "from collections import Counter\n",
    "\n",
    "mv_score = 0\n",
    "for id in ids:\n",
    "    filtered_numbers = [num for num in all_boxed_answers[str(id)] if num is not None]\n",
    "    if filtered_numbers == []: continue\n",
    "    counter = Counter(filtered_numbers)\n",
    "    most_common_element, count = counter.most_common(1)[0]\n",
    "    if correct_answers[id] == most_common_element:\n",
    "        mv_score += 1\n",
    "\n",
    "log[\"mv_score\"] = mv_score\n",
    "with open(save_path, \"w\") as json_file:\n",
    "    json.dump(log, json_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "187f1b00-715f-43fb-8dcf-3101d38aca95",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# Solve Problems with Manual Advice"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "807fffc9-1697-406a-9e8c-40eac5110499",
   "metadata": {},
   "outputs": [],
   "source": [
    "advices = {\n",
    "    \"0\":\"When you have to search all possibilities, try to find a regularity and make a generalized method to explore all the possibilities from it.\",\n",
    "    \"5\":\"When you have a geometry problem, you should check if there are enough conditions to solve the problem. If there are not, you should imagine what condition you should prove to solve the problem.\",\n",
    "    \"7\":\"When solving an integer problem with countless possibilities, find some regularities and prove a statement which narrows down the options\",\n",
    "    \"4\":\"When solving an integer problem with countless possibilities, find some regularities and prove a statement which narrows down the options\",\n",
    "    \"1\":\"\"\"In this problem, you may be able to use Chinese Reminder Theorem;\n",
    "\n",
    "The Chinese Remainder Theorem (CRT) is a fundamental result in number theory that provides a way to solve systems of simultaneous congruences with different moduli. Here's a summary of the theorem:\n",
    "\n",
    "1. **Statement of the Theorem**:\n",
    "   - Let \\( n_1, n_2, \\ldots, n_k \\) be positive integers that are pairwise coprime (i.e., \\( \\gcd(n_i, n_j) = 1 \\) for all \\( i \\neq j \\)).\n",
    "   - Let \\( a_1, a_2, \\ldots, a_k \\) be any integers.\n",
    "   - The Chinese Remainder Theorem states that there exists an integer \\( x \\) that satisfies the system of congruences:\n",
    "     \\[\n",
    "     \\begin{cases}\n",
    "     x \\equiv a_1 \\pmod{n_1} \\\\\n",
    "     x \\equiv a_2 \\pmod{n_2} \\\\\n",
    "     \\vdots \\\\\n",
    "     x \\equiv a_k \\pmod{n_k}\n",
    "     \\end{cases}\n",
    "     \\]\n",
    "   - Moreover, the solution \\( x \\) is unique modulo \\( N \\), where \\( N = n_1 n_2 \\cdots n_k \\).\n",
    "\n",
    "2. **Existence and Uniqueness**:\n",
    "   - The theorem guarantees the existence of a solution \\( x \\) that satisfies all the given congruences.\n",
    "   - The solution is unique modulo \\( N \\), meaning that if \\( x \\) and \\( y \\) are both solutions, then \\( x \\equiv y \\pmod{N} \\).\n",
    "\n",
    "3. **Constructing the Solution**:\n",
    "   - One method to find the solution \\( x \\) is to use the formula:\n",
    "     \\[\n",
    "     x = \\sum_{i=1}^{k} a_i N_i M_i \\pmod{N}\n",
    "     \\]\n",
    "     where \\( N_i = \\frac{N}{n_i} \\) and \\( M_i \\) is the modular inverse of \\( N_i \\) modulo \\( n_i \\) (i.e., \\( N_i M_i \\equiv 1 \\pmod{n_i} \\)).\n",
    "\n",
    "4. **Applications**:\n",
    "   - The Chinese Remainder Theorem has numerous applications in number theory, cryptography, and computer science.\n",
    "   - It is used in designing algorithms for parallel computations, in cryptographic protocols like secret sharing, and in solving Diophantine equations.\n",
    "\n",
    "In summary, the Chinese Remainder Theorem provides a powerful tool for solving systems of congruences with coprime moduli, ensuring the existence and uniqueness of solutions modulo the product of the moduli.\"\"\",\n",
    "    \"6\":\"If your answer became too long, summarize your answer and check if it follows the problem to prevent mistakes.\",\n",
    "}\n",
    "\n",
    "ids = []\n",
    "advice_model = \"./mistral8b_model\"\n",
    "solve_model = \"./qwq_awq_model\"\n",
    "\n",
    "num_advice = 1\n",
    "num_sample_for_improved = 3\n",
    "num_iteration = 1\n",
    "load_file = \"log9-5.json\"\n",
    "save_file = \"log9-5-2-advice.json\"\n",
    "max_request_advice = 1  # Mistral API\n",
    "max_request_improve = 1  # QwQ vLLM.AsyncEngine\n",
    "\n",
    "#with open(\"log9-5.json\") as f:\n",
    "#    log1 = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25788540-cbf0-49dc-8982-5fd6c4a7a1f4",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import os, json, re\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "with open(load_file) as json_file:\n",
    "    log = json.load(json_file)\n",
    "\n",
    "# advice_result_log: {str(iteration):{str(problem_id):{\"num_problem\":, \"num_correct\":}}\n",
    "advice_result_log = {}\n",
    "\n",
    "\n",
    "qwq_tokenizer = AutoTokenizer.from_pretrained(\"./qwq_awq_model\", padding_side='left')\n",
    "#qwq_tokenizer = AutoTokenizer.from_pretrained(\"MBMMurad/QwQ-32B-preview-AWQ-AIMO-earlysharing\", padding_side='left')\n",
    "\n",
    "def add_numbers_to_lines(text):\n",
    "    # Split the text into lines\n",
    "    lines = text.split('\\n\\n')\n",
    "\n",
    "    # Initialize a counter\n",
    "    counter = 1\n",
    "\n",
    "    # Create a list to hold the numbered lines\n",
    "    numbered_lines = []\n",
    "    numbered_texts = []\n",
    "\n",
    "    # Iterate through the lines\n",
    "    for line in lines:\n",
    "        if line.strip():  # Check if the line is not empty\n",
    "            # Add the number and the line to the list\n",
    "            numbered_lines.append((counter, line))\n",
    "            numbered_texts.append(f\"{counter}. {line}\")\n",
    "            # Increment the counter\n",
    "            counter += 1\n",
    "\n",
    "    numbered_text = '\\n\\n'.join(numbered_texts)    \n",
    "\n",
    "    return numbered_lines, numbered_text\n",
    "\n",
    "\n",
    "def get_text_before_number(numbered_lines, number):\n",
    "    # Find the index of the tuple with the given number\n",
    "    for i, (num, line) in enumerate(numbered_lines):\n",
    "        if num == number:\n",
    "            # Return the original text before the given number\n",
    "            return '\\n\\n'.join(line for _, line in numbered_lines[:i])\n",
    "\n",
    "    # If the number is not found, return an empty string\n",
    "    return \"\"\n",
    "\n",
    "\n",
    "def extract_text_inside_backticks(text, arbitrary_text):\n",
    "    # Define the pattern to match the text inside ``` that follows the arbitrary text\n",
    "    pattern = re.compile(r'```{}\\s*([\\s\\S]*?)\\s*```'.format(re.escape(arbitrary_text)))\n",
    "\n",
    "    # Search for the pattern in the text\n",
    "    match = pattern.search(text)\n",
    "\n",
    "    if match:\n",
    "        return match.group(1).strip()\n",
    "    else:\n",
    "        return None\n",
    "\n",
    "\n",
    "# Define get_result (Optional): If you want to evaluate the output somehow while running process_requests, you can define get_result function and pass it to process_requests method. This will let you read log files easily with some evaluation.\n",
    "def get_result(request_dict, save_dir):\n",
    "    # request_dict: {\"prompt\":, \"output\":}\n",
    "    # save_dir: this is a directory path for progress log. put your evaluation file in here\n",
    "\n",
    "    problem_id = int(request_dict[\"log_ids\"][0])\n",
    "    output = request_dict[\"output\"]\n",
    "    iter = request_dict[\"iter\"]\n",
    "    eval_file_path = f\"{save_dir}/score-{iter}.json\"\n",
    "\n",
    "    if os.path.exists(eval_file_path):\n",
    "        with open(eval_file_path) as f:\n",
    "            log = json.load(f)\n",
    "        if str(problem_id) in log[\"num_answered\"]:\n",
    "            num_answered = log[\"num_answered\"][str(problem_id)]\n",
    "            num_correct = log[\"num_correct\"][str(problem_id)]\n",
    "        else:\n",
    "            num_answered = 0\n",
    "            num_correct = 0\n",
    "    else:\n",
    "        log = {\"num_answered\":{}, \"num_correct\":{}}\n",
    "        num_answered = 0\n",
    "        num_correct = 0\n",
    "\n",
    "    is_correct = False\n",
    "    pattern = r'\\\\boxed{(\\d+)}'\n",
    "    matches = re.findall(pattern, output)\n",
    "    if matches == []:\n",
    "        final_answer = None\n",
    "    else:\n",
    "        final_answer = int(matches[0])\n",
    "        if correct_answers[problem_id] == final_answer:\n",
    "            is_correct = True\n",
    "            num_correct += 1\n",
    "\n",
    "    request_dict[\"final_answer\"] = final_answer\n",
    "    request_dict[\"is_correct\"] = is_correct\n",
    "\n",
    "    num_answered += 1\n",
    "    log[\"num_answered\"][str(problem_id)] = num_answered\n",
    "    log[\"num_correct\"][str(problem_id)] = num_correct\n",
    "\n",
    "    with open(eval_file_path, \"w\") as f:\n",
    "        json.dump(log, f)\n",
    "\n",
    "    return request_dict\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def get_log_dict(log, log_ids):\n",
    "    if len(log_ids) == 0:\n",
    "        return log\n",
    "    \n",
    "    problem_id = log_ids.pop(0)\n",
    "    log_dict = log[str(problem_id)]\n",
    "    \n",
    "    for log_id in log_ids:\n",
    "        log_dict = log_dict[\"children\"][str(log_id)]\n",
    "\n",
    "    return log_dict\n",
    "\n",
    "\n",
    "def get_edit_log_dict(log_ids):\n",
    "    global log\n",
    "    \n",
    "    if len(log_ids) == 0:\n",
    "        return log\n",
    "    \n",
    "    problem_id = log_ids.pop(0)\n",
    "    log_dict = log[str(problem_id)]\n",
    "    \n",
    "    for log_id in log_ids:\n",
    "        log_dict = log_dict[\"children\"][str(log_id)]\n",
    "\n",
    "    return log_dict\n",
    "\n",
    "\n",
    "def set_log_element(log, log_ids, key, element):\n",
    "    if len(log_ids) == 0:\n",
    "        raise Exception(\"log_ids should have at least one element.\")\n",
    "        \n",
    "    problem_id = log_ids.pop(0)\n",
    "    log_dict = log[problem_id]\n",
    "\n",
    "    for log_id in log_ids:\n",
    "        if not \"children\" in log_dict:\n",
    "            log_dict[\"children\"] = {}\n",
    "        log_dict = log_dict[\"children\"]\n",
    "\n",
    "        if not log_id in log_dict:\n",
    "            log_dict[log_id] = {}\n",
    "        log_dict = log_dict[log_id]\n",
    "\n",
    "    log_dict[key] = element\n",
    "\n",
    "    return log\n",
    "\n",
    "\n",
    "\n",
    "for iter in range(num_iteration):\n",
    "\n",
    "    # Prepare for the \n",
    "    all_requests = {str(i):[] for i in range(max_request_advice)}\n",
    "    request_id = 0\n",
    "    # Make all_request for advice by searching log recursively\n",
    "    def search_log(log_dict, log_ids):  # node_id: str\n",
    "        global all_requests, request_id\n",
    "        node_id = log_ids[-1]\n",
    "\n",
    "        if type(log_dict[node_id]) != dict:\n",
    "            return None\n",
    "            \n",
    "        if \"children\" in log_dict[node_id]:\n",
    "            for next_node_id in log_dict[node_id][\"children\"]:\n",
    "                search_log(log_dict[node_id][\"children\"], log_ids + [next_node_id])\n",
    "        elif \"corrects\" in log_dict[node_id]:\n",
    "            for sample_id_str in log_dict[node_id][\"corrects\"]:\n",
    "                if not log_dict[node_id][\"corrects\"][sample_id_str]:\n",
    "    \n",
    "                    problem_id_str = log_ids[0]\n",
    "                    problem_id = int(problem_id_str)\n",
    "                    prompt = log_dict[node_id][\"prompts\"][sample_id_str]\n",
    "                    output = log_dict[node_id][\"outputs\"][sample_id_str]\n",
    "                    student_answer = prompt.split(\"<|im_start|>assistant\")[1] + output  #log_dict[node_id][\"outputs\"][sample_id_str]\n",
    "                    numbered_lines, numbered_answer = add_numbers_to_lines(student_answer)\n",
    "\n",
    "                    advice = advices[problem_id_str]\n",
    "                    \n",
    "                    prompt = f\"\"\"<s>[INST]### Problem:\n",
    "'''\n",
    "{problems[problem_id]}\n",
    "'''\n",
    "\n",
    "\n",
    "### Correct Solution:\n",
    "'''\n",
    "{solutions[problem_id]}\n",
    "'''\n",
    "\n",
    "\n",
    "### Incorrect Student’s Answer:\n",
    "'''\n",
    "{numbered_answer}\n",
    "'''\n",
    "\n",
    "\n",
    "### Advice:\n",
    "'''\n",
    "{advice}\n",
    "'''\n",
    "\n",
    "\n",
    "You are an advanced language model tasked with analyzing a student’s answer to a mathematical problems and insert an advice in the student’s answer to lead him to the correct answer. You are given the math problem, the correct solution of it, a student’s incorrect answer and a piece of advice for it. Please think where to think about the advice in student’s answer and return the id of sentence where the advice should be thought before following the instructions below.\n",
    "\n",
    "\n",
    "### Instructions:\n",
    "1. **Analyze the Problem, Student’s Answer, Correct Solution and Advice**: Carefully understand and analyze them. \n",
    "2. **Summarize Answer and Solution**: Based on your analysis, summarize student’s answer and correct solution respectively.\n",
    "3. **Think How Advice Leads to the Correct Solution**: Think about how the given advice contributes him to get a correct solution and how he rewrites the answer considering the advice.\n",
    "4. **Think Where the Advice should be Considered**: Based on the advice you created and analysis so far, think where in the student’s answer the advice should be considered. There are ids at the beginning of all sentences in student’s answer. Please provide the id of line in the answer which should be modified by considering the advice.\n",
    "5. **Generate Output**: Based on your analysis so far, return the id in backticks like\n",
    "\n",
    "```id\n",
    "(The id of line where the advice should be considered.)\n",
    "```\n",
    "\n",
    "\n",
    "Let’s think step by step following each step of the instructions.[/INST]\"\"\"\n",
    "                    \n",
    "                    new_log_ids = log_ids+[sample_id_str]\n",
    "                    request_dict = {\"iter\":iter, \"log_ids\":new_log_ids, \"prompt\":prompt, \"numbered_lines\":numbered_lines, \"advice\":advice}\n",
    "                    all_requests[str(request_id)].append(request_dict)\n",
    "                    request_id+=1\n",
    "                    if request_id == max_request_advice:\n",
    "                        request_id = 0\n",
    "    \n",
    "    for problem_id_str in log:\n",
    "        if problem_id_str in advices:\n",
    "            search_log(log, [problem_id_str])\n",
    "    \n",
    "    \n",
    "    # Process all_requests\n",
    "    # If you had trouble in last process and want to continue to get the output, set restart = True\n",
    "    \n",
    "    #all_results = await asyncio.gather(\n",
    "    #    *[process_api_requests(all_requests[request_id_str], int(request_id_str), max_tokens = 3000, restart = True) for request_id_str in all_requests]\n",
    "    #)\n",
    "    \n",
    "    # *Note this part should be modified so that batch inference can be run\n",
    "\n",
    "    # For using llm.generate\n",
    "    results1 = process_requests(all_requests[str(0)], int(iter), model=advice_model, max_tokens = 3000, restart = True, save_dir=\"progless_log1\")\n",
    "    \n",
    "    # Update log and create next requests (all_requests2)\n",
    "    all_requests2 = {str(i):[] for i in range(max_request_improve)}\n",
    "    request_id2 = 0\n",
    "    \n",
    "    for result_dict in results1:\n",
    "        print(\"log_ids: \", result_dict[\"log_ids\"])\n",
    "        log_ids = result_dict[\"log_ids\"]\n",
    "        prompt = result_dict[\"prompt\"]\n",
    "        output = result_dict[\"output\"]\n",
    "        numbered_lines = result_dict[\"numbered_lines\"]\n",
    "        advice = result_dict[\"advice\"]\n",
    "        \n",
    "        #advice = extract_text_inside_backticks(output, \"advice\")\n",
    "        insert_id_text = extract_text_inside_backticks(output, \"id\")\n",
    "\n",
    "        if insert_id_text:\n",
    "            try:\n",
    "                insert_id_text_ = re.sub(r'[^0-9]', '', insert_id_text)\n",
    "                insert_id = int(insert_id_text_)\n",
    "                new_answer_base = get_text_before_number(numbered_lines, insert_id) + \"\\n\\nNow let's follow the user's advice.\"\n",
    "    \n",
    "                problem_id = int(log_ids[0])\n",
    "                problem = problems[problem_id]\n",
    "                messages = [\n",
    "                    {\"role\": \"system\", \"content\": f\"\"\"Please answer to user's problem following the advice below. If the final answer is a number larger than 1000, take modulo 1000.\"\"\"},\n",
    "                    {\"role\": \"user\", \"content\": f\"\"\"Problem: {problem}\n",
    "\n",
    "Advice: {advice}\"\"\"}\n",
    "                ]\n",
    "                \n",
    "                new_prompt = qwq_tokenizer.apply_chat_template(\n",
    "                    messages,\n",
    "                    tokenize=False,\n",
    "                    add_generation_prompt=True\n",
    "                )\n",
    "    \n",
    "                new_prompt += new_answer_base\n",
    "                prompts_dict = {str(i):new_prompt for i in range(num_sample_for_improved)}\n",
    "                edit_log_dict = get_edit_log_dict(log_ids[:-1])\n",
    "                if \"children\" in edit_log_dict:\n",
    "                    edit_log_dict[\"children\"][str(log_ids[-1])] = {\"advice\":advice, \"insert_id\":insert_id, \"prompts\":prompts_dict}\n",
    "                else:\n",
    "                    edit_log_dict[\"children\"] = {str(log_ids[-1]):{\"advice\":advice, \"insert_id\":insert_id, \"prompts\":prompts_dict}}\n",
    "                #log = set_log_element(log, log_ids[:-1], log_ids[-1], {\"advice\":advice, \"insert_id\":insert_id, \"prompts\":prompts_dict})\n",
    "    \n",
    "                for i in range(num_sample_for_improved):\n",
    "                    next_request = {\"log_ids\":log_ids+[str(i)], \"prompt\":new_prompt, \"iter\":iter}\n",
    "                    all_requests2[str(request_id2)].append(next_request)\n",
    "                    \n",
    "                request_id2 += 1\n",
    "                if request_id2 == max_request_improve:\n",
    "                    request_id2 = 0\n",
    "            except:\n",
    "                pass\n",
    "\n",
    "    \n",
    "    with open(save_file, \"w\") as f:\n",
    "        json.dump(log, f)\n",
    "\n",
    "    print(\"log saved\")\n",
    "    \n",
    "    \n",
    "    # Process all_requests\n",
    "    # If you had trouble in last process and want to continue to get the output, set restart = True\n",
    "    # all_results2 = [[{\"output\":}, ... ], ...]\n",
    "    #all_results2 = await asyncio.gather(\n",
    "    #    *[process_requests(all_requests2[request_id_str], int(request_id_str), max_tokens = 10000, restart = False, get_result = get_result) for request_id_str in all_requests2]\n",
    "    #)\n",
    "\n",
    "    results2 = process_requests(all_requests2[str(0)], int(iter), model=solve_model, max_tokens = 10000, restart = True, get_result = get_result, save_dir=\"progless_log2\")\n",
    "    \n",
    "    advice_result_log[str(iter)] = {}\n",
    "    # Check if the outputs are correct\n",
    "    for results_dict in results2:\n",
    "        log_ids = results_dict[\"log_ids\"]\n",
    "        prompt = results_dict[\"prompt\"]\n",
    "        output = results_dict[\"output\"]\n",
    "\n",
    "        problem_id = int(log_ids[0])\n",
    "\n",
    "        is_correct = False\n",
    "        pattern = r'\\\\boxed{(\\d+)}'\n",
    "        matches = re.findall(pattern, output)\n",
    "        if matches == []:\n",
    "            final_answer = None\n",
    "        else:\n",
    "            final_answer = int(matches[0])\n",
    "            if correct_answers[problem_id] == final_answer:\n",
    "                is_correct = True\n",
    "\n",
    "        edit_log_dict = get_edit_log_dict(log_ids[:-1])\n",
    "        sample_id = log_ids[-1]\n",
    "\n",
    "        if \"outputs\" in edit_log_dict:\n",
    "            edit_log_dict[\"outputs\"][str(sample_id)] = output\n",
    "        else:\n",
    "            edit_log_dict[\"outputs\"] = {str(sample_id):output}\n",
    "\n",
    "        if \"final_answers\" in edit_log_dict:\n",
    "            edit_log_dict[\"final_answers\"][str(sample_id)] = final_answer\n",
    "        else:\n",
    "            edit_log_dict[\"final_answers\"] = {str(sample_id):final_answer}\n",
    "\n",
    "        if \"corrects\" in edit_log_dict:\n",
    "            edit_log_dict[\"corrects\"][str(sample_id)] = is_correct\n",
    "        else:\n",
    "            edit_log_dict[\"corrects\"] = {str(sample_id):is_correct}\n",
    "\n",
    "        if str(log_ids[0]) in advice_result_log[str(iter)]:\n",
    "            advice_result_log[str(iter)][str(log_ids[0])][\"num_problem\"] += 1\n",
    "            if is_correct: advice_result_log[str(iter)][str(log_ids[0])][\"num_correct\"] += 1\n",
    "        else:\n",
    "            if is_correct:\n",
    "                advice_result_log[str(iter)][str(log_ids[0])] = {\"num_problem\":1, \"num_correct\":1}\n",
    "            else:\n",
    "                advice_result_log[str(iter)][str(log_ids[0])] = {\"num_problem\":1, \"num_correct\":0}\n",
    "\n",
    "\n",
    "    log[\"advice_result_log\"] = advice_result_log\n",
    "    with open(save_file, \"w\") as json_file:\n",
    "        json.dump(log, json_file)\n",
    "\n",
    "print()\n",
    "print(\"-- ALL FINISHED --\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fe438a5-6e69-4f67-ba6f-5af6c102ff61",
   "metadata": {},
   "outputs": [],
   "source": [
    "requests = []  # requests: [{\"prompt\":, ...} ... ]\n",
    "\n",
    "qwq_tokenizer = AutoTokenizer.from_pretrained(\"./qwq_awq_model\", padding_side='left')\n",
    "\n",
    "def extract_text_inside_backticks(text, arbitrary_text):\n",
    "    # Define the pattern to match the text inside ``` that follows the arbitrary text\n",
    "    pattern = re.compile(r'```{}\\s*([\\s\\S]*?)\\s*```'.format(re.escape(arbitrary_text)))\n",
    "\n",
    "    # Search for the pattern in the text\n",
    "    match = pattern.search(text)\n",
    "\n",
    "    if match:\n",
    "        return match.group(1).strip()\n",
    "    else:\n",
    "        return None\n",
    "\n",
    "\n",
    "def add_numbers_to_lines(text):\n",
    "    # Split the text into lines\n",
    "    lines = text.split('\\n\\n')\n",
    "\n",
    "    # Initialize a counter\n",
    "    counter = 1\n",
    "\n",
    "    # Create a list to hold the numbered lines\n",
    "    numbered_lines = []\n",
    "    numbered_texts = []\n",
    "\n",
    "    # Iterate through the lines\n",
    "    for line in lines:\n",
    "        if line.strip():  # Check if the line is not empty\n",
    "            # Add the number and the line to the list\n",
    "            numbered_lines.append((counter, line))\n",
    "            numbered_texts.append(f\"{counter}. {line}\")\n",
    "            # Increment the counter\n",
    "            counter += 1\n",
    "\n",
    "    numbered_text = '\\n\\n'.join(numbered_texts)    \n",
    "\n",
    "    return numbered_lines, numbered_text\n",
    "\n",
    "\n",
    "def get_text_before_number(numbered_lines, number):\n",
    "    # Find the index of the tuple with the given number\n",
    "    for i, (num, line) in enumerate(numbered_lines):\n",
    "        if num == number:\n",
    "            # Return the original text before the given number\n",
    "            return '\\n\\n'.join(line for _, line in numbered_lines[:i])\n",
    "\n",
    "    # If the number is not found, return an empty string\n",
    "    return \"\"\n",
    "\n",
    "\n",
    "\n",
    "for id in ids:\n",
    "    advice = advices[str(id)]\n",
    "\n",
    "    for i in range(len(log1[str(id)][\"outputs\"])):\n",
    "        numbered_lines, numbered_text = add_numbers_to_lines(log1[str(id)][\"outputs\"][str(i)])\n",
    "    \n",
    "        prompt = f\"\"\"<s>[INST]### Problem:\n",
    "'''\n",
    "{problems[id]}\n",
    "'''\n",
    "\n",
    "\n",
    "### Correct Solution:\n",
    "'''\n",
    "{solutions[id]}\n",
    "'''\n",
    "\n",
    "\n",
    "### Incorrect Student’s Answer:\n",
    "'''\n",
    "{numbered_text}\n",
    "'''\n",
    "\n",
    "\n",
    "### Advice:\n",
    "'''\n",
    "{advice}\n",
    "'''\n",
    "\n",
    "\n",
    "You are an advanced language model tasked with analyzing a student’s answer to a mathematical problems and insert an advice in the student’s answer to lead him to the correct answer. You are given the math problem, the correct solution of it, a student’s incorrect answer and a piece of advice for it. Please think where to think about the advice in student’s answer and return the id of sentence where the advice should be thought before following the instructions below.\n",
    "\n",
    "\n",
    "### Instructions:\n",
    "1. **Analyze the Problem, Student’s Answer, Correct Solution and Advice**: Carefully understand and analyze them. \n",
    "2. **Summarize Answer and Solution**: Based on your analysis, summarize student’s answer and correct solution respectively.\n",
    "3. **Think How Advice Leads to the Correct Solution**: Think about how the given advice contributes him to get a correct solution and how he rewrites the answer considering the advice.\n",
    "4. **Think Where the Advice should be Considered**: Based on the advice you created and analysis so far, think where in the student’s answer the advice should be considered. There are ids at the beginning of all sentences in student’s answer. Please provide the id of line in the answer which should be modified by considering the advice.\n",
    "5. **Generate Output**: Based on your analysis so far, return the id in backticks like\n",
    "\n",
    "```id\n",
    "(The id of line where the advice should be considered.)\n",
    "```\n",
    "\n",
    "\n",
    "Let’s think step by step following each step of the instructions.[/INST]\"\"\"\n",
    "\n",
    "        requests.append({\"prompt\":prompt, \"id\":id, \"numbered_lines\":numbered_lines})\n",
    "\n",
    "results = await process_api_requests(requests, 0, max_tokens = 3000, temperature=0.4, save_dir = \"api_progress_log\", restart = False, get_result = None, delete_save_file = False)\n",
    "\n",
    "all_requests2 = []\n",
    "for result in results:\n",
    "    id = result[\"id\"]\n",
    "    output = result[\"output\"]\n",
    "    numbered_lines = result[\"numbered_lines\"]\n",
    "\n",
    "    id_text = extract_text_inside_backticks(output, \"id\")\n",
    "    \n",
    "    if id_text:\n",
    "        try:\n",
    "            insert_id_text_ = re.sub(r'[^0-9]', '', id_text)\n",
    "            insert_id = int(insert_id_text_)\n",
    "            new_answer_base = get_text_before_number(numbered_lines, insert_id) + \"\\n\\nNow let's follow the user's advice.\"\n",
    "\n",
    "            problem_id = int(log_ids[0])\n",
    "            problem = problems[problem_id]\n",
    "            messages = [\n",
    "                {\"role\": \"system\", \"content\": f\"\"\"Please answer to user's problem following the advice below. If the final answer is a number larger than 1000, take modulo 1000.\"\"\"},\n",
    "                {\"role\": \"user\", \"content\": f\"\"\"Problem: {problem}\n",
    "\n",
    "Advice: {advice}\"\"\"}\n",
    "            ]\n",
    "            \n",
    "            new_prompt = qwq_tokenizer.apply_chat_template(\n",
    "                messages,\n",
    "                tokenize=False,\n",
    "                add_generation_prompt=True\n",
    "            )\n",
    "\n",
    "            new_prompt += new_answer_base\n",
    "\n",
    "            for i in range(num_sample_for_improved):\n",
    "                next_request = {\"prompt\":new_prompt, \"insert_id\":insert_id, \"id\":id}\n",
    "                all_requests2[str(0)].append(next_request)\n",
    "\n",
    "        except:\n",
    "            pass\n",
    "\n",
    "results2 = process_requests(all_requests2[str(0)], int(iter), model=solve_model, max_tokens = 10000, restart = True, get_result = get_result, save_dir=\"progless_log2\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4460601c-131c-4e2a-9286-f1f5158ad0d2",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# Advice Prompt Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02e7af52-0835-4edf-8f73-5e8ccd427d52",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#mistral_model = \"./mistral8b_model\"\n",
    "#solve_model = \"./mistral8b_model\"\n",
    "\n",
    "num_advice = 1\n",
    "num_sample_for_improved = 1\n",
    "num_iteration = 10\n",
    "load_file = \"code-log5.json\"\n",
    "save_file = \"code-log5-advice1.json\"\n",
    "save_dir_base = \"progress_log_test\"\n",
    "max_request_advice = 15  # max number of requests to generate advice by AsyncEngine\n",
    "max_request_improve = 15  # max number of requests to generate improved answer by AsyncEngine\n",
    "\n",
    "\n",
    "import os, json, re\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "with open(load_file) as json_file:\n",
    "    log = json.load(json_file)\n",
    "\n",
    "total_num_problem = log[\"num_problems\"] * log[\"num_sample\"]\n",
    "\n",
    "# advice_result_log: {str(iteration):{str(problem_id):{\"num_problem\":, \"num_correct\":}}}\n",
    "advice_result_log = {}\n",
    "\n",
    "def extract_text_inside_backticks(text, arbitrary_text):\n",
    "    # Define the pattern to match the text inside ``` that follows the arbitrary text\n",
    "    pattern = re.compile(r'```{}\\s*([\\s\\S]*?)\\s*```'.format(re.escape(arbitrary_text)))\n",
    "\n",
    "    # Search for the pattern in the text\n",
    "    match = pattern.search(text)\n",
    "\n",
    "    if match:\n",
    "        return match.group(1).strip()\n",
    "    else:\n",
    "        return None\n",
    "\n",
    "\n",
    "def get_log_dict(log, log_ids):\n",
    "    if len(log_ids) == 0:\n",
    "        return log\n",
    "    \n",
    "    problem_id = log_ids.pop(0)\n",
    "    log_dict = log[str(problem_id)]\n",
    "    \n",
    "    for log_id in log_ids:\n",
    "        log_dict = log_dict[\"children\"][str(log_id)]\n",
    "\n",
    "    return log_dict\n",
    "\n",
    "def get_edit_log_dict(log_ids):\n",
    "    global log\n",
    "    \n",
    "    if len(log_ids) == 0:\n",
    "        return log\n",
    "    \n",
    "    problem_id = log_ids.pop(0)\n",
    "    log_dict = log[str(problem_id)]\n",
    "    \n",
    "    for log_id in log_ids:\n",
    "        log_dict = log_dict[\"children\"][str(log_id)]\n",
    "\n",
    "    return log_dict\n",
    "\n",
    "\n",
    "for iter in range(num_iteration):\n",
    "\n",
    "    # Prepare for the \n",
    "    #all_requests = {str(i):[] for i in range(max_request_advice)}\n",
    "    #request_id = 0\n",
    "    all_requests1 = AllRequests(max_request_advice)\n",
    "    # Make all_request for advice by searching log recursively\n",
    "    def search_log(log_dict, log_ids):  # node_id: str\n",
    "        global all_requests, request_id\n",
    "        node_id = log_ids[-1]\n",
    "\n",
    "        if type(log_dict[node_id]) != dict:\n",
    "            return None\n",
    "            \n",
    "        if \"children\" in log_dict[node_id]:\n",
    "            for next_node_id in log_dict[node_id][\"children\"]:\n",
    "                search_log(log_dict[node_id][\"children\"], log_ids + [next_node_id])\n",
    "        elif \"corrects\" in log_dict[node_id]:\n",
    "            for sample_id_str in log_dict[node_id][\"corrects\"]:\n",
    "                if not log_dict[node_id][\"corrects\"][sample_id_str]:\n",
    "    \n",
    "                    problem_id_str = log_ids[0]\n",
    "                    problem_id = int(problem_id_str)\n",
    "                    prompt = log_dict[node_id][\"prompts\"][sample_id_str]\n",
    "                    output = log_dict[node_id][\"outputs\"][sample_id_str]\n",
    "                    error = log_dict[node_id][\"errors\"][sample_id_str]\n",
    "                    traceback_ = log_dict[node_id][\"tracebacks\"][sample_id_str]\n",
    "                    #student_answer = prompt.split(\"<|im_start|>assistant\")[1] + output  #log_dict[node_id][\"outputs\"][sample_id_str]\n",
    "                    #numbered_lines, numbered_answer = add_numbers_to_lines(student_answer)\n",
    "\n",
    "                    messages = [\n",
    "                        {\"role\": \"user\", \"content\": f\"\"\"### Problem:\n",
    "'''\n",
    "{instruct_prompt[problem_id]}\n",
    "'''\n",
    "\n",
    "\n",
    "### Correct Solution:\n",
    "'''\n",
    "{canonical_solution[problem_id]}\n",
    "'''\n",
    "\n",
    "\n",
    "### Student's Incorrect Answer:\n",
    "'''\n",
    "{output}\n",
    "'''\n",
    "\n",
    "\n",
    "### Test Code and Its Error\n",
    "'''\n",
    "```\n",
    "{test[problem_id]}\n",
    "```\n",
    "\n",
    "{error}\n",
    "'''\n",
    "\n",
    "\n",
    "You are an advanced language model tasked with analyzing a student’s answer to a coding problems and make some instructions to lead him to the correct solution. You are given the coding problem, the correct solution of it, a student’s incorrect answer, test code of the answer code and error cause of the student's incorrect answer. Please make some instructions and let him answer correctly following the instructions below.\n",
    "\n",
    "\n",
    "### Instructions:\n",
    "1. **Think Why Student’s Answer was Wrong**: Compare the correct solution and student’s incorrect answer, and analyze why the student’s answer was wrong and think about where it went in a different direction from the correct solution.\n",
    "2. **Think What was the Idea Missing in Student’s Answer**: Think what idea was included in the correct solution but was missing from the students' answer.\n",
    "3. **Think From Where the Student’s Answer Should be Modified**: Based on the missing idea, think from where the student’s answer should be modified. There are IDs at the beginning of all sentences in the student’s answer. Please provide the ID of line in the answer from which the answer should be rewritten.\n",
    "4. **Imagine Many Thinking Processes Which May Lead to the Idea**: Imagine as many thinking processes as possible which may lead him to think of the missing idea.\n",
    "5. **Give Short and Abstract Instructions**: Expanding your imagination, make as many instructions as possible which may lead him to the missing idea which was not included in the student’s answer. All the instructions should be abstract and general so that it can be applied to other problems too. These are the examples of the instruction; “Explore all the possibilities of it”, “Check if there are enough conditions to solve the problem”, “Imagine what condition will lead you to solve the problem”, “Find some regularities and prove a statement which narrows down the options”, “Summarize your thought and check if it really follows the problem”.\n",
    "6. **Generate Output**: Based on the result so far, return the missing idea, ID, and pieces of advice in backticks like\n",
    "\n",
    "```idea\n",
    "(The missing idea in the student’s answer.)\n",
    "```\n",
    "\n",
    "```ID\n",
    "(The ID of line from where the answer should be rewritten.)\n",
    "```\n",
    "\n",
    "```instructions\n",
    "[\n",
    "    \"(Instruction 1)\",\n",
    "    ...\n",
    "]\n",
    "```\n",
    "\n",
    "\n",
    "Let’s think step by step following each step of the instructions.\"\"\"}\n",
    "                    ]\n",
    "                    \n",
    "                    prompt = tokenizer.apply_chat_template(\n",
    "                        messages,\n",
    "                        tokenize=False,\n",
    "                        add_generation_prompt=True\n",
    "                    )\n",
    "                    \n",
    "                    new_log_ids = log_ids+[sample_id_str]\n",
    "                    request_dict = {\"log_ids\":new_log_ids, \"prompt\":prompt}\n",
    "                    all_requests1.add(request_dict)\n",
    "                    \n",
    "    \n",
    "    for problem_id_str in log:\n",
    "        search_log(log, [problem_id_str])\n",
    "    \n",
    "    \n",
    "    all_results1 = await all_requests1.process(max_tokens=3000, restart = True, save_dir=f\"{save_dir_base}/advice-iter{iter}\", delete_save_file = True, test_num_request = 1)\n",
    "    all_requests2 = AllRequests(max_request_improve)\n",
    "\n",
    "\n",
    "    for result_dict in all_results1:\n",
    "        log_ids = result_dict[\"log_ids\"]\n",
    "        prompt = result_dict[\"prompt\"]\n",
    "        output = result_dict[\"output\"]\n",
    "        \n",
    "        instructions = extract_text_inside_backticks(output, \"instructions\")\n",
    "\n",
    "\n",
    "        if instructions:\n",
    "            problem_id = int(log_ids[0])\n",
    "            problem = instruct_prompt[problem_id]\n",
    "\n",
    "            print()\n",
    "            print()\n",
    "            print(f\"Problem: {problem}\")\n",
    "            print()\n",
    "            print(f\"Advice: {instructions}\")\n",
    "\n",
    "        else:\n",
    "            print()\n",
    "            print()\n",
    "            print(f\"Error\")\n",
    "            print()\n",
    "            print(f\"Output: {output}\")\n",
    "            \n",
    "'''\n",
    "\n",
    "            messages = [\n",
    "                {\"role\": \"user\", \"content\": f\"\"\"Please answer to user's problem following the advice below.\n",
    "\n",
    "Problem:\n",
    "{problem}\n",
    "\n",
    "Advice: {advice}\"\"\"}\n",
    "            ]\n",
    "            \n",
    "            new_prompt = tokenizer.apply_chat_template(\n",
    "                messages,\n",
    "                tokenize=False,\n",
    "                add_generation_prompt=True\n",
    "            )\n",
    "\n",
    "            prompts_dict = {str(i):new_prompt for i in range(num_sample_for_improved)}\n",
    "            edit_log_dict = get_edit_log_dict(log_ids[:-1])\n",
    "            if \"children\" in edit_log_dict:\n",
    "                edit_log_dict[\"children\"][str(log_ids[-1])] = {\"advice\":advice, \"prompts\":prompts_dict}\n",
    "            else:\n",
    "                edit_log_dict[\"children\"] = {str(log_ids[-1]):{\"advice\":advice, \"prompts\":prompts_dict}}\n",
    "            #log = set_log_element(log, log_ids[:-1], log_ids[-1], {\"advice\":advice, \"insert_id\":insert_id, \"prompts\":prompts_dict})\n",
    "\n",
    "            for i in range(num_sample_for_improved):\n",
    "                next_request = {\"log_ids\":log_ids+[str(i)], \"prompt\":new_prompt}\n",
    "                all_requests2.add(next_request)\n",
    "    \n",
    "    with open(save_file, \"w\") as f:\n",
    "        json.dump(log, f)\n",
    "\n",
    "    print(\"log saved\")\n",
    "    \n",
    "    \n",
    "    # Process all_requests\n",
    "    # If you had trouble in last process and want to continue to get the output, set restart = True\n",
    "\n",
    "    #results2 = process_requests(all_requests2[str(0)], int(iter), model=solve_model, max_tokens = 10000, restart = True, get_result = get_result, save_dir=\"progless_log2\")\n",
    "    all_results2 = await all_requests2.process(max_tokens=3000, restart = True, save_dir=f\"{save_dir_base}/answer-iter{iter}\")\n",
    "    \n",
    "    advice_result_log[str(iter)] = {\"num_problem\":{}, \"num_correct\":{},}\n",
    "    test_cases = []\n",
    "    candidates = []\n",
    "    problem_ids = []\n",
    "    sample_ids = []\n",
    "    log_ids_list = []\n",
    "    # Check if the outputs are correct\n",
    "    for results_dict in all_results2:\n",
    "        log_ids = results_dict[\"log_ids\"]\n",
    "        prompt = results_dict[\"prompt\"]\n",
    "        output = results_dict[\"output\"]\n",
    "        problem_id = int(log_ids[0])\n",
    "        sample_id = int(log_ids[-1])\n",
    "        test_code = test[problem_id]\n",
    "\n",
    "        output_code = extract_text_inside_backticks(output, \"python\")\n",
    "        if not output_code: output_code = extract_text_inside_backticks(output, \"\")\n",
    "        if not output_code: output_code = \"\"\n",
    "\n",
    "        edit_log_dict = get_edit_log_dict(log_ids[:-1])\n",
    "\n",
    "        if \"prompts\" in edit_log_dict:\n",
    "            edit_log_dict[\"prompts\"][str(sample_id)] = prompt\n",
    "        else:\n",
    "            edit_log_dict[\"prompts\"] = {str(sample_id):prompt}\n",
    "\n",
    "        if \"outputs\" in edit_log_dict:\n",
    "            edit_log_dict[\"outputs\"][str(sample_id)] = output\n",
    "        else:\n",
    "            edit_log_dict[\"outputs\"] = {str(sample_id):output}\n",
    "\n",
    "        if \"output_codes\" in edit_log_dict:\n",
    "            edit_log_dict[\"output_codes\"][str(sample_id)] = output_code\n",
    "        else:\n",
    "            edit_log_dict[\"output_codes\"] = {str(sample_id):output_code}\n",
    "\n",
    "        test_cases.append(test_code)\n",
    "        candidates.append([output_code])\n",
    "        problem_ids.append(problem_id)\n",
    "        sample_ids.append(sample_id)\n",
    "        log_ids_list.append(log_ids)\n",
    "\n",
    "    \n",
    "    import os, time\n",
    "    os.environ[\"HF_ALLOW_CODE_EVAL\"] = \"1\"\n",
    "    from code_eval.code_eval import CodeEval\n",
    "    code_eval_metric = CodeEval()\n",
    "    # Compute pass@k\n",
    "    k_values = [1]\n",
    "    print(\"Evaluating generated code...\")\n",
    "    start = time.time()\n",
    "    pass_at_k, results = code_eval_metric._compute(\n",
    "        references=test_cases,\n",
    "        predictions=candidates,\n",
    "        k=k_values,\n",
    "        num_workers=10,  # Adjust based on your system\n",
    "        timeout=150.0,   # Adjust the timeout as needed\n",
    "    )\n",
    "    end = time.time()\n",
    "    print(\"calculation time(s): \", end-start)\n",
    "    \n",
    "    # For unknown reason, this shows weird value. \n",
    "    #for k in k_values:\n",
    "    #    print(f\"Pass@{k}: {pass_at_k[f'pass@{k}'] * 100:.2f}%\")\n",
    "    #    advice_result_log[str(iter)][f\"Pass@{k}\"] = pass_at_k[f'pass@{k}']\n",
    "    \n",
    "    for i in range(len(results)):\n",
    "        problem_id = problem_ids[i]\n",
    "        sample_id = sample_ids[i]\n",
    "        unexpected_error = False\n",
    "        if results[problem_id] == []:\n",
    "            is_correct = False  # [] appeared sometimes for unknown reason. I define it as incorrect for now, but it should be fixed.\n",
    "            unexpected_error = True\n",
    "        else: is_correct = results[problem_id][0][1][\"passed\"]\n",
    "        \n",
    "        log_ids = log_ids_list[i]\n",
    "        edit_log_dict = get_edit_log_dict(log_ids[:-1])\n",
    "\n",
    "        if \"corrects\" in edit_log_dict:\n",
    "            edit_log_dict[\"corrects\"][str(sample_id)] = is_correct\n",
    "        else:\n",
    "            edit_log_dict[\"corrects\"] = {str(sample_id):is_correct}\n",
    "\n",
    "        if not is_correct:\n",
    "            if not unexpected_error:\n",
    "                error = results[problem_id][0][1][\"result\"][\"error\"]\n",
    "                traceback_ = results[problem_id][0][1][\"result\"][\"traceback\"]\n",
    "            else:\n",
    "                error = \"\"\n",
    "                traceback_ = \"\"\n",
    "    \n",
    "            if \"errors\" in edit_log_dict:\n",
    "                edit_log_dict[\"errors\"][str(sample_id)] = error\n",
    "            else:\n",
    "                edit_log_dict[\"errors\"] = {str(sample_id):error}\n",
    "    \n",
    "            if \"tracebacks\" in edit_log_dict:\n",
    "                edit_log_dict[\"tracebacks\"][str(sample_id)] = traceback_\n",
    "            else:\n",
    "                edit_log_dict[\"tracebacks\"] = {str(sample_id):traceback_}\n",
    "    \n",
    "        if str(log_ids[0]) in advice_result_log[str(iter)][\"num_correct\"]:\n",
    "            if is_correct:\n",
    "                advice_result_log[str(iter)][\"num_correct\"][str(log_ids[0])] += 1\n",
    "        else:\n",
    "            if is_correct:\n",
    "                advice_result_log[str(iter)][\"num_correct\"][str(log_ids[0])] = 1\n",
    "            else:\n",
    "                advice_result_log[str(iter)][\"num_correct\"][str(log_ids[0])] = 0\n",
    "\n",
    "        if str(log_ids[0]) in advice_result_log[str(iter)][\"num_problem\"]:\n",
    "            advice_result_log[str(iter)][\"num_problem\"][str(log_ids[0])] += 1\n",
    "        else:\n",
    "            advice_result_log[str(iter)][\"num_problem\"][str(log_ids[0])] = 1\n",
    "\n",
    "    \n",
    "    num_problem = 0\n",
    "    num_correct = 0\n",
    "    for problem_id_str in advice_result_log[str(iter)][\"num_problem\"]:\n",
    "        num_problem += advice_result_log[str(iter)][\"num_problem\"][problem_id_str]\n",
    "    for problem_id_str in advice_result_log[str(iter)][\"num_correct\"]:\n",
    "        num_correct += advice_result_log[str(iter)][\"num_correct\"][problem_id_str]\n",
    "\n",
    "    num_already_correct_problem = total_num_problem - len(all_results1)\n",
    "\n",
    "    print(\"total_num_problem: \", total_num_problem)\n",
    "    print(\"num_already_correct_problem: \", num_already_correct_problem)\n",
    "    print(\"num_problem: \", num_problem)\n",
    "    print(\"num_correct: \", num_correct)\n",
    "    \n",
    "    pass1 = num_correct/num_problem\n",
    "    total_pass1 = (num_already_correct_problem+num_correct)/total_num_problem\n",
    "\n",
    "    advice_result_log[str(iter)][\"pass@1\"] = pass1\n",
    "    advice_result_log[str(iter)][\"total_pass@1\"] = total_pass1\n",
    "    print(\"pass1: \", pass1)\n",
    "    print(\"total_pass1: \", total_pass1)\n",
    "    \n",
    "    \n",
    "    log[\"advice_result_log\"] = advice_result_log\n",
    "    with open(save_file, \"w\") as json_file:\n",
    "        json.dump(log, json_file)\n",
    "\n",
    "print()\n",
    "print(\"-- ALL FINISHED --\")\n",
    "'''\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
