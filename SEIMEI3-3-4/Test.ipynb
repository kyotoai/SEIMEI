{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "82b47947-9409-4d44-a7b1-ce633fb5bbea",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting datasets\n",
      "  Downloading datasets-3.0.1-py3-none-any.whl.metadata (20 kB)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from datasets) (3.9.0)\n",
      "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from datasets) (1.24.1)\n",
      "Collecting pyarrow>=15.0.0 (from datasets)\n",
      "  Downloading pyarrow-17.0.0-cp310-cp310-manylinux_2_28_x86_64.whl.metadata (3.3 kB)\n",
      "Collecting dill<0.3.9,>=0.3.0 (from datasets)\n",
      "  Downloading dill-0.3.8-py3-none-any.whl.metadata (10 kB)\n",
      "Collecting pandas (from datasets)\n",
      "  Downloading pandas-2.2.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (89 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m89.9/89.9 kB\u001b[0m \u001b[31m5.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting requests>=2.32.2 (from datasets)\n",
      "  Downloading requests-2.32.3-py3-none-any.whl.metadata (4.6 kB)\n",
      "Collecting tqdm>=4.66.3 (from datasets)\n",
      "  Downloading tqdm-4.66.5-py3-none-any.whl.metadata (57 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m57.6/57.6 kB\u001b[0m \u001b[31m31.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting xxhash (from datasets)\n",
      "  Downloading xxhash-3.5.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (12 kB)\n",
      "Collecting multiprocess (from datasets)\n",
      "  Downloading multiprocess-0.70.17-py310-none-any.whl.metadata (7.2 kB)\n",
      "Requirement already satisfied: fsspec<=2024.6.1,>=2023.1.0 in /usr/local/lib/python3.10/dist-packages (from fsspec[http]<=2024.6.1,>=2023.1.0->datasets) (2023.4.0)\n",
      "Collecting aiohttp (from datasets)\n",
      "  Downloading aiohttp-3.10.10-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (7.6 kB)\n",
      "Collecting huggingface-hub>=0.22.0 (from datasets)\n",
      "  Downloading huggingface_hub-0.26.0-py3-none-any.whl.metadata (13 kB)\n",
      "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from datasets) (23.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from datasets) (6.0.1)\n",
      "Collecting aiohappyeyeballs>=2.3.0 (from aiohttp->datasets)\n",
      "  Downloading aiohappyeyeballs-2.4.3-py3-none-any.whl.metadata (6.1 kB)\n",
      "Collecting aiosignal>=1.1.2 (from aiohttp->datasets)\n",
      "  Downloading aiosignal-1.3.1-py3-none-any.whl.metadata (4.0 kB)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (23.1.0)\n",
      "Collecting frozenlist>=1.1.1 (from aiohttp->datasets)\n",
      "  Downloading frozenlist-1.4.1-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (12 kB)\n",
      "Collecting multidict<7.0,>=4.5 (from aiohttp->datasets)\n",
      "  Downloading multidict-6.1.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (5.0 kB)\n",
      "Collecting yarl<2.0,>=1.12.0 (from aiohttp->datasets)\n",
      "  Downloading yarl-1.15.5-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (61 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m61.7/61.7 kB\u001b[0m \u001b[31m26.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting async-timeout<5.0,>=4.0 (from aiohttp->datasets)\n",
      "  Downloading async_timeout-4.0.3-py3-none-any.whl.metadata (4.2 kB)\n",
      "INFO: pip is looking at multiple versions of huggingface-hub to determine which version is compatible with other requirements. This could take a while.\n",
      "Collecting huggingface-hub>=0.22.0 (from datasets)\n",
      "  Downloading huggingface_hub-0.25.2-py3-none-any.whl.metadata (13 kB)\n",
      "  Downloading huggingface_hub-0.25.1-py3-none-any.whl.metadata (13 kB)\n",
      "  Downloading huggingface_hub-0.25.0-py3-none-any.whl.metadata (13 kB)\n",
      "  Downloading huggingface_hub-0.24.7-py3-none-any.whl.metadata (13 kB)\n",
      "  Downloading huggingface_hub-0.24.6-py3-none-any.whl.metadata (13 kB)\n",
      "  Downloading huggingface_hub-0.24.5-py3-none-any.whl.metadata (13 kB)\n",
      "  Downloading huggingface_hub-0.24.4-py3-none-any.whl.metadata (13 kB)\n",
      "INFO: pip is still looking at multiple versions of huggingface-hub to determine which version is compatible with other requirements. This could take a while.\n",
      "  Downloading huggingface_hub-0.24.3-py3-none-any.whl.metadata (13 kB)\n",
      "  Downloading huggingface_hub-0.24.2-py3-none-any.whl.metadata (13 kB)\n",
      "  Downloading huggingface_hub-0.24.1-py3-none-any.whl.metadata (13 kB)\n",
      "  Downloading huggingface_hub-0.24.0-py3-none-any.whl.metadata (13 kB)\n",
      "  Downloading huggingface_hub-0.23.5-py3-none-any.whl.metadata (12 kB)\n",
      "INFO: This is taking longer than usual. You might need to provide the dependency resolver with stricter constraints to reduce runtime. See https://pip.pypa.io/warnings/backtracking for guidance. If you want to abort this run, press Ctrl + C.\n",
      "  Downloading huggingface_hub-0.23.4-py3-none-any.whl.metadata (12 kB)\n",
      "  Downloading huggingface_hub-0.23.3-py3-none-any.whl.metadata (12 kB)\n",
      "  Downloading huggingface_hub-0.23.2-py3-none-any.whl.metadata (12 kB)\n",
      "  Downloading huggingface_hub-0.23.1-py3-none-any.whl.metadata (12 kB)\n",
      "  Downloading huggingface_hub-0.23.0-py3-none-any.whl.metadata (12 kB)\n",
      "  Downloading huggingface_hub-0.22.2-py3-none-any.whl.metadata (12 kB)\n",
      "  Downloading huggingface_hub-0.22.1-py3-none-any.whl.metadata (12 kB)\n",
      "  Downloading huggingface_hub-0.22.0-py3-none-any.whl.metadata (12 kB)\n",
      "Collecting fsspec[http]<=2024.6.1,>=2023.1.0 (from datasets)\n",
      "  Downloading fsspec-2024.6.1-py3-none-any.whl.metadata (11 kB)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.22.0->datasets) (4.4.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets) (2.1.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets) (1.26.13)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets) (2022.12.7)\n",
      "INFO: pip is looking at multiple versions of multiprocess to determine which version is compatible with other requirements. This could take a while.\n",
      "Collecting multiprocess (from datasets)\n",
      "  Downloading multiprocess-0.70.16-py310-none-any.whl.metadata (7.2 kB)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2.8.2)\n",
      "Collecting pytz>=2020.1 (from pandas->datasets)\n",
      "  Downloading pytz-2024.2-py2.py3-none-any.whl.metadata (22 kB)\n",
      "Collecting tzdata>=2022.7 (from pandas->datasets)\n",
      "  Downloading tzdata-2024.2-py2.py3-none-any.whl.metadata (1.4 kB)\n",
      "Requirement already satisfied: six>=1.5 in /usr/lib/python3/dist-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.16.0)\n",
      "Collecting propcache>=0.2.0 (from yarl<2.0,>=1.12.0->aiohttp->datasets)\n",
      "  Downloading propcache-0.2.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (7.7 kB)\n",
      "Downloading datasets-3.0.1-py3-none-any.whl (471 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m471.6/471.6 kB\u001b[0m \u001b[31m130.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading dill-0.3.8-py3-none-any.whl (116 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m116.3/116.3 kB\u001b[0m \u001b[31m76.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading fsspec-2024.6.1-py3-none-any.whl (177 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m177.6/177.6 kB\u001b[0m \u001b[31m103.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading huggingface_hub-0.26.0-py3-none-any.whl (447 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m447.4/447.4 kB\u001b[0m \u001b[31m184.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading aiohttp-3.10.10-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.2 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m248.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading pyarrow-17.0.0-cp310-cp310-manylinux_2_28_x86_64.whl (39.9 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m39.9/39.9 MB\u001b[0m \u001b[31m95.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading requests-2.32.3-py3-none-any.whl (64 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m64.9/64.9 kB\u001b[0m \u001b[31m28.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading tqdm-4.66.5-py3-none-any.whl (78 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m78.4/78.4 kB\u001b[0m \u001b[31m36.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading multiprocess-0.70.16-py310-none-any.whl (134 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m134.8/134.8 kB\u001b[0m \u001b[31m57.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading pandas-2.2.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (13.1 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.1/13.1 MB\u001b[0m \u001b[31m99.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m:01\u001b[0mm\n",
      "\u001b[?25hDownloading xxhash-3.5.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (194 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m194.1/194.1 kB\u001b[0m \u001b[31m56.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading aiohappyeyeballs-2.4.3-py3-none-any.whl (14 kB)\n",
      "Downloading aiosignal-1.3.1-py3-none-any.whl (7.6 kB)\n",
      "Downloading async_timeout-4.0.3-py3-none-any.whl (5.7 kB)\n",
      "Downloading frozenlist-1.4.1-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (239 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m239.5/239.5 kB\u001b[0m \u001b[31m72.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading multidict-6.1.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (124 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m124.6/124.6 kB\u001b[0m \u001b[31m43.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading pytz-2024.2-py2.py3-none-any.whl (508 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m508.0/508.0 kB\u001b[0m \u001b[31m109.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading tzdata-2024.2-py2.py3-none-any.whl (346 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m346.6/346.6 kB\u001b[0m \u001b[31m97.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading yarl-1.15.5-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (314 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m314.5/314.5 kB\u001b[0m \u001b[31m93.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading propcache-0.2.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (208 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m208.9/208.9 kB\u001b[0m \u001b[31m75.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: pytz, xxhash, tzdata, tqdm, requests, pyarrow, propcache, multidict, fsspec, frozenlist, dill, async-timeout, aiohappyeyeballs, yarl, pandas, multiprocess, huggingface-hub, aiosignal, aiohttp, datasets\n",
      "  Attempting uninstall: requests\n",
      "    Found existing installation: requests 2.31.0\n",
      "    Uninstalling requests-2.31.0:\n",
      "      Successfully uninstalled requests-2.31.0\n",
      "  Attempting uninstall: fsspec\n",
      "    Found existing installation: fsspec 2023.4.0\n",
      "    Uninstalling fsspec-2023.4.0:\n",
      "      Successfully uninstalled fsspec-2023.4.0\n",
      "Successfully installed aiohappyeyeballs-2.4.3 aiohttp-3.10.10 aiosignal-1.3.1 async-timeout-4.0.3 datasets-3.0.1 dill-0.3.8 frozenlist-1.4.1 fsspec-2024.6.1 huggingface-hub-0.26.0 multidict-6.1.0 multiprocess-0.70.16 pandas-2.2.3 propcache-0.2.0 pyarrow-17.0.0 pytz-2024.2 requests-2.32.3 tqdm-4.66.5 tzdata-2024.2 xxhash-3.5.0 yarl-1.15.5\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.3.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.2\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython -m pip install --upgrade pip\u001b[0m\n",
      "Collecting transformers\n",
      "  Downloading transformers-4.45.2-py3-none-any.whl.metadata (44 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m44.4/44.4 kB\u001b[0m \u001b[31m1.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.9.0)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.23.2 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.26.0)\n",
      "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (1.24.1)\n",
      "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (23.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0.1)\n",
      "Collecting regex!=2019.12.17 (from transformers)\n",
      "  Downloading regex-2024.9.11-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (40 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m40.5/40.5 kB\u001b[0m \u001b[31m6.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.32.3)\n",
      "Collecting safetensors>=0.4.1 (from transformers)\n",
      "  Downloading safetensors-0.4.5-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (3.8 kB)\n",
      "Collecting tokenizers<0.21,>=0.20 (from transformers)\n",
      "  Downloading tokenizers-0.20.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.7 kB)\n",
      "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.66.5)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.23.2->transformers) (2024.6.1)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.23.2->transformers) (4.4.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2.1.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (1.26.13)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2022.12.7)\n",
      "Downloading transformers-4.45.2-py3-none-any.whl (9.9 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m9.9/9.9 MB\u001b[0m \u001b[31m24.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0mm\n",
      "\u001b[?25hDownloading regex-2024.9.11-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (782 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m782.7/782.7 kB\u001b[0m \u001b[31m154.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading safetensors-0.4.5-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (435 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m435.0/435.0 kB\u001b[0m \u001b[31m126.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading tokenizers-0.20.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.0 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.0/3.0 MB\u001b[0m \u001b[31m201.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: safetensors, regex, tokenizers, transformers\n",
      "Successfully installed regex-2024.9.11 safetensors-0.4.5 tokenizers-0.20.1 transformers-4.45.2\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.3.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.2\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython -m pip install --upgrade pip\u001b[0m\n",
      "Collecting accelerate\n",
      "  Downloading accelerate-1.0.1-py3-none-any.whl.metadata (19 kB)\n",
      "Requirement already satisfied: numpy<3.0.0,>=1.17 in /usr/local/lib/python3.10/dist-packages (from accelerate) (1.24.1)\n",
      "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from accelerate) (23.2)\n",
      "Requirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (from accelerate) (5.9.6)\n",
      "Requirement already satisfied: pyyaml in /usr/local/lib/python3.10/dist-packages (from accelerate) (6.0.1)\n",
      "Requirement already satisfied: torch>=1.10.0 in /usr/local/lib/python3.10/dist-packages (from accelerate) (2.1.0+cu118)\n",
      "Requirement already satisfied: huggingface-hub>=0.21.0 in /usr/local/lib/python3.10/dist-packages (from accelerate) (0.26.0)\n",
      "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.10/dist-packages (from accelerate) (0.4.5)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.21.0->accelerate) (3.9.0)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.21.0->accelerate) (2024.6.1)\n",
      "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.21.0->accelerate) (2.32.3)\n",
      "Requirement already satisfied: tqdm>=4.42.1 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.21.0->accelerate) (4.66.5)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.21.0->accelerate) (4.4.0)\n",
      "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (1.12)\n",
      "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (3.0)\n",
      "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (3.1.2)\n",
      "Requirement already satisfied: triton==2.1.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (2.1.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.10.0->accelerate) (2.1.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.21.0->accelerate) (2.1.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.21.0->accelerate) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.21.0->accelerate) (1.26.13)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.21.0->accelerate) (2022.12.7)\n",
      "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=1.10.0->accelerate) (1.3.0)\n",
      "Downloading accelerate-1.0.1-py3-none-any.whl (330 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m330.9/330.9 kB\u001b[0m \u001b[31m7.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: accelerate\n",
      "Successfully installed accelerate-1.0.1\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.3.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.2\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython -m pip install --upgrade pip\u001b[0m\n",
      "\u001b[31mERROR: Invalid requirement: 'numpy,'\u001b[0m\u001b[31m\n",
      "\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.3.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.2\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython -m pip install --upgrade pip\u001b[0m\n",
      "Collecting matplotlib\n",
      "  Downloading matplotlib-3.9.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (11 kB)\n",
      "Collecting contourpy>=1.0.1 (from matplotlib)\n",
      "  Downloading contourpy-1.3.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (5.4 kB)\n",
      "Collecting cycler>=0.10 (from matplotlib)\n",
      "  Downloading cycler-0.12.1-py3-none-any.whl.metadata (3.8 kB)\n",
      "Collecting fonttools>=4.22.0 (from matplotlib)\n",
      "  Downloading fonttools-4.54.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (163 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m163.7/163.7 kB\u001b[0m \u001b[31m6.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting kiwisolver>=1.3.1 (from matplotlib)\n",
      "  Downloading kiwisolver-1.4.7-cp310-cp310-manylinux_2_12_x86_64.manylinux2010_x86_64.whl.metadata (6.3 kB)\n",
      "Requirement already satisfied: numpy>=1.23 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (1.24.1)\n",
      "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (23.2)\n",
      "Requirement already satisfied: pillow>=8 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (9.3.0)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in /usr/lib/python3/dist-packages (from matplotlib) (2.4.7)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (2.8.2)\n",
      "Requirement already satisfied: six>=1.5 in /usr/lib/python3/dist-packages (from python-dateutil>=2.7->matplotlib) (1.16.0)\n",
      "Downloading matplotlib-3.9.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (8.3 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m8.3/8.3 MB\u001b[0m \u001b[31m179.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading contourpy-1.3.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (322 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m322.0/322.0 kB\u001b[0m \u001b[31m147.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading cycler-0.12.1-py3-none-any.whl (8.3 kB)\n",
      "Downloading fonttools-4.54.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (4.6 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.6/4.6 MB\u001b[0m \u001b[31m282.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading kiwisolver-1.4.7-cp310-cp310-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (1.6 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.6/1.6 MB\u001b[0m \u001b[31m273.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: kiwisolver, fonttools, cycler, contourpy, matplotlib\n",
      "Successfully installed contourpy-1.3.0 cycler-0.12.1 fonttools-4.54.1 kiwisolver-1.4.7 matplotlib-3.9.2\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.3.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.2\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython -m pip install --upgrade pip\u001b[0m\n",
      "Collecting sentence_transformers\n",
      "  Downloading sentence_transformers-3.2.0-py3-none-any.whl.metadata (10 kB)\n",
      "Requirement already satisfied: transformers<5.0.0,>=4.41.0 in /usr/local/lib/python3.10/dist-packages (from sentence_transformers) (4.45.2)\n",
      "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from sentence_transformers) (4.66.5)\n",
      "Requirement already satisfied: torch>=1.11.0 in /usr/local/lib/python3.10/dist-packages (from sentence_transformers) (2.1.0+cu118)\n",
      "Collecting scikit-learn (from sentence_transformers)\n",
      "  Downloading scikit_learn-1.5.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (13 kB)\n",
      "Collecting scipy (from sentence_transformers)\n",
      "  Downloading scipy-1.14.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (60 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m60.8/60.8 kB\u001b[0m \u001b[31m2.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: huggingface-hub>=0.20.0 in /usr/local/lib/python3.10/dist-packages (from sentence_transformers) (0.26.0)\n",
      "Requirement already satisfied: Pillow in /usr/local/lib/python3.10/dist-packages (from sentence_transformers) (9.3.0)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.20.0->sentence_transformers) (3.9.0)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.20.0->sentence_transformers) (2024.6.1)\n",
      "Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.20.0->sentence_transformers) (23.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.20.0->sentence_transformers) (6.0.1)\n",
      "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.20.0->sentence_transformers) (2.32.3)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.20.0->sentence_transformers) (4.4.0)\n",
      "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=1.11.0->sentence_transformers) (1.12)\n",
      "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.11.0->sentence_transformers) (3.0)\n",
      "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.11.0->sentence_transformers) (3.1.2)\n",
      "Requirement already satisfied: triton==2.1.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.11.0->sentence_transformers) (2.1.0)\n",
      "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers<5.0.0,>=4.41.0->sentence_transformers) (1.24.1)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers<5.0.0,>=4.41.0->sentence_transformers) (2024.9.11)\n",
      "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from transformers<5.0.0,>=4.41.0->sentence_transformers) (0.4.5)\n",
      "Requirement already satisfied: tokenizers<0.21,>=0.20 in /usr/local/lib/python3.10/dist-packages (from transformers<5.0.0,>=4.41.0->sentence_transformers) (0.20.1)\n",
      "Collecting joblib>=1.2.0 (from scikit-learn->sentence_transformers)\n",
      "  Downloading joblib-1.4.2-py3-none-any.whl.metadata (5.4 kB)\n",
      "Collecting threadpoolctl>=3.1.0 (from scikit-learn->sentence_transformers)\n",
      "  Downloading threadpoolctl-3.5.0-py3-none-any.whl.metadata (13 kB)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.11.0->sentence_transformers) (2.1.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.20.0->sentence_transformers) (2.1.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.20.0->sentence_transformers) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.20.0->sentence_transformers) (1.26.13)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.20.0->sentence_transformers) (2022.12.7)\n",
      "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=1.11.0->sentence_transformers) (1.3.0)\n",
      "Downloading sentence_transformers-3.2.0-py3-none-any.whl (255 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m255.2/255.2 kB\u001b[0m \u001b[31m7.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading scikit_learn-1.5.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (13.3 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.3/13.3 MB\u001b[0m \u001b[31m194.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m:01\u001b[0m\n",
      "\u001b[?25hDownloading scipy-1.14.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (41.2 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m41.2/41.2 MB\u001b[0m \u001b[31m136.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading joblib-1.4.2-py3-none-any.whl (301 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m301.8/301.8 kB\u001b[0m \u001b[31m123.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading threadpoolctl-3.5.0-py3-none-any.whl (18 kB)\n",
      "Installing collected packages: threadpoolctl, scipy, joblib, scikit-learn, sentence_transformers\n",
      "Successfully installed joblib-1.4.2 scikit-learn-1.5.2 scipy-1.14.1 sentence_transformers-3.2.0 threadpoolctl-3.5.0\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.3.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.2\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython -m pip install --upgrade pip\u001b[0m\n",
      "Requirement already satisfied: huggingface_hub in /usr/local/lib/python3.10/dist-packages (0.26.0)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from huggingface_hub) (3.9.0)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface_hub) (2024.6.1)\n",
      "Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.10/dist-packages (from huggingface_hub) (23.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from huggingface_hub) (6.0.1)\n",
      "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from huggingface_hub) (2.32.3)\n",
      "Requirement already satisfied: tqdm>=4.42.1 in /usr/local/lib/python3.10/dist-packages (from huggingface_hub) (4.66.5)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface_hub) (4.4.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface_hub) (2.1.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface_hub) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface_hub) (1.26.13)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface_hub) (2022.12.7)\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.3.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.2\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython -m pip install --upgrade pip\u001b[0m\n",
      "Collecting lm-format-enforcer\n",
      "  Downloading lm_format_enforcer-0.10.9-py3-none-any.whl.metadata (17 kB)\n",
      "Collecting interegular>=0.3.2 (from lm-format-enforcer)\n",
      "  Downloading interegular-0.3.3-py37-none-any.whl.metadata (3.0 kB)\n",
      "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from lm-format-enforcer) (23.2)\n",
      "Collecting pydantic>=1.10.8 (from lm-format-enforcer)\n",
      "  Downloading pydantic-2.9.2-py3-none-any.whl.metadata (149 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m149.4/149.4 kB\u001b[0m \u001b[31m4.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: pyyaml in /usr/local/lib/python3.10/dist-packages (from lm-format-enforcer) (6.0.1)\n",
      "Collecting annotated-types>=0.6.0 (from pydantic>=1.10.8->lm-format-enforcer)\n",
      "  Downloading annotated_types-0.7.0-py3-none-any.whl.metadata (15 kB)\n",
      "Collecting pydantic-core==2.23.4 (from pydantic>=1.10.8->lm-format-enforcer)\n",
      "  Downloading pydantic_core-2.23.4-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.6 kB)\n",
      "Collecting typing-extensions>=4.6.1 (from pydantic>=1.10.8->lm-format-enforcer)\n",
      "  Downloading typing_extensions-4.12.2-py3-none-any.whl.metadata (3.0 kB)\n",
      "Downloading lm_format_enforcer-0.10.9-py3-none-any.whl (43 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m43.9/43.9 kB\u001b[0m \u001b[31m22.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading interegular-0.3.3-py37-none-any.whl (23 kB)\n",
      "Downloading pydantic-2.9.2-py3-none-any.whl (434 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m434.9/434.9 kB\u001b[0m \u001b[31m118.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading pydantic_core-2.23.4-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.1 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.1/2.1 MB\u001b[0m \u001b[31m227.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading annotated_types-0.7.0-py3-none-any.whl (13 kB)\n",
      "Downloading typing_extensions-4.12.2-py3-none-any.whl (37 kB)\n",
      "Installing collected packages: typing-extensions, interegular, annotated-types, pydantic-core, pydantic, lm-format-enforcer\n",
      "  Attempting uninstall: typing-extensions\n",
      "    Found existing installation: typing_extensions 4.4.0\n",
      "    Uninstalling typing_extensions-4.4.0:\n",
      "      Successfully uninstalled typing_extensions-4.4.0\n",
      "Successfully installed annotated-types-0.7.0 interegular-0.3.3 lm-format-enforcer-0.10.9 pydantic-2.9.2 pydantic-core-2.23.4 typing-extensions-4.12.2\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.3.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.2\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython -m pip install --upgrade pip\u001b[0m\n",
      "Collecting vllm\n",
      "  Downloading vllm-0.6.3.post1-cp38-abi3-manylinux1_x86_64.whl.metadata (10 kB)\n",
      "Requirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (from vllm) (5.9.6)\n",
      "Collecting sentencepiece (from vllm)\n",
      "  Downloading sentencepiece-0.2.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (7.7 kB)\n",
      "Requirement already satisfied: numpy<2.0.0 in /usr/local/lib/python3.10/dist-packages (from vllm) (1.24.1)\n",
      "Requirement already satisfied: requests>=2.26.0 in /usr/local/lib/python3.10/dist-packages (from vllm) (2.32.3)\n",
      "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from vllm) (4.66.5)\n",
      "Collecting py-cpuinfo (from vllm)\n",
      "  Downloading py_cpuinfo-9.0.0-py3-none-any.whl.metadata (794 bytes)\n",
      "Requirement already satisfied: transformers>=4.45.2 in /usr/local/lib/python3.10/dist-packages (from vllm) (4.45.2)\n",
      "Requirement already satisfied: tokenizers>=0.19.1 in /usr/local/lib/python3.10/dist-packages (from vllm) (0.20.1)\n",
      "Collecting protobuf (from vllm)\n",
      "  Downloading protobuf-5.28.2-cp38-abi3-manylinux2014_x86_64.whl.metadata (592 bytes)\n",
      "Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from vllm) (3.10.10)\n",
      "Collecting openai>=1.40.0 (from vllm)\n",
      "  Downloading openai-1.52.0-py3-none-any.whl.metadata (24 kB)\n",
      "Collecting uvicorn[standard] (from vllm)\n",
      "  Downloading uvicorn-0.32.0-py3-none-any.whl.metadata (6.6 kB)\n",
      "Requirement already satisfied: pydantic>=2.9 in /usr/local/lib/python3.10/dist-packages (from vllm) (2.9.2)\n",
      "Requirement already satisfied: pillow in /usr/local/lib/python3.10/dist-packages (from vllm) (9.3.0)\n",
      "Requirement already satisfied: prometheus-client>=0.18.0 in /usr/local/lib/python3.10/dist-packages (from vllm) (0.18.0)\n",
      "Collecting prometheus-fastapi-instrumentator>=7.0.0 (from vllm)\n",
      "  Downloading prometheus_fastapi_instrumentator-7.0.0-py3-none-any.whl.metadata (13 kB)\n",
      "Collecting tiktoken>=0.6.0 (from vllm)\n",
      "  Downloading tiktoken-0.8.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.6 kB)\n",
      "Collecting lm-format-enforcer==0.10.6 (from vllm)\n",
      "  Downloading lm_format_enforcer-0.10.6-py3-none-any.whl.metadata (16 kB)\n",
      "Collecting outlines<0.1,>=0.0.43 (from vllm)\n",
      "  Downloading outlines-0.0.46-py3-none-any.whl.metadata (15 kB)\n",
      "Requirement already satisfied: typing-extensions>=4.10 in /usr/local/lib/python3.10/dist-packages (from vllm) (4.12.2)\n",
      "Collecting filelock>=3.10.4 (from vllm)\n",
      "  Downloading filelock-3.16.1-py3-none-any.whl.metadata (2.9 kB)\n",
      "Collecting partial-json-parser (from vllm)\n",
      "  Downloading partial_json_parser-0.2.1.1.post4-py3-none-any.whl.metadata (6.2 kB)\n",
      "Requirement already satisfied: pyzmq in /usr/local/lib/python3.10/dist-packages (from vllm) (24.0.1)\n",
      "Collecting msgspec (from vllm)\n",
      "  Downloading msgspec-0.18.6-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.9 kB)\n",
      "Collecting gguf==0.10.0 (from vllm)\n",
      "  Downloading gguf-0.10.0-py3-none-any.whl.metadata (3.5 kB)\n",
      "Requirement already satisfied: importlib-metadata in /usr/lib/python3/dist-packages (from vllm) (4.6.4)\n",
      "Collecting mistral-common>=1.4.4 (from mistral-common[opencv]>=1.4.4->vllm)\n",
      "  Downloading mistral_common-1.4.4-py3-none-any.whl.metadata (4.6 kB)\n",
      "Requirement already satisfied: pyyaml in /usr/local/lib/python3.10/dist-packages (from vllm) (6.0.1)\n",
      "Collecting einops (from vllm)\n",
      "  Downloading einops-0.8.0-py3-none-any.whl.metadata (12 kB)\n",
      "Collecting compressed-tensors==0.6.0 (from vllm)\n",
      "  Downloading compressed_tensors-0.6.0-py3-none-any.whl.metadata (6.8 kB)\n",
      "Collecting ray>=2.9 (from vllm)\n",
      "  Downloading ray-2.37.0-cp310-cp310-manylinux2014_x86_64.whl.metadata (16 kB)\n",
      "Collecting nvidia-ml-py (from vllm)\n",
      "  Downloading nvidia_ml_py-12.560.30-py3-none-any.whl.metadata (8.6 kB)\n",
      "Collecting torch==2.4.0 (from vllm)\n",
      "  Downloading torch-2.4.0-cp310-cp310-manylinux1_x86_64.whl.metadata (26 kB)\n",
      "Collecting torchvision==0.19 (from vllm)\n",
      "  Downloading torchvision-0.19.0-cp310-cp310-manylinux1_x86_64.whl.metadata (6.0 kB)\n",
      "Collecting xformers==0.0.27.post2 (from vllm)\n",
      "  Downloading xformers-0.0.27.post2-cp310-cp310-manylinux2014_x86_64.whl.metadata (1.0 kB)\n",
      "Collecting fastapi!=0.113.*,!=0.114.0,>=0.107.0 (from vllm)\n",
      "  Downloading fastapi-0.115.2-py3-none-any.whl.metadata (27 kB)\n",
      "Requirement already satisfied: interegular>=0.3.2 in /usr/local/lib/python3.10/dist-packages (from lm-format-enforcer==0.10.6->vllm) (0.3.3)\n",
      "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from lm-format-enforcer==0.10.6->vllm) (23.2)\n",
      "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch==2.4.0->vllm) (1.12)\n",
      "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch==2.4.0->vllm) (3.0)\n",
      "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch==2.4.0->vllm) (3.1.2)\n",
      "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch==2.4.0->vllm) (2024.6.1)\n",
      "Collecting nvidia-cuda-nvrtc-cu12==12.1.105 (from torch==2.4.0->vllm)\n",
      "  Downloading nvidia_cuda_nvrtc_cu12-12.1.105-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\n",
      "Collecting nvidia-cuda-runtime-cu12==12.1.105 (from torch==2.4.0->vllm)\n",
      "  Downloading nvidia_cuda_runtime_cu12-12.1.105-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\n",
      "Collecting nvidia-cuda-cupti-cu12==12.1.105 (from torch==2.4.0->vllm)\n",
      "  Downloading nvidia_cuda_cupti_cu12-12.1.105-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\n",
      "Collecting nvidia-cudnn-cu12==9.1.0.70 (from torch==2.4.0->vllm)\n",
      "  Downloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
      "Collecting nvidia-cublas-cu12==12.1.3.1 (from torch==2.4.0->vllm)\n",
      "  Downloading nvidia_cublas_cu12-12.1.3.1-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\n",
      "Collecting nvidia-cufft-cu12==11.0.2.54 (from torch==2.4.0->vllm)\n",
      "  Downloading nvidia_cufft_cu12-11.0.2.54-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\n",
      "Collecting nvidia-curand-cu12==10.3.2.106 (from torch==2.4.0->vllm)\n",
      "  Downloading nvidia_curand_cu12-10.3.2.106-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\n",
      "Collecting nvidia-cusolver-cu12==11.4.5.107 (from torch==2.4.0->vllm)\n",
      "  Downloading nvidia_cusolver_cu12-11.4.5.107-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\n",
      "Collecting nvidia-cusparse-cu12==12.1.0.106 (from torch==2.4.0->vllm)\n",
      "  Downloading nvidia_cusparse_cu12-12.1.0.106-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\n",
      "Collecting nvidia-nccl-cu12==2.20.5 (from torch==2.4.0->vllm)\n",
      "  Downloading nvidia_nccl_cu12-2.20.5-py3-none-manylinux2014_x86_64.whl.metadata (1.8 kB)\n",
      "Collecting nvidia-nvtx-cu12==12.1.105 (from torch==2.4.0->vllm)\n",
      "  Downloading nvidia_nvtx_cu12-12.1.105-py3-none-manylinux1_x86_64.whl.metadata (1.7 kB)\n",
      "Collecting triton==3.0.0 (from torch==2.4.0->vllm)\n",
      "  Downloading triton-3.0.0-1-cp310-cp310-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (1.3 kB)\n",
      "Collecting nvidia-nvjitlink-cu12 (from nvidia-cusolver-cu12==11.4.5.107->torch==2.4.0->vllm)\n",
      "  Downloading nvidia_nvjitlink_cu12-12.6.77-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
      "Collecting starlette<0.41.0,>=0.37.2 (from fastapi!=0.113.*,!=0.114.0,>=0.107.0->vllm)\n",
      "  Downloading starlette-0.40.0-py3-none-any.whl.metadata (6.0 kB)\n",
      "Collecting jsonschema<5.0.0,>=4.21.1 (from mistral-common>=1.4.4->mistral-common[opencv]>=1.4.4->vllm)\n",
      "  Downloading jsonschema-4.23.0-py3-none-any.whl.metadata (7.9 kB)\n",
      "Collecting numpy<2.0.0 (from vllm)\n",
      "  Downloading numpy-1.26.4-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (61 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m61.0/61.0 kB\u001b[0m \u001b[31m22.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting pillow (from vllm)\n",
      "  Downloading pillow-10.4.0-cp310-cp310-manylinux_2_28_x86_64.whl.metadata (9.2 kB)\n",
      "Collecting tiktoken>=0.6.0 (from vllm)\n",
      "  Downloading tiktoken-0.7.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.6 kB)\n",
      "Collecting opencv-python-headless<5.0.0,>=4.0.0 (from mistral-common[opencv]>=1.4.4->vllm)\n",
      "  Downloading opencv_python_headless-4.10.0.84-cp37-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (20 kB)\n",
      "Requirement already satisfied: anyio<5,>=3.5.0 in /usr/local/lib/python3.10/dist-packages (from openai>=1.40.0->vllm) (4.0.0)\n",
      "Requirement already satisfied: distro<2,>=1.7.0 in /usr/lib/python3/dist-packages (from openai>=1.40.0->vllm) (1.7.0)\n",
      "Collecting httpx<1,>=0.23.0 (from openai>=1.40.0->vllm)\n",
      "  Downloading httpx-0.27.2-py3-none-any.whl.metadata (7.1 kB)\n",
      "Collecting jiter<1,>=0.4.0 (from openai>=1.40.0->vllm)\n",
      "  Downloading jiter-0.6.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (5.2 kB)\n",
      "Requirement already satisfied: sniffio in /usr/local/lib/python3.10/dist-packages (from openai>=1.40.0->vllm) (1.3.0)\n",
      "Collecting lark (from outlines<0.1,>=0.0.43->vllm)\n",
      "  Downloading lark-1.2.2-py3-none-any.whl.metadata (1.8 kB)\n",
      "Requirement already satisfied: nest-asyncio in /usr/local/lib/python3.10/dist-packages (from outlines<0.1,>=0.0.43->vllm) (1.5.8)\n",
      "Collecting cloudpickle (from outlines<0.1,>=0.0.43->vllm)\n",
      "  Downloading cloudpickle-3.1.0-py3-none-any.whl.metadata (7.0 kB)\n",
      "Collecting diskcache (from outlines<0.1,>=0.0.43->vllm)\n",
      "  Downloading diskcache-5.6.3-py3-none-any.whl.metadata (20 kB)\n",
      "Collecting numba (from outlines<0.1,>=0.0.43->vllm)\n",
      "  Downloading numba-0.60.0-cp310-cp310-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (2.7 kB)\n",
      "Requirement already satisfied: referencing in /usr/local/lib/python3.10/dist-packages (from outlines<0.1,>=0.0.43->vllm) (0.30.2)\n",
      "Requirement already satisfied: datasets in /usr/local/lib/python3.10/dist-packages (from outlines<0.1,>=0.0.43->vllm) (3.0.1)\n",
      "Collecting pycountry (from outlines<0.1,>=0.0.43->vllm)\n",
      "  Downloading pycountry-24.6.1-py3-none-any.whl.metadata (12 kB)\n",
      "Collecting pyairports (from outlines<0.1,>=0.0.43->vllm)\n",
      "  Downloading pyairports-2.1.1-py3-none-any.whl.metadata (1.7 kB)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.10/dist-packages (from pydantic>=2.9->vllm) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.23.4 in /usr/local/lib/python3.10/dist-packages (from pydantic>=2.9->vllm) (2.23.4)\n",
      "Collecting click>=7.0 (from ray>=2.9->vllm)\n",
      "  Downloading click-8.1.7-py3-none-any.whl.metadata (3.0 kB)\n",
      "Collecting msgpack<2.0.0,>=1.0.0 (from ray>=2.9->vllm)\n",
      "  Downloading msgpack-1.1.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (8.4 kB)\n",
      "Requirement already satisfied: aiosignal in /usr/local/lib/python3.10/dist-packages (from ray>=2.9->vllm) (1.3.1)\n",
      "Requirement already satisfied: frozenlist in /usr/local/lib/python3.10/dist-packages (from ray>=2.9->vllm) (1.4.1)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->vllm) (2.1.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->vllm) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->vllm) (1.26.13)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->vllm) (2022.12.7)\n",
      "Requirement already satisfied: regex>=2022.1.18 in /usr/local/lib/python3.10/dist-packages (from tiktoken>=0.6.0->vllm) (2024.9.11)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.16.4 in /usr/local/lib/python3.10/dist-packages (from tokenizers>=0.19.1->vllm) (0.26.0)\n",
      "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from transformers>=4.45.2->vllm) (0.4.5)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->vllm) (2.4.3)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->vllm) (23.1.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->vllm) (6.1.0)\n",
      "Requirement already satisfied: yarl<2.0,>=1.12.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->vllm) (1.15.5)\n",
      "Requirement already satisfied: async-timeout<5.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->vllm) (4.0.3)\n",
      "Collecting h11>=0.8 (from uvicorn[standard]->vllm)\n",
      "  Downloading h11-0.14.0-py3-none-any.whl.metadata (8.2 kB)\n",
      "Collecting httptools>=0.5.0 (from uvicorn[standard]->vllm)\n",
      "  Downloading httptools-0.6.4-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (3.6 kB)\n",
      "Collecting python-dotenv>=0.13 (from uvicorn[standard]->vllm)\n",
      "  Downloading python_dotenv-1.0.1-py3-none-any.whl.metadata (23 kB)\n",
      "Collecting uvloop!=0.15.0,!=0.15.1,>=0.14.0 (from uvicorn[standard]->vllm)\n",
      "  Downloading uvloop-0.21.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.9 kB)\n",
      "Collecting watchfiles>=0.13 (from uvicorn[standard]->vllm)\n",
      "  Downloading watchfiles-0.24.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.9 kB)\n",
      "Collecting websockets>=10.4 (from uvicorn[standard]->vllm)\n",
      "  Downloading websockets-13.1-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.8 kB)\n",
      "Requirement already satisfied: exceptiongroup>=1.0.2 in /usr/local/lib/python3.10/dist-packages (from anyio<5,>=3.5.0->openai>=1.40.0->vllm) (1.1.3)\n",
      "Collecting httpcore==1.* (from httpx<1,>=0.23.0->openai>=1.40.0->vllm)\n",
      "  Downloading httpcore-1.0.6-py3-none-any.whl.metadata (21 kB)\n",
      "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in /usr/local/lib/python3.10/dist-packages (from jsonschema<5.0.0,>=4.21.1->mistral-common>=1.4.4->mistral-common[opencv]>=1.4.4->vllm) (2023.7.1)\n",
      "Requirement already satisfied: rpds-py>=0.7.1 in /usr/local/lib/python3.10/dist-packages (from jsonschema<5.0.0,>=4.21.1->mistral-common>=1.4.4->mistral-common[opencv]>=1.4.4->vllm) (0.12.0)\n",
      "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.10/dist-packages (from yarl<2.0,>=1.12.0->aiohttp->vllm) (0.2.0)\n",
      "Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.10/dist-packages (from datasets->outlines<0.1,>=0.0.43->vllm) (17.0.0)\n",
      "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from datasets->outlines<0.1,>=0.0.43->vllm) (0.3.8)\n",
      "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from datasets->outlines<0.1,>=0.0.43->vllm) (2.2.3)\n",
      "Requirement already satisfied: xxhash in /usr/local/lib/python3.10/dist-packages (from datasets->outlines<0.1,>=0.0.43->vllm) (3.5.0)\n",
      "Requirement already satisfied: multiprocess in /usr/local/lib/python3.10/dist-packages (from datasets->outlines<0.1,>=0.0.43->vllm) (0.70.16)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch==2.4.0->vllm) (2.1.2)\n",
      "Collecting llvmlite<0.44,>=0.43.0dev0 (from numba->outlines<0.1,>=0.0.43->vllm)\n",
      "  Downloading llvmlite-0.43.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.8 kB)\n",
      "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch==2.4.0->vllm) (1.3.0)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets->outlines<0.1,>=0.0.43->vllm) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets->outlines<0.1,>=0.0.43->vllm) (2024.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets->outlines<0.1,>=0.0.43->vllm) (2024.2)\n",
      "Requirement already satisfied: six>=1.5 in /usr/lib/python3/dist-packages (from python-dateutil>=2.8.2->pandas->datasets->outlines<0.1,>=0.0.43->vllm) (1.16.0)\n",
      "Downloading vllm-0.6.3.post1-cp38-abi3-manylinux1_x86_64.whl (194.8 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m194.8/194.8 MB\u001b[0m \u001b[31m51.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading compressed_tensors-0.6.0-py3-none-any.whl (92 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m92.0/92.0 kB\u001b[0m \u001b[31m53.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading gguf-0.10.0-py3-none-any.whl (71 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m71.6/71.6 kB\u001b[0m \u001b[31m48.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading lm_format_enforcer-0.10.6-py3-none-any.whl (43 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m43.7/43.7 kB\u001b[0m \u001b[31m27.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading torch-2.4.0-cp310-cp310-manylinux1_x86_64.whl (797.2 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m797.2/797.2 MB\u001b[0m \u001b[31m15.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading torchvision-0.19.0-cp310-cp310-manylinux1_x86_64.whl (7.0 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.0/7.0 MB\u001b[0m \u001b[31m197.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading xformers-0.0.27.post2-cp310-cp310-manylinux2014_x86_64.whl (20.8 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m20.8/20.8 MB\u001b[0m \u001b[31m143.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cublas_cu12-12.1.3.1-py3-none-manylinux1_x86_64.whl (410.6 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m410.6/410.6 MB\u001b[0m \u001b[31m28.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cuda_cupti_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (14.1 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m14.1/14.1 MB\u001b[0m \u001b[31m56.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cuda_nvrtc_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (23.7 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m23.7/23.7 MB\u001b[0m \u001b[31m181.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cuda_runtime_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (823 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m823.6/823.6 kB\u001b[0m \u001b[31m185.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl (664.8 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m19.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cufft_cu12-11.0.2.54-py3-none-manylinux1_x86_64.whl (121.6 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m121.6/121.6 MB\u001b[0m \u001b[31m77.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_curand_cu12-10.3.2.106-py3-none-manylinux1_x86_64.whl (56.5 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.5/56.5 MB\u001b[0m \u001b[31m42.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m:00:01\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cusolver_cu12-11.4.5.107-py3-none-manylinux1_x86_64.whl (124.2 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m124.2/124.2 MB\u001b[0m \u001b[31m85.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cusparse_cu12-12.1.0.106-py3-none-manylinux1_x86_64.whl (196.0 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m196.0/196.0 MB\u001b[0m \u001b[31m57.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_nccl_cu12-2.20.5-py3-none-manylinux2014_x86_64.whl (176.2 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m176.2/176.2 MB\u001b[0m \u001b[31m62.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_nvtx_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (99 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m99.1/99.1 kB\u001b[0m \u001b[31m57.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading triton-3.0.0-1-cp310-cp310-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (209.4 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m209.4/209.4 MB\u001b[0m \u001b[31m51.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading fastapi-0.115.2-py3-none-any.whl (94 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m94.7/94.7 kB\u001b[0m \u001b[31m48.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading filelock-3.16.1-py3-none-any.whl (16 kB)\n",
      "Downloading mistral_common-1.4.4-py3-none-any.whl (6.0 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.0/6.0 MB\u001b[0m \u001b[31m189.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading sentencepiece-0.2.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m225.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading numpy-1.26.4-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (18.2 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m18.2/18.2 MB\u001b[0m \u001b[31m218.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading openai-1.52.0-py3-none-any.whl (386 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m386.9/386.9 kB\u001b[0m \u001b[31m137.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading outlines-0.0.46-py3-none-any.whl (101 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m101.9/101.9 kB\u001b[0m \u001b[31m58.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading pillow-10.4.0-cp310-cp310-manylinux_2_28_x86_64.whl (4.5 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.5/4.5 MB\u001b[0m \u001b[31m260.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading prometheus_fastapi_instrumentator-7.0.0-py3-none-any.whl (19 kB)\n",
      "Downloading ray-2.37.0-cp310-cp310-manylinux2014_x86_64.whl (65.6 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m65.6/65.6 MB\u001b[0m \u001b[31m80.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading protobuf-5.28.2-cp38-abi3-manylinux2014_x86_64.whl (316 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m316.6/316.6 kB\u001b[0m \u001b[31m123.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading tiktoken-0.7.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.1 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.1/1.1 MB\u001b[0m \u001b[31m168.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading einops-0.8.0-py3-none-any.whl (43 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m43.2/43.2 kB\u001b[0m \u001b[31m12.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading msgspec-0.18.6-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (210 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m210.3/210.3 kB\u001b[0m \u001b[31m57.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_ml_py-12.560.30-py3-none-any.whl (40 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m40.5/40.5 kB\u001b[0m \u001b[31m14.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading partial_json_parser-0.2.1.1.post4-py3-none-any.whl (9.9 kB)\n",
      "Downloading py_cpuinfo-9.0.0-py3-none-any.whl (22 kB)\n",
      "Downloading click-8.1.7-py3-none-any.whl (97 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m97.9/97.9 kB\u001b[0m \u001b[31m31.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading h11-0.14.0-py3-none-any.whl (58 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.3/58.3 kB\u001b[0m \u001b[31m18.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading httptools-0.6.4-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (442 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m442.1/442.1 kB\u001b[0m \u001b[31m131.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading httpx-0.27.2-py3-none-any.whl (76 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m76.4/76.4 kB\u001b[0m \u001b[31m43.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading httpcore-1.0.6-py3-none-any.whl (78 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m78.0/78.0 kB\u001b[0m \u001b[31m43.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading jiter-0.6.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (325 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m325.2/325.2 kB\u001b[0m \u001b[31m128.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading jsonschema-4.23.0-py3-none-any.whl (88 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m88.5/88.5 kB\u001b[0m \u001b[31m50.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading msgpack-1.1.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (378 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m378.0/378.0 kB\u001b[0m \u001b[31m138.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading opencv_python_headless-4.10.0.84-cp37-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (49.9 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m49.9/49.9 MB\u001b[0m \u001b[31m132.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading python_dotenv-1.0.1-py3-none-any.whl (19 kB)\n",
      "Downloading starlette-0.40.0-py3-none-any.whl (73 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m73.3/73.3 kB\u001b[0m \u001b[31m41.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading uvloop-0.21.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.8 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.8/3.8 MB\u001b[0m \u001b[31m248.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading watchfiles-0.24.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (425 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m425.7/425.7 kB\u001b[0m \u001b[31m149.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading websockets-13.1-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (164 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m164.1/164.1 kB\u001b[0m \u001b[31m85.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading cloudpickle-3.1.0-py3-none-any.whl (22 kB)\n",
      "Downloading diskcache-5.6.3-py3-none-any.whl (45 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m45.5/45.5 kB\u001b[0m \u001b[31m25.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading lark-1.2.2-py3-none-any.whl (111 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m111.0/111.0 kB\u001b[0m \u001b[31m60.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading numba-0.60.0-cp310-cp310-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (3.7 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.7/3.7 MB\u001b[0m \u001b[31m253.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading pyairports-2.1.1-py3-none-any.whl (371 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m371.7/371.7 kB\u001b[0m \u001b[31m138.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading pycountry-24.6.1-py3-none-any.whl (6.3 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.3/6.3 MB\u001b[0m \u001b[31m78.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0mm\n",
      "\u001b[?25hDownloading uvicorn-0.32.0-py3-none-any.whl (63 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m63.7/63.7 kB\u001b[0m \u001b[31m35.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading llvmlite-0.43.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (43.9 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m43.9/43.9 MB\u001b[0m \u001b[31m141.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_nvjitlink_cu12-12.6.77-py3-none-manylinux2014_x86_64.whl (19.7 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m19.7/19.7 MB\u001b[0m \u001b[31m223.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: sentencepiece, pyairports, py-cpuinfo, nvidia-ml-py, websockets, uvloop, python-dotenv, pycountry, protobuf, pillow, partial-json-parser, nvidia-nvtx-cu12, nvidia-nvjitlink-cu12, nvidia-nccl-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, numpy, msgspec, msgpack, llvmlite, lark, jiter, httptools, h11, filelock, einops, diskcache, cloudpickle, click, watchfiles, uvicorn, triton, tiktoken, starlette, opencv-python-headless, nvidia-cusparse-cu12, nvidia-cudnn-cu12, numba, httpcore, gguf, prometheus-fastapi-instrumentator, nvidia-cusolver-cu12, lm-format-enforcer, jsonschema, httpx, fastapi, torch, ray, openai, mistral-common, xformers, torchvision, compressed-tensors, outlines, vllm\n",
      "  Attempting uninstall: pillow\n",
      "    Found existing installation: Pillow 9.3.0\n",
      "    Uninstalling Pillow-9.3.0:\n",
      "      Successfully uninstalled Pillow-9.3.0\n",
      "  Attempting uninstall: numpy\n",
      "    Found existing installation: numpy 1.24.1\n",
      "    Uninstalling numpy-1.24.1:\n",
      "      Successfully uninstalled numpy-1.24.1\n",
      "  Attempting uninstall: filelock\n",
      "    Found existing installation: filelock 3.9.0\n",
      "    Uninstalling filelock-3.9.0:\n",
      "      Successfully uninstalled filelock-3.9.0\n",
      "  Attempting uninstall: triton\n",
      "    Found existing installation: triton 2.1.0\n",
      "    Uninstalling triton-2.1.0:\n",
      "      Successfully uninstalled triton-2.1.0\n",
      "  Attempting uninstall: lm-format-enforcer\n",
      "    Found existing installation: lm-format-enforcer 0.10.9\n",
      "    Uninstalling lm-format-enforcer-0.10.9:\n",
      "      Successfully uninstalled lm-format-enforcer-0.10.9\n",
      "  Attempting uninstall: jsonschema\n",
      "    Found existing installation: jsonschema 4.19.2\n",
      "    Uninstalling jsonschema-4.19.2:\n",
      "      Successfully uninstalled jsonschema-4.19.2\n",
      "  Attempting uninstall: torch\n",
      "    Found existing installation: torch 2.1.0+cu118\n",
      "    Uninstalling torch-2.1.0+cu118:\n",
      "      Successfully uninstalled torch-2.1.0+cu118\n",
      "  Attempting uninstall: torchvision\n",
      "    Found existing installation: torchvision 0.16.0+cu118\n",
      "    Uninstalling torchvision-0.16.0+cu118:\n",
      "      Successfully uninstalled torchvision-0.16.0+cu118\n",
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "torchaudio 2.1.0+cu118 requires torch==2.1.0, but you have torch 2.4.0 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0mSuccessfully installed click-8.1.7 cloudpickle-3.1.0 compressed-tensors-0.6.0 diskcache-5.6.3 einops-0.8.0 fastapi-0.115.2 filelock-3.16.1 gguf-0.10.0 h11-0.14.0 httpcore-1.0.6 httptools-0.6.4 httpx-0.27.2 jiter-0.6.1 jsonschema-4.23.0 lark-1.2.2 llvmlite-0.43.0 lm-format-enforcer-0.10.6 mistral-common-1.4.4 msgpack-1.1.0 msgspec-0.18.6 numba-0.60.0 numpy-1.26.4 nvidia-cublas-cu12-12.1.3.1 nvidia-cuda-cupti-cu12-12.1.105 nvidia-cuda-nvrtc-cu12-12.1.105 nvidia-cuda-runtime-cu12-12.1.105 nvidia-cudnn-cu12-9.1.0.70 nvidia-cufft-cu12-11.0.2.54 nvidia-curand-cu12-10.3.2.106 nvidia-cusolver-cu12-11.4.5.107 nvidia-cusparse-cu12-12.1.0.106 nvidia-ml-py-12.560.30 nvidia-nccl-cu12-2.20.5 nvidia-nvjitlink-cu12-12.6.77 nvidia-nvtx-cu12-12.1.105 openai-1.52.0 opencv-python-headless-4.10.0.84 outlines-0.0.46 partial-json-parser-0.2.1.1.post4 pillow-10.4.0 prometheus-fastapi-instrumentator-7.0.0 protobuf-5.28.2 py-cpuinfo-9.0.0 pyairports-2.1.1 pycountry-24.6.1 python-dotenv-1.0.1 ray-2.37.0 sentencepiece-0.2.0 starlette-0.40.0 tiktoken-0.7.0 torch-2.4.0 torchvision-0.19.0 triton-3.0.0 uvicorn-0.32.0 uvloop-0.21.0 vllm-0.6.3.post1 watchfiles-0.24.0 websockets-13.1 xformers-0.0.27.post2\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.3.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.2\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython -m pip install --upgrade pip\u001b[0m\n",
      "Requirement already satisfied: ray in /usr/local/lib/python3.10/dist-packages (2.37.0)\n",
      "Requirement already satisfied: click>=7.0 in /usr/local/lib/python3.10/dist-packages (from ray) (8.1.7)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from ray) (3.16.1)\n",
      "Requirement already satisfied: jsonschema in /usr/local/lib/python3.10/dist-packages (from ray) (4.23.0)\n",
      "Requirement already satisfied: msgpack<2.0.0,>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from ray) (1.1.0)\n",
      "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from ray) (23.2)\n",
      "Requirement already satisfied: protobuf!=3.19.5,>=3.15.3 in /usr/local/lib/python3.10/dist-packages (from ray) (5.28.2)\n",
      "Requirement already satisfied: pyyaml in /usr/local/lib/python3.10/dist-packages (from ray) (6.0.1)\n",
      "Requirement already satisfied: aiosignal in /usr/local/lib/python3.10/dist-packages (from ray) (1.3.1)\n",
      "Requirement already satisfied: frozenlist in /usr/local/lib/python3.10/dist-packages (from ray) (1.4.1)\n",
      "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from ray) (2.32.3)\n",
      "Requirement already satisfied: attrs>=22.2.0 in /usr/local/lib/python3.10/dist-packages (from jsonschema->ray) (23.1.0)\n",
      "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in /usr/local/lib/python3.10/dist-packages (from jsonschema->ray) (2023.7.1)\n",
      "Requirement already satisfied: referencing>=0.28.4 in /usr/local/lib/python3.10/dist-packages (from jsonschema->ray) (0.30.2)\n",
      "Requirement already satisfied: rpds-py>=0.7.1 in /usr/local/lib/python3.10/dist-packages (from jsonschema->ray) (0.12.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->ray) (2.1.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->ray) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->ray) (1.26.13)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->ray) (2022.12.7)\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.3.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.2\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython -m pip install --upgrade pip\u001b[0m\n",
      "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (23.2)\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.3.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.2\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython -m pip install --upgrade pip\u001b[0m\n",
      "Collecting typing\n",
      "  Downloading typing-3.7.4.3.tar.gz (78 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m78.6/78.6 kB\u001b[0m \u001b[31m2.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hBuilding wheels for collected packages: typing\n",
      "  Building wheel for typing (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for typing: filename=typing-3.7.4.3-py3-none-any.whl size=26308 sha256=ab3b7a51a918eed030ac526b4e9562e1ea6cd67574e350a0847662fa0d7c6618\n",
      "  Stored in directory: /root/.cache/pip/wheels/7c/d0/9e/1f26ebb66d9e1732e4098bc5a6c2d91f6c9a529838f0284890\n",
      "Successfully built typing\n",
      "Installing collected packages: typing\n",
      "Successfully installed typing-3.7.4.3\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.3.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.2\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython -m pip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install datasets\n",
    "!pip install transformers\n",
    "!pip install accelerate\n",
    "!pip install numpy, pandas\n",
    "!pip install matplotlib\n",
    "!pip install sentence_transformers\n",
    "!pip install huggingface_hub\n",
    "!pip install lm-format-enforcer\n",
    "\n",
    "# For vLLM\n",
    "!pip install vllm\n",
    "!pip install ray\n",
    "!pip install packaging\n",
    "!pip install typing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e1b38d26-7213-42c3-bacc-77709fa2bcd8",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from huggingface_hub import login\n",
    "login(token=\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3259f72d-e530-4e1d-b8f1-8097334c9da6",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# Sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2f6b55f9-f10b-49b8-af84-fea6bd0168de",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 10-18 06:51:51 config.py:185] gemma2 has interleaved attention, which is currently not supported by vLLM. Disabling sliding window and capping the max length to the sliding window size (4096).\n",
      "INFO 10-18 06:51:55 llm_engine.py:237] Initializing an LLM engine (v0.6.3.post1) with config: model='google/gemma-2-9b-it', speculative_config=None, tokenizer='google/gemma-2-9b-it', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, rope_scaling=None, rope_theta=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=4096, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto, quantization_param_path=None, device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='outlines'), observability_config=ObservabilityConfig(otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=0, served_model_name=google/gemma-2-9b-it, num_scheduler_steps=1, chunked_prefill_enabled=False multi_step_stream_outputs=True, enable_prefix_caching=False, use_async_output_proc=True, use_cached_outputs=False, mm_processor_kwargs=None)\n",
      "INFO 10-18 06:51:57 model_runner.py:1056] Starting to load model google/gemma-2-9b-it...\n",
      "INFO 10-18 06:51:57 weight_utils.py:243] Using model weights format ['*.safetensors']\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c0c7875b6cb24d26b1cc73d56407c2d2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading safetensors checkpoint shards:   0% Completed | 0/4 [00:00<?, ?it/s]\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 10-18 06:52:01 model_runner.py:1067] Loading model weights took 17.2179 GB\n",
      "INFO 10-18 06:52:05 gpu_executor.py:122] # GPU blocks: 3861, # CPU blocks: 780\n",
      "INFO 10-18 06:52:05 gpu_executor.py:126] Maximum concurrency for 4096 tokens per request: 15.08x\n",
      "INFO 10-18 06:52:10 model_runner.py:1395] Capturing the model for CUDA graphs. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI.\n",
      "INFO 10-18 06:52:10 model_runner.py:1399] CUDA graphs can take additional 1~3 GiB memory per GPU. If you are running out of memory, consider decreasing `gpu_memory_utilization` or enforcing eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.\n",
      "INFO 10-18 06:52:35 model_runner.py:1523] Graph capturing finished in 25 secs.\n",
      "\n",
      "request 0 started\n",
      "INFO 10-18 06:52:35 async_llm_engine.py:207] Added request 0.\n",
      "\n",
      "request 1 started\n",
      "INFO 10-18 06:52:35 async_llm_engine.py:207] Added request 1.\n",
      "INFO 10-18 06:52:40 metrics.py:349] Avg prompt throughput: 2.2 tokens/s, Avg generation throughput: 50.2 tokens/s, Running: 2 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.5%, CPU KV cache usage: 0.0%.\n",
      "INFO 10-18 06:52:45 metrics.py:349] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 50.3 tokens/s, Running: 2 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.9%, CPU KV cache usage: 0.0%.\n",
      "INFO 10-18 06:52:49 async_llm_engine.py:175] Finished request 1.\n",
      "\n",
      "request_id: 1\n",
      "calculation time: 13.774735689163208\n",
      "text_outputs: ['What is Japan?\\n\\nJapan is an island nation located in East Asia. \\n\\nHere are some key facts about Japan:\\n\\n* **Geography:** Japan is an archipelago of over 6,800 islands, with the four largest being Honshu, Hokkaido, Kyushu, and Shikoku. It is situated in the Pacific Ocean, east of the Asian mainland.\\n* **Culture:** Japan has a rich and unique culture, known for its traditions, art, cuisine, and technology. Some notable aspects include:\\n    * **Shintoism and Buddhism:** The two major religions in Japan.\\n    * **Tea ceremony:** A traditional ritual involving the preparation and serving of matcha tea.\\n    * **Calligraphy and painting:** Highly valued art forms with a long history.\\n    * **Anime and manga:** Popular forms of Japanese animation and comics.\\n* **Economy:** Japan is a highly developed and industrialized nation with a strong economy. It is a major exporter of automobiles, electronics, and other manufactured goods.\\n* **Government:** Japan is a constitutional monarchy with a parliamentary system of government. The Emperor is the head of state, while the Prime Minister is the head of government.\\n* **Population:** Japan has a population of over 125 million people, with a high life expectancy and a relatively low birth rate.\\n\\n**Interesting Facts:**\\n\\n* Japan is home to Mount Fuji, the highest mountain in Japan and a popular tourist destination.\\n* The Japanese writing system uses a combination of three scripts: kanji (Chinese characters), hiragana, and katakana.\\n* Japan has a unique tradition of onsen, natural hot springs that are believed to have health benefits.\\n\\n\\n']\n",
      "INFO 10-18 06:52:50 metrics.py:349] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 43.9 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.6%, CPU KV cache usage: 0.0%.\n",
      "INFO 10-18 06:52:54 async_llm_engine.py:175] Finished request 0.\n",
      "\n",
      "request_id: 0\n",
      "calculation time: 19.308053731918335\n",
      "text_outputs: [\"What is LLM?\\n\\nLLM stands for **Large Language Model**. \\n\\nIt's a type of artificial intelligence (AI) that excels at understanding and generating human-like text. \\n\\nHere's a breakdown:\\n\\n* **Large:** LLMs are trained on massive datasets of text and code, often containing billions or even trillions of words. This vast amount of data allows them to learn complex patterns and relationships within language.\\n* **Language:** LLMs are specifically designed to work with language. They can read, understand, interpret, and generate text in various formats, including articles, stories, poems, code, and more.\\n* **Model:** An LLM is a mathematical model that represents knowledge about language. It uses this knowledge to perform tasks such as:\\n\\n    * **Text Generation:** Writing creative content, summarizing text, translating languages, and more.\\n    * **Text Comprehension:** Answering questions, identifying sentiment, and extracting information from text.\\n    * **Code Generation:** Writing and debugging code in different programming languages.\\n    * **Dialogue Systems:** Engaging in natural-sounding conversations with humans.\\n\\n**Examples of LLMs:**\\n\\n* GPT-3 (Generative Pre-trained Transformer 3) by OpenAI\\n* LaMDA (Language Model for Dialogue Applications) by Google\\n* BERT (Bidirectional Encoder Representations from Transformers) by Google\\n\\n**Key Features of LLMs:**\\n\\n* **Contextual Understanding:** LLMs can understand the context of a conversation or text passage, allowing them to generate more relevant and coherent responses.\\n* **Generative Capabilities:** They can create new text that is grammatically correct and stylistically appropriate.\\n* **Adaptability:** LLMs can be fine-tuned for specific tasks or domains, improving their performance in those areas.\\n\\n**Applications of LLMs:**\\n\\nLLMs have a wide range of applications, including:\\n\\n* **Chatbots and Virtual Assistants:** Providing automated customer service and support.\\n* **Content Creation:** Generating articles, blog posts, marketing copy, and other types of content.\\n* **Education:** Assisting with homework, providing personalized learning experiences, and automating grading.\\n* **Research:** Analyzing large amounts of text data to uncover insights and trends.\\n* **Healthcare:** Summarizing patient records, assisting with diagnosis, and providing medical information.\\n\\n\\n\\nLet me know if you have any other questions about LLMs!\\n\"]\n",
      "\n",
      "\n",
      "---------\n",
      "answer1:  [\"What is LLM?\\n\\nLLM stands for **Large Language Model**. \\n\\nIt's a type of artificial intelligence (AI) that excels at understanding and generating human-like text. \\n\\nHere's a breakdown:\\n\\n* **Large:** LLMs are trained on massive datasets of text and code, often containing billions or even trillions of words. This vast amount of data allows them to learn complex patterns and relationships within language.\\n* **Language:** LLMs are specifically designed to work with language. They can read, understand, interpret, and generate text in various formats, including articles, stories, poems, code, and more.\\n* **Model:** An LLM is a mathematical model that represents knowledge about language. It uses this knowledge to perform tasks such as:\\n\\n    * **Text Generation:** Writing creative content, summarizing text, translating languages, and more.\\n    * **Text Comprehension:** Answering questions, identifying sentiment, and extracting information from text.\\n    * **Code Generation:** Writing and debugging code in different programming languages.\\n    * **Dialogue Systems:** Engaging in natural-sounding conversations with humans.\\n\\n**Examples of LLMs:**\\n\\n* GPT-3 (Generative Pre-trained Transformer 3) by OpenAI\\n* LaMDA (Language Model for Dialogue Applications) by Google\\n* BERT (Bidirectional Encoder Representations from Transformers) by Google\\n\\n**Key Features of LLMs:**\\n\\n* **Contextual Understanding:** LLMs can understand the context of a conversation or text passage, allowing them to generate more relevant and coherent responses.\\n* **Generative Capabilities:** They can create new text that is grammatically correct and stylistically appropriate.\\n* **Adaptability:** LLMs can be fine-tuned for specific tasks or domains, improving their performance in those areas.\\n\\n**Applications of LLMs:**\\n\\nLLMs have a wide range of applications, including:\\n\\n* **Chatbots and Virtual Assistants:** Providing automated customer service and support.\\n* **Content Creation:** Generating articles, blog posts, marketing copy, and other types of content.\\n* **Education:** Assisting with homework, providing personalized learning experiences, and automating grading.\\n* **Research:** Analyzing large amounts of text data to uncover insights and trends.\\n* **Healthcare:** Summarizing patient records, assisting with diagnosis, and providing medical information.\\n\\n\\n\\nLet me know if you have any other questions about LLMs!\\n\"]\n",
      "answer2:  ['What is Japan?\\n\\nJapan is an island nation located in East Asia. \\n\\nHere are some key facts about Japan:\\n\\n* **Geography:** Japan is an archipelago of over 6,800 islands, with the four largest being Honshu, Hokkaido, Kyushu, and Shikoku. It is situated in the Pacific Ocean, east of the Asian mainland.\\n* **Culture:** Japan has a rich and unique culture, known for its traditions, art, cuisine, and technology. Some notable aspects include:\\n    * **Shintoism and Buddhism:** The two major religions in Japan.\\n    * **Tea ceremony:** A traditional ritual involving the preparation and serving of matcha tea.\\n    * **Calligraphy and painting:** Highly valued art forms with a long history.\\n    * **Anime and manga:** Popular forms of Japanese animation and comics.\\n* **Economy:** Japan is a highly developed and industrialized nation with a strong economy. It is a major exporter of automobiles, electronics, and other manufactured goods.\\n* **Government:** Japan is a constitutional monarchy with a parliamentary system of government. The Emperor is the head of state, while the Prime Minister is the head of government.\\n* **Population:** Japan has a population of over 125 million people, with a high life expectancy and a relatively low birth rate.\\n\\n**Interesting Facts:**\\n\\n* Japan is home to Mount Fuji, the highest mountain in Japan and a popular tourist destination.\\n* The Japanese writing system uses a combination of three scripts: kanji (Chinese characters), hiragana, and katakana.\\n* Japan has a unique tradition of onsen, natural hot springs that are believed to have health benefits.\\n\\n\\n']\n"
     ]
    }
   ],
   "source": [
    "# Succeed Code\n",
    "\n",
    "# Please refer to entrypoints/api_server.py for\n",
    "# the complete example.\n",
    "import asyncio\n",
    "from vllm import AsyncLLMEngine, AsyncEngineArgs, SamplingParams\n",
    "import time\n",
    "\n",
    "engine_args = AsyncEngineArgs(\n",
    "    model=\"google/gemma-2-9b-it\",\n",
    ")\n",
    "\n",
    "# initialize the engine and the example input\n",
    "engine = AsyncLLMEngine.from_engine_args(engine_args)\n",
    "\n",
    "async def add_request(prompt, request_id, start):\n",
    "    print()\n",
    "    print(f\"request {request_id} started\")\n",
    "    \n",
    "    # start the generation\n",
    "    final_output = None\n",
    "    results_generator = engine.generate(prompt, SamplingParams(temperature=0.0, max_tokens=1000), request_id)\n",
    "    async for request_output in results_generator:\n",
    "        final_output = request_output\n",
    "\n",
    "    text_outputs = [prompt + output.text for output in final_output.outputs]\n",
    "\n",
    "    print()\n",
    "    print(f\"request_id: {request_id}\")\n",
    "    print(f\"calculation time: {time.time()-start}\")\n",
    "    print(f\"text_outputs: {text_outputs}\")\n",
    "    \n",
    "    return text_outputs\n",
    "    \n",
    "start = time.time()\n",
    "task1 = asyncio.create_task(add_request(\"What is LLM?\", 0, start))\n",
    "task2 = asyncio.create_task(add_request(\"What is Japan?\", 1, start))\n",
    "\n",
    "answer1 = await task1\n",
    "answer2 = await task2\n",
    "\n",
    "print()\n",
    "print()\n",
    "print(\"---------\")\n",
    "print(\"answer1: \", answer1)\n",
    "print(\"answer2: \", answer2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "124f1eb0-9a39-4faf-8110-d1cff39a69bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import asyncio\n",
    "from vllm import AsyncLLMEngine, AsyncEngineArgs, SamplingParams\n",
    "import time\n",
    "\n",
    "engine_args = AsyncEngineArgs(\n",
    "    model=\"google/gemma-2-9b-it\",\n",
    ")\n",
    "\n",
    "# initialize the engine and the example input\n",
    "engine = AsyncLLMEngine.from_engine_args(engine_args)\n",
    "start = time.time()\n",
    "\n",
    "async def add_request(prompt, request_id, start):\n",
    "    print()\n",
    "    print(f\"request {request_id} started\")\n",
    "    \n",
    "    # start the generation\n",
    "    final_output = None\n",
    "    results_generator = engine.generate(prompt, SamplingParams(temperature=0.0, max_tokens=1000), request_id)\n",
    "    async for request_output in results_generator:\n",
    "        # print(request_output) => for streaming\n",
    "        final_output = request_output\n",
    "\n",
    "    text_outputs = [prompt + output.text for output in final_output.outputs]\n",
    "\n",
    "    print()\n",
    "    print(f\"request_id: {request_id}\")\n",
    "    print(f\"calculation time: {time.time()-start}\")\n",
    "    #print(f\"text_outputs: {text_outputs}\")\n",
    "\n",
    "    if request_id == 0:\n",
    "        \n",
    "        answer2 = await add_request(f\"Give me some keywords of the following sentence '''{final_output.outputs[0].text}'''\", 1, start)\n",
    "\n",
    "        text_outputs += answer2\n",
    "    \n",
    "    return text_outputs\n",
    "\n",
    "\n",
    "async def get_answer():\n",
    "    #task1 = asyncio.create_task(add_request(\"What is LLM?\", 0, start))\n",
    "    #answer1 = await task1\n",
    "\n",
    "    answer1 = await add_request(\"What is LLM?\", 0, start)\n",
    "\n",
    "    print()\n",
    "    print()\n",
    "    print(\"======\")\n",
    "    print(answer1)\n",
    "\n",
    "\n",
    "await get_answer()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d03d461b-0788-400d-90cc-753969e2f486",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 10-20 05:46:26 config.py:185] gemma2 has interleaved attention, which is currently not supported by vLLM. Disabling sliding window and capping the max length to the sliding window size (4096).\n",
      "INFO 10-20 05:46:30 llm_engine.py:237] Initializing an LLM engine (v0.6.3.post1) with config: model='google/gemma-2-9b-it', speculative_config=None, tokenizer='google/gemma-2-9b-it', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, rope_scaling=None, rope_theta=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=4096, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto, quantization_param_path=None, device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='outlines'), observability_config=ObservabilityConfig(otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=0, served_model_name=google/gemma-2-9b-it, num_scheduler_steps=1, chunked_prefill_enabled=False multi_step_stream_outputs=True, enable_prefix_caching=False, use_async_output_proc=True, use_cached_outputs=False, mm_processor_kwargs=None)\n",
      "INFO 10-20 05:46:31 model_runner.py:1056] Starting to load model google/gemma-2-9b-it...\n",
      "INFO 10-20 05:46:32 weight_utils.py:243] Using model weights format ['*.safetensors']\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f978be12ab064070a631477100482979",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading safetensors checkpoint shards:   0% Completed | 0/4 [00:00<?, ?it/s]\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 10-20 05:46:36 model_runner.py:1067] Loading model weights took 17.2179 GB\n",
      "INFO 10-20 05:46:40 gpu_executor.py:122] # GPU blocks: 3859, # CPU blocks: 780\n",
      "INFO 10-20 05:46:40 gpu_executor.py:126] Maximum concurrency for 4096 tokens per request: 15.07x\n",
      "INFO 10-20 05:46:44 model_runner.py:1395] Capturing the model for CUDA graphs. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI.\n",
      "INFO 10-20 05:46:44 model_runner.py:1399] CUDA graphs can take additional 1~3 GiB memory per GPU. If you are running out of memory, consider decreasing `gpu_memory_utilization` or enforcing eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.\n",
      "INFO 10-20 05:47:00 model_runner.py:1523] Graph capturing finished in 16 secs.\n",
      "\n",
      "request 0 started\n",
      "INFO 10-20 05:47:00 async_llm_engine.py:207] Added request 0.\n",
      "\n",
      "request 1 started\n",
      "INFO 10-20 05:47:00 async_llm_engine.py:207] Added request 1.\n",
      "\n",
      "request 2 started\n",
      "INFO 10-20 05:47:00 async_llm_engine.py:207] Added request 2.\n",
      "INFO 10-20 05:47:05 metrics.py:349] Avg prompt throughput: 2.6 tokens/s, Avg generation throughput: 75.4 tokens/s, Running: 3 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.7%, CPU KV cache usage: 0.0%.\n",
      "INFO 10-20 05:47:10 metrics.py:349] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 75.2 tokens/s, Running: 3 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 1.3%, CPU KV cache usage: 0.0%.\n",
      "INFO 10-20 05:47:15 metrics.py:349] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 74.8 tokens/s, Running: 3 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 1.9%, CPU KV cache usage: 0.0%.\n",
      "INFO 10-20 05:47:15 async_llm_engine.py:175] Finished request 1.\n",
      "\n",
      "request_id: 1 ended\n",
      "calculation time: 15.57522702217102\n",
      "INFO 10-20 05:47:18 async_llm_engine.py:175] Finished request 2.\n",
      "\n",
      "request_id: 2 ended\n",
      "calculation time: 18.36235284805298\n",
      "INFO 10-20 05:47:20 async_llm_engine.py:175] Finished request 0.\n",
      "\n",
      "request_id: 0 ended\n",
      "calculation time: 19.633436679840088\n",
      "\n",
      "\n",
      "=== result ===\n",
      "[\"?\\n\\nLLM stands for **Large Language Model**. \\n\\nIt's a type of artificial intelligence (AI) that excels at understanding and generating human-like text. \\n\\nHere's a breakdown:\\n\\n* **Large:** LLMs are trained on massive datasets of text and code, often containing billions or even trillions of words. This vast amount of data allows them to learn complex patterns and relationships within language.\\n* **Language:** LLMs are specifically designed to work with language. They can read, understand, interpret, and generate text in various formats, including articles, stories, poems, code, and more.\\n* **Model:** An LLM is a mathematical model that represents knowledge about language. It uses this knowledge to perform tasks such as:\\n\\n    * **Text Generation:** Writing creative content, summarizing text, translating languages, and more.\\n    * **Text Comprehension:** Answering questions, identifying sentiment, and extracting information from text.\\n    * **Code Generation:** Writing and debugging code in different programming languages.\\n    * **Dialogue Systems:** Engaging in natural-sounding conversations with humans.\\n\\n**Examples of LLMs:**\\n\\n* GPT-3 (Generative Pre-trained Transformer 3) by OpenAI\\n* LaMDA (Language Model for Dialogue Applications) by Google\\n* BERT (Bidirectional Encoder Representations from Transformers) by Google\\n\\n**Key Features of LLMs:**\\n\\n* **Contextual Understanding:** LLMs can understand the context of a conversation or text passage, allowing them to generate more relevant and coherent responses.\\n* **Generative Capabilities:** They can create new text that is grammatically correct and stylistically appropriate.\\n* **Adaptability:** LLMs can be fine-tuned for specific tasks or domains, improving their performance in those areas.\\n\\n**Applications of LLMs:**\\n\\nLLMs have a wide range of applications, including:\\n\\n* **Chatbots and Virtual Assistants:** Providing automated customer service and support.\\n* **Content Creation:** Generating articles, blog posts, marketing copy, and other types of content.\\n* **Education:** Assisting with homework, providing personalized learning experiences, and automating grading.\\n* **Research:** Analyzing large amounts of text data to uncover insights and trends.\\n* **Healthcare:** Summarizing patient records, assisting with diagnosis, and providing medical information.\\n\\n\\n\\nLet me know if you have any other questions about LLMs!\\n\", \"'s stance on the South China Sea dispute?\\n\\nJapan's stance on the South China Sea dispute is complex and multifaceted. While Japan does not have direct territorial claims in the South China Sea, it has a strong interest in maintaining freedom of navigation and overflight in the region, which is vital for its trade and security.\\n\\nHere are some key points of Japan's stance:\\n\\n* **Support for International Law:** Japan strongly supports the United Nations Convention on the Law of the Sea (UNCLOS) and calls for all parties involved in the dispute to resolve their differences peacefully in accordance with international law.\\n* **Freedom of Navigation:** Japan emphasizes the importance of maintaining freedom of navigation and overflight in the South China Sea, which is a crucial international waterway for global trade and security.\\n* **Concerns over China's Assertiveness:** Japan has expressed concerns over China's increasingly assertive actions in the South China Sea, including its construction of artificial islands and its deployment of military assets.\\n* **Cooperation with Regional Partners:** Japan has been working closely with its regional partners, such as the United States, Australia, and ASEAN countries, to promote a peaceful and rules-based order in the South China Sea.\\n* **Economic Interests:** Japan has significant economic interests in the South China Sea, as it relies on the region for energy imports and trade.\\n\\n**Japan's stance can be seen as a balancing act:**\\n\\n* **Maintaining good relations with China:** Japan recognizes the importance of maintaining a stable relationship with China, its largest trading partner.\\n* **Promoting its own security interests:** Japan is also concerned about China's growing military power and its potential threat to regional security.\\n\\n**Overall, Japan's stance on the South China Sea dispute is one of cautious engagement. It seeks to promote a peaceful resolution of the dispute while also safeguarding its own interests and those of the international community.**\\n\", ' Protocol?\\n\\nThe Kyoto Protocol is an international treaty that extends the 1992 United Nations Framework Convention on Climate Change (UNFCCC) that commits industrialized countries to reduce greenhouse gas emissions.\\n\\n**Key Features:**\\n\\n* **Emission Reduction Targets:** Developed countries agreed to legally binding emission reduction targets for the period 2008-2012, averaging 5.2% below 1990 levels.\\n* **Flexible Mechanisms:** The Protocol introduced flexible mechanisms to help countries meet their targets, including:\\n    * **Emissions Trading:** Allows countries to buy and sell emission allowances.\\n    * **Joint Implementation:** Enables developed countries to invest in emission reduction projects in other developed countries.\\n    * **Clean Development Mechanism (CDM):** Allows developed countries to invest in emission reduction projects in developing countries.\\n* **Differentiated Responsibilities:** The Protocol recognizes that developed countries have a greater historical responsibility for climate change and therefore bear a greater burden in reducing emissions.\\n* **Monitoring and Reporting:** Countries are required to monitor and report their greenhouse gas emissions and progress towards meeting their targets.\\n\\n**Status:**\\n\\n* The Kyoto Protocol entered into force in 2005.\\n* While it has been ratified by 192 countries, the United States, a major emitter, never ratified the Protocol.\\n* The first commitment period (2008-2012) ended in 2012.\\n* A second commitment period (2013-2020) was agreed upon, but participation was limited.\\n\\n**Significance:**\\n\\n* The Kyoto Protocol was a landmark agreement that established a framework for international cooperation on climate change.\\n* It helped raise awareness of the issue and spurred the development of clean technologies.\\n* However, its effectiveness in reducing global emissions has been limited due to factors such as the absence of major emitters like the United States and the relatively weak emission reduction targets.\\n\\n**Successor:**\\n\\nThe Paris Agreement, adopted in 2015, is the successor to the Kyoto Protocol. It aims to limit global warming to well below 2 degrees Celsius, preferably to 1.5 degrees Celsius, compared to pre-industrial levels.\\n\\n\\n']\n"
     ]
    }
   ],
   "source": [
    "import asyncio\n",
    "from vllm import AsyncLLMEngine, AsyncEngineArgs, SamplingParams\n",
    "import time\n",
    "\n",
    "engine_args = AsyncEngineArgs(\n",
    "    model=\"google/gemma-2-9b-it\",\n",
    ")\n",
    "\n",
    "# initialize the engine and the example input\n",
    "engine = AsyncLLMEngine.from_engine_args(engine_args)\n",
    "start = time.time()\n",
    "\n",
    "async def add_request(prompt, request_id, start):\n",
    "    print()\n",
    "    print(f\"request {request_id} started\")\n",
    "    \n",
    "    # start the generation\n",
    "    final_output = None\n",
    "    results_generator = engine.generate(prompt, SamplingParams(temperature=0.0, max_tokens=1000), request_id)\n",
    "    async for request_output in results_generator:\n",
    "        # print(request_output) => for streaming\n",
    "        final_output = request_output\n",
    "\n",
    "    text_outputs = [prompt + output.text for output in final_output.outputs]\n",
    "\n",
    "    print()\n",
    "    print(f\"request_id: {request_id} ended\")\n",
    "    print(f\"calculation time: {time.time()-start}\")\n",
    "    \n",
    "    return final_output.outputs[0].text\n",
    "\n",
    "\n",
    "async def get_answer():\n",
    "    whats = [\"LLM\", \"Japan\", \"Kyoto\"]\n",
    "    \n",
    "    result = await asyncio.gather(\n",
    "        *[add_request(f\"What is {whats[i]}\", i, start) for i in range(len(whats))]\n",
    "    )\n",
    "\n",
    "    print()\n",
    "    print()\n",
    "    print(\"=== result ===\")\n",
    "    print(result)\n",
    "\n",
    "\n",
    "await get_answer()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3a265a9-7f4c-47c7-ae9b-bf48b18033fd",
   "metadata": {},
   "source": [
    "# Tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "af38f4ce-c959-4dfb-8f70-71c6c51a8831",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "LLMEngine.__init__() missing 12 required positional arguments: 'cache_config', 'parallel_config', 'scheduler_config', 'device_config', 'load_config', 'lora_config', 'speculative_config', 'decoding_config', 'observability_config', 'prompt_adapter_config', 'executor_class', and 'log_stats'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[7], line 4\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mvllm\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m LLMEngine\n\u001b[1;32m      3\u001b[0m \u001b[38;5;66;03m# initialize engine\u001b[39;00m\n\u001b[0;32m----> 4\u001b[0m engine \u001b[38;5;241m=\u001b[39m \u001b[43mLLMEngine\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_config\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mgoogle/gemma-2-9b-it\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;66;03m# set request arguments\u001b[39;00m\n\u001b[1;32m      6\u001b[0m example_prompt \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mWho is the president of the United States?\u001b[39m\u001b[38;5;124m\"\u001b[39m\n",
      "\u001b[0;31mTypeError\u001b[0m: LLMEngine.__init__() missing 12 required positional arguments: 'cache_config', 'parallel_config', 'scheduler_config', 'device_config', 'load_config', 'lora_config', 'speculative_config', 'decoding_config', 'observability_config', 'prompt_adapter_config', 'executor_class', and 'log_stats'"
     ]
    }
   ],
   "source": [
    "from vllm import LLMEngine\n",
    "\n",
    "# initialize engine\n",
    "engine = LLMEngine(model_config = \"google/gemma-2-9b-it\")\n",
    "# set request arguments\n",
    "example_prompt = \"Who is the president of the United States?\"\n",
    "sampling_params = SamplingParams(temperature=0.0)\n",
    "request_id = 0\n",
    "\n",
    "# add the request to the engine\n",
    "engine.add_request(\n",
    "    str(request_id),\n",
    "    example_prompt,\n",
    "    SamplingParams(temperature=0.0))\n",
    "    # continue the request processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f71f5745-0582-4309-84f8-4c7b0a4860e8",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "usage: ipykernel_launcher.py [-h] [--model MODEL] [--tokenizer TOKENIZER]\n",
      "                             [--skip-tokenizer-init] [--revision REVISION]\n",
      "                             [--code-revision CODE_REVISION]\n",
      "                             [--tokenizer-revision TOKENIZER_REVISION]\n",
      "                             [--tokenizer-mode {auto,slow,mistral}]\n",
      "                             [--trust-remote-code]\n",
      "                             [--download-dir DOWNLOAD_DIR]\n",
      "                             [--load-format {auto,pt,safetensors,npcache,dummy,tensorizer,sharded_state,gguf,bitsandbytes,mistral}]\n",
      "                             [--config-format {auto,hf,mistral}]\n",
      "                             [--dtype {auto,half,float16,bfloat16,float,float32}]\n",
      "                             [--kv-cache-dtype {auto,fp8,fp8_e5m2,fp8_e4m3}]\n",
      "                             [--quantization-param-path QUANTIZATION_PARAM_PATH]\n",
      "                             [--max-model-len MAX_MODEL_LEN]\n",
      "                             [--guided-decoding-backend {outlines,lm-format-enforcer}]\n",
      "                             [--distributed-executor-backend {ray,mp}]\n",
      "                             [--worker-use-ray]\n",
      "                             [--pipeline-parallel-size PIPELINE_PARALLEL_SIZE]\n",
      "                             [--tensor-parallel-size TENSOR_PARALLEL_SIZE]\n",
      "                             [--max-parallel-loading-workers MAX_PARALLEL_LOADING_WORKERS]\n",
      "                             [--ray-workers-use-nsight]\n",
      "                             [--block-size {8,16,32}]\n",
      "                             [--enable-prefix-caching]\n",
      "                             [--disable-sliding-window]\n",
      "                             [--use-v2-block-manager]\n",
      "                             [--num-lookahead-slots NUM_LOOKAHEAD_SLOTS]\n",
      "                             [--seed SEED] [--swap-space SWAP_SPACE]\n",
      "                             [--cpu-offload-gb CPU_OFFLOAD_GB]\n",
      "                             [--gpu-memory-utilization GPU_MEMORY_UTILIZATION]\n",
      "                             [--num-gpu-blocks-override NUM_GPU_BLOCKS_OVERRIDE]\n",
      "                             [--max-num-batched-tokens MAX_NUM_BATCHED_TOKENS]\n",
      "                             [--max-num-seqs MAX_NUM_SEQS]\n",
      "                             [--max-logprobs MAX_LOGPROBS]\n",
      "                             [--disable-log-stats]\n",
      "                             [--quantization {aqlm,awq,deepspeedfp,tpu_int8,fp8,fbgemm_fp8,modelopt,marlin,gguf,gptq_marlin_24,gptq_marlin,awq_marlin,gptq,compressed-tensors,bitsandbytes,qqq,experts_int8,neuron_quant,ipex,None}]\n",
      "                             [--rope-scaling ROPE_SCALING]\n",
      "                             [--rope-theta ROPE_THETA] [--enforce-eager]\n",
      "                             [--max-context-len-to-capture MAX_CONTEXT_LEN_TO_CAPTURE]\n",
      "                             [--max-seq-len-to-capture MAX_SEQ_LEN_TO_CAPTURE]\n",
      "                             [--disable-custom-all-reduce]\n",
      "                             [--tokenizer-pool-size TOKENIZER_POOL_SIZE]\n",
      "                             [--tokenizer-pool-type TOKENIZER_POOL_TYPE]\n",
      "                             [--tokenizer-pool-extra-config TOKENIZER_POOL_EXTRA_CONFIG]\n",
      "                             [--limit-mm-per-prompt LIMIT_MM_PER_PROMPT]\n",
      "                             [--mm-processor-kwargs MM_PROCESSOR_KWARGS]\n",
      "                             [--enable-lora] [--max-loras MAX_LORAS]\n",
      "                             [--max-lora-rank MAX_LORA_RANK]\n",
      "                             [--lora-extra-vocab-size LORA_EXTRA_VOCAB_SIZE]\n",
      "                             [--lora-dtype {auto,float16,bfloat16,float32}]\n",
      "                             [--long-lora-scaling-factors LONG_LORA_SCALING_FACTORS]\n",
      "                             [--max-cpu-loras MAX_CPU_LORAS]\n",
      "                             [--fully-sharded-loras] [--enable-prompt-adapter]\n",
      "                             [--max-prompt-adapters MAX_PROMPT_ADAPTERS]\n",
      "                             [--max-prompt-adapter-token MAX_PROMPT_ADAPTER_TOKEN]\n",
      "                             [--device {auto,cuda,neuron,cpu,openvino,tpu,xpu}]\n",
      "                             [--num-scheduler-steps NUM_SCHEDULER_STEPS]\n",
      "                             [--multi-step-stream-outputs [MULTI_STEP_STREAM_OUTPUTS]]\n",
      "                             [--scheduler-delay-factor SCHEDULER_DELAY_FACTOR]\n",
      "                             [--enable-chunked-prefill [ENABLE_CHUNKED_PREFILL]]\n",
      "                             [--speculative-model SPECULATIVE_MODEL]\n",
      "                             [--speculative-model-quantization {aqlm,awq,deepspeedfp,tpu_int8,fp8,fbgemm_fp8,modelopt,marlin,gguf,gptq_marlin_24,gptq_marlin,awq_marlin,gptq,compressed-tensors,bitsandbytes,qqq,experts_int8,neuron_quant,ipex,None}]\n",
      "                             [--num-speculative-tokens NUM_SPECULATIVE_TOKENS]\n",
      "                             [--speculative-disable-mqa-scorer]\n",
      "                             [--speculative-draft-tensor-parallel-size SPECULATIVE_DRAFT_TENSOR_PARALLEL_SIZE]\n",
      "                             [--speculative-max-model-len SPECULATIVE_MAX_MODEL_LEN]\n",
      "                             [--speculative-disable-by-batch-size SPECULATIVE_DISABLE_BY_BATCH_SIZE]\n",
      "                             [--ngram-prompt-lookup-max NGRAM_PROMPT_LOOKUP_MAX]\n",
      "                             [--ngram-prompt-lookup-min NGRAM_PROMPT_LOOKUP_MIN]\n",
      "                             [--spec-decoding-acceptance-method {rejection_sampler,typical_acceptance_sampler}]\n",
      "                             [--typical-acceptance-sampler-posterior-threshold TYPICAL_ACCEPTANCE_SAMPLER_POSTERIOR_THRESHOLD]\n",
      "                             [--typical-acceptance-sampler-posterior-alpha TYPICAL_ACCEPTANCE_SAMPLER_POSTERIOR_ALPHA]\n",
      "                             [--disable-logprobs-during-spec-decoding [DISABLE_LOGPROBS_DURING_SPEC_DECODING]]\n",
      "                             [--model-loader-extra-config MODEL_LOADER_EXTRA_CONFIG]\n",
      "                             [--ignore-patterns IGNORE_PATTERNS]\n",
      "                             [--preemption-mode PREEMPTION_MODE]\n",
      "                             [--served-model-name SERVED_MODEL_NAME [SERVED_MODEL_NAME ...]]\n",
      "                             [--qlora-adapter-name-or-path QLORA_ADAPTER_NAME_OR_PATH]\n",
      "                             [--otlp-traces-endpoint OTLP_TRACES_ENDPOINT]\n",
      "                             [--collect-detailed-traces COLLECT_DETAILED_TRACES]\n",
      "                             [--disable-async-output-proc]\n",
      "                             [--override-neuron-config OVERRIDE_NEURON_CONFIG]\n",
      "                             [--scheduling-policy {fcfs,priority}]\n",
      "ipykernel_launcher.py: error: unrecognized arguments: -f /root/.local/share/jupyter/runtime/kernel-fba81b9c-2542-4c35-b594-6411021a84b9.json\n"
     ]
    },
    {
     "ename": "SystemExit",
     "evalue": "2",
     "output_type": "error",
     "traceback": [
      "An exception has occurred, use %tb to see the full traceback.\n",
      "\u001b[0;31mSystemExit\u001b[0m\u001b[0;31m:\u001b[0m 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/IPython/core/interactiveshell.py:3556: UserWarning: To exit: use 'exit', 'quit', or Ctrl-D.\n",
      "  warn(\"To exit: use 'exit', 'quit', or Ctrl-D.\", stacklevel=1)\n"
     ]
    }
   ],
   "source": [
    "import argparse\n",
    "from typing import List, Tuple\n",
    "\n",
    "from vllm import EngineArgs, LLMEngine, RequestOutput, SamplingParams\n",
    "from vllm.utils import FlexibleArgumentParser\n",
    "\n",
    "\n",
    "def create_test_prompts() -> List[Tuple[str, SamplingParams]]:\n",
    "    \"\"\"Create a list of test prompts with their sampling parameters.\"\"\"\n",
    "    return [\n",
    "        (\"A robot may not injure a human being\",\n",
    "         SamplingParams(temperature=0.0, logprobs=1, prompt_logprobs=1)),\n",
    "        (\"To be or not to be,\",\n",
    "         SamplingParams(temperature=0.8, top_k=5, presence_penalty=0.2)),\n",
    "        (\"What is the meaning of life?\",\n",
    "         SamplingParams(n=2,\n",
    "                        best_of=5,\n",
    "                        temperature=0.8,\n",
    "                        top_p=0.95,\n",
    "                        frequency_penalty=0.1)),\n",
    "    ]\n",
    "\n",
    "\n",
    "def process_requests(engine: LLMEngine,\n",
    "                     test_prompts: List[Tuple[str, SamplingParams]]):\n",
    "    \"\"\"Continuously process a list of prompts and handle the outputs.\"\"\"\n",
    "    request_id = 0\n",
    "\n",
    "    while test_prompts or engine.has_unfinished_requests():\n",
    "        if test_prompts:\n",
    "            prompt, sampling_params = test_prompts.pop(0)\n",
    "            engine.add_request(str(request_id), prompt, sampling_params)\n",
    "            request_id += 1\n",
    "\n",
    "        request_outputs: List[RequestOutput] = engine.step()\n",
    "\n",
    "        for request_output in request_outputs:\n",
    "            if request_output.finished:\n",
    "                print(request_output)\n",
    "\n",
    "\n",
    "def initialize_engine(args: argparse.Namespace) -> LLMEngine:\n",
    "    \"\"\"Initialize the LLMEngine from the command line arguments.\"\"\"\n",
    "    engine_args = EngineArgs.from_cli_args(args)\n",
    "    return LLMEngine.from_engine_args(engine_args)\n",
    "\n",
    "\n",
    "def main(args: argparse.Namespace):\n",
    "    \"\"\"Main function that sets up and runs the prompt processing.\"\"\"\n",
    "    engine = initialize_engine(args)\n",
    "    test_prompts = create_test_prompts()\n",
    "    process_requests(engine, test_prompts)\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    parser = FlexibleArgumentParser(\n",
    "        description='Demo on using the LLMEngine class directly')\n",
    "    parser = EngineArgs.add_cli_args(parser)\n",
    "    args = parser.parse_args()\n",
    "    main(args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1178e636-b555-46b4-b370-718a7b776a68",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2323a3086a7e4f60b35928cea2a9c09e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/857 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 10-17 05:29:04 config.py:179] Gemma 2 uses sliding window attention for every odd layer, which is currently not supported by vLLM. Disabling sliding window and capping the max length to the sliding window size (4096).\n",
      "INFO 10-17 05:29:09 llm_engine.py:237] Initializing an LLM engine (vdev) with config: model='google/gemma-2-9b-it', speculative_config=None, tokenizer='google/gemma-2-9b-it', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, rope_scaling=None, rope_theta=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=4096, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto, quantization_param_path=None, device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='outlines'), observability_config=ObservabilityConfig(otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=0, served_model_name=google/gemma-2-9b-it, use_v2_block_manager=True, num_scheduler_steps=1, chunked_prefill_enabled=False multi_step_stream_outputs=True, enable_prefix_caching=False, use_async_output_proc=True, use_cached_outputs=False, mm_processor_kwargs=None)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "981a584aa68f41e7a2aacff8a27c2c1f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/47.0k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7dd046a32a5f4687a17d59fd19307379",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.model:   0%|          | 0.00/4.24M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6914e5588ae44141a210783564d23926",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/17.5M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f7959ee37c0a4b64b39375517264f763",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/636 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6754e2ac752e434c9f6da01747235bda",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/173 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 10-17 05:29:11 model_runner.py:1060] Starting to load model google/gemma-2-9b-it...\n",
      "INFO 10-17 05:29:11 weight_utils.py:243] Using model weights format ['*.safetensors']\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "038e0f30a7dd4552adb69512fac484ff",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00003-of-00004.safetensors:   0%|          | 0.00/4.96G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4288bec92a4f4e29a70cc64e87c6d902",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00002-of-00004.safetensors:   0%|          | 0.00/4.95G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c05d09a62ffe400dac13a8ad0670d5ac",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00004-of-00004.safetensors:   0%|          | 0.00/3.67G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f092e227525f4e62ab7373bfc65b15a5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00001-of-00004.safetensors:   0%|          | 0.00/4.90G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0304bc18cc9b4e2ca7e2f3e77ed40905",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors.index.json:   0%|          | 0.00/39.1k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0f46262c9c4245229bc1b5bee996af83",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading safetensors checkpoint shards:   0% Completed | 0/4 [00:00<?, ?it/s]\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 10-17 05:29:29 model_runner.py:1071] Loading model weights took 17.2179 GB\n",
      "INFO 10-17 05:29:35 gpu_executor.py:122] # GPU blocks: 3851, # CPU blocks: 780\n",
      "INFO 10-17 05:29:35 gpu_executor.py:126] Maximum concurrency for 4096 tokens per request: 15.04x\n",
      "INFO 10-17 05:29:38 model_runner.py:1402] Capturing the model for CUDA graphs. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI.\n",
      "INFO 10-17 05:29:38 model_runner.py:1406] CUDA graphs can take additional 1~3 GiB memory per GPU. If you are running out of memory, consider decreasing `gpu_memory_utilization` or enforcing eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.\n",
      "INFO 10-17 05:30:15 model_runner.py:1530] Graph capturing finished in 36 secs.\n"
     ]
    }
   ],
   "source": [
    "from vllm import EngineArgs\n",
    "\n",
    "engine_args = EngineArgs(\n",
    "    model=\"google/gemma-2-9b-it\",\n",
    "    enable_lora=True,\n",
    "    max_loras=1,\n",
    "    max_lora_rank=8,\n",
    "    max_cpu_loras=2,\n",
    "    max_num_seqs=256\n",
    ")\n",
    "\n",
    "from vllm import LLMEngine\n",
    "\n",
    "# initialize engine\n",
    "engine = LLMEngine.from_engine_args(engine_args)\n",
    "#engine = LLMEngine(model_config = \"google/gemma-2-9b-it\")\n",
    "# set request arguments\n",
    "example_prompt = \"Who is the president of the United States?\"\n",
    "sampling_params = SamplingParams(temperature=0.0)\n",
    "request_id = 0\n",
    "\n",
    "# add the request to the engine\n",
    "engine.add_request(\n",
    "    str(request_id),\n",
    "    example_prompt,\n",
    "    SamplingParams(temperature=0.0))\n",
    "    # continue the request processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0b47a706-3a69-4fd2-a3b3-cdd202e471ab",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/vllm/connections.py:8: RuntimeWarning: Failed to read commit hash:\n",
      "No module named 'vllm._version'\n",
      "  from vllm.version import __version__ as VLLM_VERSION\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 10-17 06:21:09 config.py:179] Gemma 2 uses sliding window attention for every odd layer, which is currently not supported by vLLM. Disabling sliding window and capping the max length to the sliding window size (4096).\n",
      "INFO 10-17 06:21:14 llm_engine.py:237] Initializing an LLM engine (vdev) with config: model='google/gemma-2-9b-it', speculative_config=None, tokenizer='google/gemma-2-9b-it', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, rope_scaling=None, rope_theta=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=4096, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto, quantization_param_path=None, device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='outlines'), observability_config=ObservabilityConfig(otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=0, served_model_name=google/gemma-2-9b-it, use_v2_block_manager=True, num_scheduler_steps=1, chunked_prefill_enabled=False multi_step_stream_outputs=True, enable_prefix_caching=False, use_async_output_proc=True, use_cached_outputs=False, mm_processor_kwargs=None)\n",
      "INFO 10-17 06:21:15 model_runner.py:1060] Starting to load model google/gemma-2-9b-it...\n",
      "INFO 10-17 06:21:16 weight_utils.py:243] Using model weights format ['*.safetensors']\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4c832da299144a718a62f8273f4db1ac",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading safetensors checkpoint shards:   0% Completed | 0/4 [00:00<?, ?it/s]\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 10-17 06:21:19 model_runner.py:1071] Loading model weights took 17.2179 GB\n",
      "INFO 10-17 06:21:24 gpu_executor.py:122] # GPU blocks: 3861, # CPU blocks: 780\n",
      "INFO 10-17 06:21:24 gpu_executor.py:126] Maximum concurrency for 4096 tokens per request: 15.08x\n",
      "INFO 10-17 06:21:28 model_runner.py:1402] Capturing the model for CUDA graphs. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI.\n",
      "INFO 10-17 06:21:28 model_runner.py:1406] CUDA graphs can take additional 1~3 GiB memory per GPU. If you are running out of memory, consider decreasing `gpu_memory_utilization` or enforcing eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.\n",
      "INFO 10-17 06:21:57 model_runner.py:1530] Graph capturing finished in 28 secs.\n",
      "INFO 10-17 06:22:02 metrics.py:345] Avg prompt throughput: 11.9 tokens/s, Avg generation throughput: 228.4 tokens/s, Running: 10 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 2.1%, CPU KV cache usage: 0.0%.\n",
      "INFO 10-17 06:22:07 metrics.py:345] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 242.6 tokens/s, Running: 10 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 4.1%, CPU KV cache usage: 0.0%.\n",
      "INFO 10-17 06:22:12 metrics.py:345] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 235.7 tokens/s, Running: 10 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 6.0%, CPU KV cache usage: 0.0%.\n",
      "INFO 10-17 06:22:17 metrics.py:345] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 230.4 tokens/s, Running: 10 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 7.8%, CPU KV cache usage: 0.0%.\n",
      "request 0 has finished\n",
      "request 1 has finished\n",
      "request 2 has finished\n",
      "request 3 has finished\n",
      "request 4 has finished\n",
      "request 5 has finished\n",
      "request 6 has finished\n",
      "request 7 has finished\n",
      "request 8 has finished\n",
      "request 9 has finished\n"
     ]
    }
   ],
   "source": [
    "# Please see the example/ folder for more detailed examples.\n",
    "from vllm import EngineArgs, LLMEngine, SamplingParams\n",
    "import time\n",
    "\n",
    "engine_args = EngineArgs(\n",
    "    model=\"google/gemma-2-9b-it\",\n",
    ")\n",
    "\"\"\"\n",
    "    enable_lora=True,\n",
    "    max_loras=1,\n",
    "    max_lora_rank=8,\n",
    "    max_cpu_loras=2,\n",
    "    max_num_seqs=256\n",
    ")\n",
    "\"\"\"\n",
    "\n",
    "# initialize engine and request arguments\n",
    "engine = LLMEngine.from_engine_args(engine_args)\n",
    "\n",
    "example_inputs = [(i, \"What is LLM?\", SamplingParams(temperature=0.0, max_tokens=1000)) for i in range(10)]\n",
    "\n",
    "finished_request_ids = []\n",
    "# Start the engine with an event loop\n",
    "while True:\n",
    "    if example_inputs:\n",
    "        req_id, prompt, sampling_params = example_inputs.pop(0)\n",
    "        engine.add_request(str(req_id),prompt,sampling_params)\n",
    "\n",
    "    # continue the request processing\n",
    "    request_outputs = engine.step()\n",
    "    for request_output in request_outputs:\n",
    "        if request_output.finished and request_output.request_id not in finished_request_ids:\n",
    "            # return or show the request output\n",
    "            finished_request_ids.append(request_output.request_id)\n",
    "            print(f\"request {request_output.request_id} has finished\")\n",
    "            #print(request_output.outputs[0].text)\n",
    "\n",
    "    time.sleep(10)\n",
    "\n",
    "    if not (engine.has_unfinished_requests() or example_inputs):\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "57487002-9b03-4b88-a238-e1590fd3db5e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "LLM stands for **Large Language Model**. \n",
      "\n",
      "It's a type of artificial intelligence (AI) that excels at understanding and generating human-like text. \n",
      "\n",
      "Here's a breakdown:\n",
      "\n",
      "* **Large:** LLMs are trained on massive datasets of text and code, often containing billions or even trillions of words. This vast amount of data allows them to learn complex patterns and relationships within language.\n",
      "* **Language:** LLMs are specifically designed to work with language. They can read, understand, interpret, and generate text in various formats, including articles, stories, poems, code, and more.\n",
      "* **Model:** An LLM is a mathematical model that represents knowledge about language. It uses this knowledge to perform tasks such as:\n",
      "\n",
      "    * **Text Generation:** Writing creative content, summarizing text, translating languages, and more.\n",
      "    * **Text Comprehension:** Answering questions, identifying sentiment, and extracting information from text.\n",
      "    * **Code Generation:** Writing and debugging code in different programming languages.\n",
      "    * **Dialogue Systems:** Engaging in natural-sounding conversations with humans.\n",
      "\n",
      "**Examples of LLMs:**\n",
      "\n",
      "* GPT-3 (Generative Pre-trained Transformer 3) by OpenAI\n",
      "* LaMDA (Language Model for Dialogue Applications) by Google\n",
      "* BERT (Bidirectional Encoder Representations from Transformers) by Google\n",
      "\n",
      "**Key Features of LLMs:**\n",
      "\n",
      "* **Contextual Understanding:** LLMs can understand the context of a conversation or text passage, allowing them to generate more relevant and coherent responses.\n",
      "* **Generative Capabilities:** They can create new text that is grammatically correct and stylistically appropriate.\n",
      "* **Adaptability:** LLMs can be fine-tuned for specific tasks or domains, improving their performance in those areas.\n",
      "\n",
      "**Applications of LLMs:**\n",
      "\n",
      "LLMs have a wide range of applications, including:\n",
      "\n",
      "* **Chatbots and Virtual Assistants:** Providing automated customer service and support.\n",
      "* **Content Creation:** Generating articles, blog posts, marketing copy, and other types of content.\n",
      "* **Education:** Assisting with homework, providing personalized learning experiences, and grading essays.\n",
      "* **Research:** Analyzing large amounts of text data to uncover insights and trends.\n",
      "* **Healthcare:** Summarizing patient records, assisting with diagnosis, and providing medical information.\n",
      "\n",
      "\n",
      "\n",
      "Let me know if you have any other questions about LLMs!\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n\\nLLM stands for **Large Language Model**. \\n\\nIt's a type of artificial intelligence (AI) that excels at understanding and generating human-like text. \\n\\nHere's a breakdown:\\n\\n* **Large:** LLMs are trained on massive datasets of text and code, often containing billions or even trillions of words. This vast amount of data allows them to learn complex patterns and relationships within language.\\n* **Language:** LLMs are specifically designed to work with language. They can read, understand, interpret, and generate text in various formats, including articles, stories, poems, code, and more.\\n* **Model:** An LLM is a mathematical model that represents knowledge about language. It uses this knowledge to perform tasks such as:\\n\\n    * **Text Generation:** Writing creative content, summarizing text, translating languages, and more.\\n    * **Text Comprehension:** Answering questions, identifying sentiment, and extracting information from text.\\n    * **Code Generation:** Writing and debugging code in different programming languages.\\n    * **Dialogue Systems:** Engaging in natural-sounding conversations with humans.\\n\\n**Examples of LLMs:**\\n\\n* GPT-3 (Generative Pre-trained Transformer 3) by OpenAI\\n* LaMDA (Language Model for Dialogue Applications) by Google\\n* BERT (Bidirectional Encoder Representations from Transformers) by Google\\n\\n**Key Features of LLMs:**\\n\\n* **Contextual Understanding:** LLMs can understand the context of a conversation or text passage, allowing them to generate more relevant and coherent responses.\\n* **Generative Capabilities:** They can create new text that is grammatically correct and stylistically appropriate.\\n* **Adaptability:** LLMs can be fine-tuned for specific tasks or domains, improving their performance in those areas.\\n\\n**Applications of LLMs:**\\n\\nLLMs have a wide range of applications, including:\\n\\n* **Chatbots and Virtual Assistants:** Providing automated customer service and support.\\n* **Content Creation:** Generating articles, blog posts, marketing copy, and other types of content.\\n* **Education:** Assisting with homework, providing personalized learning experiences, and grading essays.\\n* **Research:** Analyzing large amounts of text data to uncover insights and trends.\\n* **Healthcare:** Summarizing patient records, assisting with diagnosis, and providing medical information.\\n\\n\\n\\nLet me know if you have any other questions about LLMs!\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "871f2858-36e6-4415-a381-f8678e152c5b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'0'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "request_output.request_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a7cb0284-dbfa-4cf9-a59f-afd8b13c71a2",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: fastapi in /usr/local/lib/python3.10/dist-packages (0.115.2)\n",
      "Requirement already satisfied: starlette<0.41.0,>=0.37.2 in /usr/local/lib/python3.10/dist-packages (from fastapi) (0.40.0)\n",
      "Requirement already satisfied: pydantic!=1.8,!=1.8.1,!=2.0.0,!=2.0.1,!=2.1.0,<3.0.0,>=1.7.4 in /usr/local/lib/python3.10/dist-packages (from fastapi) (2.9.2)\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from fastapi) (4.12.2)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.10/dist-packages (from pydantic!=1.8,!=1.8.1,!=2.0.0,!=2.0.1,!=2.1.0,<3.0.0,>=1.7.4->fastapi) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.23.4 in /usr/local/lib/python3.10/dist-packages (from pydantic!=1.8,!=1.8.1,!=2.0.0,!=2.0.1,!=2.1.0,<3.0.0,>=1.7.4->fastapi) (2.23.4)\n",
      "Requirement already satisfied: anyio<5,>=3.4.0 in /usr/local/lib/python3.10/dist-packages (from starlette<0.41.0,>=0.37.2->fastapi) (4.0.0)\n",
      "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.10/dist-packages (from anyio<5,>=3.4.0->starlette<0.41.0,>=0.37.2->fastapi) (3.4)\n",
      "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.10/dist-packages (from anyio<5,>=3.4.0->starlette<0.41.0,>=0.37.2->fastapi) (1.3.0)\n",
      "Requirement already satisfied: exceptiongroup>=1.0.2 in /usr/local/lib/python3.10/dist-packages (from anyio<5,>=3.4.0->starlette<0.41.0,>=0.37.2->fastapi) (1.1.3)\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.3.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.2\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython -m pip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install fastapi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d47571c5-447b-44bd-9c94-7509b2bf0465",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "365315ab0ae4464186507e0bfa8a221a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/857 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 10-18 00:17:33 config.py:185] gemma2 has interleaved attention, which is currently not supported by vLLM. Disabling sliding window and capping the max length to the sliding window size (4096).\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'EngineArgs' object has no attribute 'disable_log_requests'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 11\u001b[0m\n\u001b[1;32m      6\u001b[0m engine_args \u001b[38;5;241m=\u001b[39m EngineArgs(\n\u001b[1;32m      7\u001b[0m     model\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgoogle/gemma-2-9b-it\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m      8\u001b[0m )\n\u001b[1;32m     10\u001b[0m \u001b[38;5;66;03m# initialize the engine and the example input\u001b[39;00m\n\u001b[0;32m---> 11\u001b[0m engine \u001b[38;5;241m=\u001b[39m \u001b[43mAsyncLLMEngine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_engine_args\u001b[49m\u001b[43m(\u001b[49m\u001b[43mengine_args\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     12\u001b[0m example_input \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m     13\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minput\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mWhat is LLM?\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m     14\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrequest_id\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;241m0\u001b[39m,\n\u001b[1;32m     15\u001b[0m }\n\u001b[1;32m     17\u001b[0m \u001b[38;5;66;03m# start the generation\u001b[39;00m\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/vllm/engine/async_llm_engine.py:675\u001b[0m, in \u001b[0;36mAsyncLLMEngine.from_engine_args\u001b[0;34m(cls, engine_args, engine_config, start_engine_loop, usage_context, stat_loggers)\u001b[0m\n\u001b[1;32m    669\u001b[0m     initialize_ray_cluster(engine_config\u001b[38;5;241m.\u001b[39mparallel_config)\n\u001b[1;32m    671\u001b[0m \u001b[38;5;66;03m# Create the async LLM engine.\u001b[39;00m\n\u001b[1;32m    672\u001b[0m engine \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mcls\u001b[39m(\n\u001b[1;32m    673\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mengine_config\u001b[38;5;241m.\u001b[39mto_dict(),\n\u001b[1;32m    674\u001b[0m     executor_class\u001b[38;5;241m=\u001b[39mexecutor_class,\n\u001b[0;32m--> 675\u001b[0m     log_requests\u001b[38;5;241m=\u001b[39m\u001b[38;5;129;01mnot\u001b[39;00m \u001b[43mengine_args\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdisable_log_requests\u001b[49m,\n\u001b[1;32m    676\u001b[0m     log_stats\u001b[38;5;241m=\u001b[39m\u001b[38;5;129;01mnot\u001b[39;00m engine_args\u001b[38;5;241m.\u001b[39mdisable_log_stats,\n\u001b[1;32m    677\u001b[0m     start_engine_loop\u001b[38;5;241m=\u001b[39mstart_engine_loop,\n\u001b[1;32m    678\u001b[0m     usage_context\u001b[38;5;241m=\u001b[39musage_context,\n\u001b[1;32m    679\u001b[0m     stat_loggers\u001b[38;5;241m=\u001b[39mstat_loggers,\n\u001b[1;32m    680\u001b[0m )\n\u001b[1;32m    681\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m engine\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'EngineArgs' object has no attribute 'disable_log_requests'"
     ]
    }
   ],
   "source": [
    "# Please refer to entrypoints/api_server.py for\n",
    "# the complete example.\n",
    "from vllm import EngineArgs, LLMEngine, SamplingParams, PoolingParams\n",
    "import time\n",
    "\n",
    "engine_args = EngineArgs(\n",
    "    model=\"google/gemma-2-9b-it\",\n",
    ")\n",
    "\n",
    "# initialize the engine and the example input\n",
    "engine = AsyncLLMEngine.from_engine_args(engine_args)\n",
    "example_input = {\n",
    "    \"input\": \"What is LLM?\",\n",
    "    \"request_id\": 0,\n",
    "}\n",
    "\n",
    "# start the generation\n",
    "results_generator = engine.encode(\n",
    "   example_input[\"input\"],\n",
    "   PoolingParams(),\n",
    "   example_input[\"request_id\"])\n",
    "\n",
    "# get the results\n",
    "final_output = None\n",
    "async for request_output in results_generator:\n",
    "    if await request.is_disconnected():\n",
    "        # Abort the request if the client disconnects.\n",
    "        await engine.abort(request_id)\n",
    "        # Return or raise an error\n",
    "        raise Exception(\"error\")\n",
    "    final_output = request_output\n",
    "    print(final_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f9e946b4-606d-49c4-8a39-4ba6a34b9f6a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 10-18 04:41:09 config.py:185] gemma2 has interleaved attention, which is currently not supported by vLLM. Disabling sliding window and capping the max length to the sliding window size (4096).\n",
      "INFO 10-18 04:41:14 llm_engine.py:237] Initializing an LLM engine (v0.6.3.post1) with config: model='google/gemma-2-9b-it', speculative_config=None, tokenizer='google/gemma-2-9b-it', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, rope_scaling=None, rope_theta=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=4096, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto, quantization_param_path=None, device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='outlines'), observability_config=ObservabilityConfig(otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=0, served_model_name=google/gemma-2-9b-it, num_scheduler_steps=1, chunked_prefill_enabled=False multi_step_stream_outputs=True, enable_prefix_caching=False, use_async_output_proc=True, use_cached_outputs=False, mm_processor_kwargs=None)\n",
      "INFO 10-18 04:41:15 model_runner.py:1056] Starting to load model google/gemma-2-9b-it...\n",
      "INFO 10-18 04:41:16 weight_utils.py:243] Using model weights format ['*.safetensors']\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b50db032a3ff4811a834073268400ab9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading safetensors checkpoint shards:   0% Completed | 0/4 [00:00<?, ?it/s]\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 10-18 04:41:19 model_runner.py:1067] Loading model weights took 17.2179 GB\n",
      "INFO 10-18 04:41:24 gpu_executor.py:122] # GPU blocks: 3861, # CPU blocks: 780\n",
      "INFO 10-18 04:41:24 gpu_executor.py:126] Maximum concurrency for 4096 tokens per request: 15.08x\n",
      "INFO 10-18 04:41:28 model_runner.py:1395] Capturing the model for CUDA graphs. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI.\n",
      "INFO 10-18 04:41:28 model_runner.py:1399] CUDA graphs can take additional 1~3 GiB memory per GPU. If you are running out of memory, consider decreasing `gpu_memory_utilization` or enforcing eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.\n",
      "INFO 10-18 04:41:49 model_runner.py:1523] Graph capturing finished in 21 secs.\n",
      "INFO 10-18 04:41:49 async_llm_engine.py:207] Added request 0.\n",
      "INFO 10-18 04:41:50 async_llm_engine.py:175] Finished request 0.\n",
      "1\n",
      "0.7815337181091309\n",
      "RequestOutput(request_id=0, prompt='What is LLM?', prompt_token_ids=[2, 1841, 603, 629, 18622, 235336], encoder_prompt=None, encoder_prompt_token_ids=None, prompt_logprobs=None, outputs=[CompletionOutput(index=0, text=\"\\n\\nLLM stands for **Large Language Model**. \\n\\nIt's a\", token_ids=(109, 1650, 235296, 12353, 604, 5231, 23218, 14944, 5708, 168428, 235248, 109, 1718, 235303, 235256, 476), cumulative_logprob=None, logprobs=None, finish_reason=length, stop_reason=None)], finished=True, metrics=RequestMetrics(arrival_time=1729226509.6000824, last_token_time=1729226510.349622, first_scheduled_time=1729226509.6106594, first_token_time=1729226509.7286708, time_in_queue=0.010576963424682617, finished_time=1729226510.3452027, scheduler_time=0.020527787506580353, model_forward_time=None, model_execute_time=None), lora_request=None)\n",
      "INFO 10-18 04:41:50 async_llm_engine.py:207] Added request 0.\n",
      "INFO 10-18 04:41:51 async_llm_engine.py:175] Finished request 0.\n",
      "2\n",
      "1.5805273056030273\n",
      "RequestOutput(request_id=0, prompt='What is LLM?', prompt_token_ids=[2, 1841, 603, 629, 18622, 235336], encoder_prompt=None, encoder_prompt_token_ids=None, prompt_logprobs=None, outputs=[CompletionOutput(index=0, text=\"\\n\\nLLM stands for **Large Language Model**. \\n\\nIt's a\", token_ids=(109, 1650, 235296, 12353, 604, 5231, 23218, 14944, 5708, 168428, 235248, 109, 1718, 235303, 235256, 476), cumulative_logprob=None, logprobs=None, finish_reason=length, stop_reason=None)], finished=True, metrics=RequestMetrics(arrival_time=1729226510.3857877, last_token_time=1729226511.150561, first_scheduled_time=1729226510.4023001, first_token_time=1729226510.4908943, time_in_queue=0.016512393951416016, finished_time=1729226511.1445196, scheduler_time=0.03442481905221939, model_forward_time=None, model_execute_time=None), lora_request=None)\n"
     ]
    }
   ],
   "source": [
    "# Please refer to entrypoints/api_server.py for\n",
    "# the complete example.\n",
    "\n",
    "from vllm import AsyncLLMEngine, EngineArgs, AsyncEngineArgs, LLMEngine, SamplingParams, PoolingParams\n",
    "import time\n",
    "\n",
    "engine_args = AsyncEngineArgs(\n",
    "    model=\"google/gemma-2-9b-it\",\n",
    ")\n",
    "\n",
    "# initialize the engine and the example input\n",
    "engine = AsyncLLMEngine.from_engine_args(engine_args)\n",
    "example_input = {\n",
    "    \"prompt\": \"What is LLM?\",\n",
    "    \"stream\": False, # assume the non-streaming case\n",
    "    \"temperature\": 0.0,\n",
    "    \"request_id\": 0,\n",
    "}\n",
    "\n",
    "# start the generation\n",
    "results_generator = engine.generate(\n",
    "   example_input[\"prompt\"],\n",
    "   SamplingParams(temperature=example_input[\"temperature\"]),\n",
    "   example_input[\"request_id\"])\n",
    "\n",
    "start = time.time()\n",
    "\n",
    "# get the results\n",
    "final_output = None\n",
    "async for request_output in results_generator:  # the mechanism under this (AsyncGenerator from the typing module): https://chatgpt.com/share/6711c243-dd54-8006-9a54-3c2cb1b59ce9\n",
    "    \"\"\"\n",
    "    if await request_output.is_disconnected():\n",
    "        # Abort the request if the client disconnects.\n",
    "        await engine.abort(request_id)\n",
    "        # Return or raise an error\n",
    "        raise Exception(\"error\")\n",
    "    \"\"\"\n",
    "    final_output = request_output\n",
    "\n",
    "wrap1 = time.time()\n",
    "\n",
    "print(\"1\")\n",
    "print(wrap1 - start)\n",
    "print(final_output)\n",
    "\n",
    "results_generator2 = engine.generate(\n",
    "   example_input[\"prompt\"],\n",
    "   SamplingParams(temperature=example_input[\"temperature\"]),\n",
    "   example_input[\"request_id\"])\n",
    "\n",
    "async for request_output in results_generator2:  # the mechanism under this (AsyncGenerator from the typing module): https://chatgpt.com/share/6711c243-dd54-8006-9a54-3c2cb1b59ce9\n",
    "    final_output = request_output\n",
    "\n",
    "wrap2 = time.time()\n",
    "\n",
    "print(\"2\")\n",
    "print(wrap2 - start)\n",
    "# Process and return the final output\n",
    "print(final_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a2fe6f35-a840-473a-8ead-b0ed627bcfe2",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 10-18 06:43:57 config.py:185] gemma2 has interleaved attention, which is currently not supported by vLLM. Disabling sliding window and capping the max length to the sliding window size (4096).\n",
      "INFO 10-18 06:44:01 llm_engine.py:237] Initializing an LLM engine (v0.6.3.post1) with config: model='google/gemma-2-9b-it', speculative_config=None, tokenizer='google/gemma-2-9b-it', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, rope_scaling=None, rope_theta=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=4096, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto, quantization_param_path=None, device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='outlines'), observability_config=ObservabilityConfig(otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=0, served_model_name=google/gemma-2-9b-it, num_scheduler_steps=1, chunked_prefill_enabled=False multi_step_stream_outputs=True, enable_prefix_caching=False, use_async_output_proc=True, use_cached_outputs=False, mm_processor_kwargs=None)\n",
      "INFO 10-18 06:44:02 model_runner.py:1056] Starting to load model google/gemma-2-9b-it...\n",
      "INFO 10-18 06:44:03 weight_utils.py:243] Using model weights format ['*.safetensors']\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "92bd732fe72c4d4e9e1849dd9ce5feba",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading safetensors checkpoint shards:   0% Completed | 0/4 [00:00<?, ?it/s]\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 10-18 06:44:06 model_runner.py:1067] Loading model weights took 17.2179 GB\n",
      "INFO 10-18 06:44:11 gpu_executor.py:122] # GPU blocks: 3861, # CPU blocks: 780\n",
      "INFO 10-18 06:44:11 gpu_executor.py:126] Maximum concurrency for 4096 tokens per request: 15.08x\n",
      "INFO 10-18 06:44:16 model_runner.py:1395] Capturing the model for CUDA graphs. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI.\n",
      "INFO 10-18 06:44:16 model_runner.py:1399] CUDA graphs can take additional 1~3 GiB memory per GPU. If you are running out of memory, consider decreasing `gpu_memory_utilization` or enforcing eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.\n",
      "INFO 10-18 06:44:39 model_runner.py:1523] Graph capturing finished in 23 secs.\n",
      "\n",
      "request 0 started\n",
      "INFO 10-18 06:44:39 async_llm_engine.py:207] Added request 0.\n",
      "\n",
      "request 1 started\n",
      "INFO 10-18 06:44:39 async_llm_engine.py:207] Added request 1.\n",
      "INFO 10-18 06:44:44 metrics.py:349] Avg prompt throughput: 2.2 tokens/s, Avg generation throughput: 50.3 tokens/s, Running: 2 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.5%, CPU KV cache usage: 0.0%.\n",
      "INFO 10-18 06:44:49 metrics.py:349] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 50.3 tokens/s, Running: 2 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.9%, CPU KV cache usage: 0.0%.\n",
      "INFO 10-18 06:44:53 async_llm_engine.py:175] Finished request 1.\n",
      "\n",
      "request_id: 1\n",
      "calculation time: 13.758394002914429\n",
      "text_outputs: ['What is Japan?\\n\\nJapan is an island nation located in East Asia. \\n\\nHere are some key facts about Japan:\\n\\n* **Geography:** Japan is an archipelago of over 6,800 islands, with the four largest being Honshu, Hokkaido, Kyushu, and Shikoku. It is situated in the Pacific Ocean, east of the Asian mainland.\\n* **Culture:** Japan has a rich and unique culture, known for its traditions, art, cuisine, and technology. Some notable aspects include:\\n    * **Shintoism and Buddhism:** The two major religions in Japan.\\n    * **Tea ceremony:** A traditional ritual involving the preparation and serving of matcha tea.\\n    * **Calligraphy and painting:** Highly valued art forms with a long history.\\n    * **Anime and manga:** Popular forms of Japanese animation and comics.\\n* **Economy:** Japan is a highly developed and industrialized nation with a strong economy. It is a major exporter of automobiles, electronics, and other manufactured goods.\\n* **Government:** Japan is a constitutional monarchy with a parliamentary system of government. The Emperor is the head of state, while the Prime Minister is the head of government.\\n* **Population:** Japan has a population of over 125 million people, with a high life expectancy and a relatively low birth rate.\\n\\n**Interesting Facts:**\\n\\n* Japan is home to Mount Fuji, the highest mountain in Japan and a popular tourist destination.\\n* The Japanese writing system uses a combination of three scripts: kanji (Chinese characters), hiragana, and katakana.\\n* Japan has a unique tradition of onsen, natural hot springs that are believed to have health benefits.\\n\\n\\n']\n",
      "INFO 10-18 06:44:54 metrics.py:349] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 44.0 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.6%, CPU KV cache usage: 0.0%.\n",
      "INFO 10-18 06:44:58 async_llm_engine.py:175] Finished request 0.\n",
      "\n",
      "request_id: 0\n",
      "calculation time: 19.259371757507324\n",
      "text_outputs: [\"What is LLM?\\n\\nLLM stands for **Large Language Model**. \\n\\nIt's a type of artificial intelligence (AI) that excels at understanding and generating human-like text. \\n\\nHere's a breakdown:\\n\\n* **Large:** LLMs are trained on massive datasets of text and code, often containing billions or even trillions of words. This vast amount of data allows them to learn complex patterns and relationships within language.\\n* **Language:** LLMs are specifically designed to work with language. They can read, understand, interpret, and generate text in various formats, including articles, stories, poems, code, and more.\\n* **Model:** An LLM is a mathematical model that represents knowledge about language. It uses this knowledge to perform tasks such as:\\n\\n    * **Text Generation:** Writing creative content, summarizing text, translating languages, and more.\\n    * **Text Comprehension:** Answering questions, identifying sentiment, and extracting information from text.\\n    * **Code Generation:** Writing and debugging code in different programming languages.\\n    * **Dialogue Systems:** Engaging in natural-sounding conversations with humans.\\n\\n**Examples of LLMs:**\\n\\n* GPT-3 (Generative Pre-trained Transformer 3) by OpenAI\\n* LaMDA (Language Model for Dialogue Applications) by Google\\n* BERT (Bidirectional Encoder Representations from Transformers) by Google\\n\\n**Key Features of LLMs:**\\n\\n* **Contextual Understanding:** LLMs can understand the context of a conversation or text passage, allowing them to generate more relevant and coherent responses.\\n* **Generative Capabilities:** They can create new text that is grammatically correct and stylistically appropriate.\\n* **Adaptability:** LLMs can be fine-tuned for specific tasks or domains, improving their performance in those areas.\\n\\n**Applications of LLMs:**\\n\\nLLMs have a wide range of applications, including:\\n\\n* **Chatbots and Virtual Assistants:** Providing automated customer service and support.\\n* **Content Creation:** Generating articles, blog posts, marketing copy, and other types of content.\\n* **Education:** Assisting with homework, providing personalized learning experiences, and automating grading.\\n* **Research:** Analyzing large amounts of text data to uncover insights and trends.\\n* **Healthcare:** Summarizing patient records, assisting with diagnosis, and providing medical information.\\n\\n\\n\\nLet me know if you have any other questions about LLMs!\\n\"]\n",
      "[\"What is LLM?\\n\\nLLM stands for **Large Language Model**. \\n\\nIt's a type of artificial intelligence (AI) that excels at understanding and generating human-like text. \\n\\nHere's a breakdown:\\n\\n* **Large:** LLMs are trained on massive datasets of text and code, often containing billions or even trillions of words. This vast amount of data allows them to learn complex patterns and relationships within language.\\n* **Language:** LLMs are specifically designed to work with language. They can read, understand, interpret, and generate text in various formats, including articles, stories, poems, code, and more.\\n* **Model:** An LLM is a mathematical model that represents knowledge about language. It uses this knowledge to perform tasks such as:\\n\\n    * **Text Generation:** Writing creative content, summarizing text, translating languages, and more.\\n    * **Text Comprehension:** Answering questions, identifying sentiment, and extracting information from text.\\n    * **Code Generation:** Writing and debugging code in different programming languages.\\n    * **Dialogue Systems:** Engaging in natural-sounding conversations with humans.\\n\\n**Examples of LLMs:**\\n\\n* GPT-3 (Generative Pre-trained Transformer 3) by OpenAI\\n* LaMDA (Language Model for Dialogue Applications) by Google\\n* BERT (Bidirectional Encoder Representations from Transformers) by Google\\n\\n**Key Features of LLMs:**\\n\\n* **Contextual Understanding:** LLMs can understand the context of a conversation or text passage, allowing them to generate more relevant and coherent responses.\\n* **Generative Capabilities:** They can create new text that is grammatically correct and stylistically appropriate.\\n* **Adaptability:** LLMs can be fine-tuned for specific tasks or domains, improving their performance in those areas.\\n\\n**Applications of LLMs:**\\n\\nLLMs have a wide range of applications, including:\\n\\n* **Chatbots and Virtual Assistants:** Providing automated customer service and support.\\n* **Content Creation:** Generating articles, blog posts, marketing copy, and other types of content.\\n* **Education:** Assisting with homework, providing personalized learning experiences, and automating grading.\\n* **Research:** Analyzing large amounts of text data to uncover insights and trends.\\n* **Healthcare:** Summarizing patient records, assisting with diagnosis, and providing medical information.\\n\\n\\n\\nLet me know if you have any other questions about LLMs!\\n\"]\n",
      "['What is Japan?\\n\\nJapan is an island nation located in East Asia. \\n\\nHere are some key facts about Japan:\\n\\n* **Geography:** Japan is an archipelago of over 6,800 islands, with the four largest being Honshu, Hokkaido, Kyushu, and Shikoku. It is situated in the Pacific Ocean, east of the Asian mainland.\\n* **Culture:** Japan has a rich and unique culture, known for its traditions, art, cuisine, and technology. Some notable aspects include:\\n    * **Shintoism and Buddhism:** The two major religions in Japan.\\n    * **Tea ceremony:** A traditional ritual involving the preparation and serving of matcha tea.\\n    * **Calligraphy and painting:** Highly valued art forms with a long history.\\n    * **Anime and manga:** Popular forms of Japanese animation and comics.\\n* **Economy:** Japan is a highly developed and industrialized nation with a strong economy. It is a major exporter of automobiles, electronics, and other manufactured goods.\\n* **Government:** Japan is a constitutional monarchy with a parliamentary system of government. The Emperor is the head of state, while the Prime Minister is the head of government.\\n* **Population:** Japan has a population of over 125 million people, with a high life expectancy and a relatively low birth rate.\\n\\n**Interesting Facts:**\\n\\n* Japan is home to Mount Fuji, the highest mountain in Japan and a popular tourist destination.\\n* The Japanese writing system uses a combination of three scripts: kanji (Chinese characters), hiragana, and katakana.\\n* Japan has a unique tradition of onsen, natural hot springs that are believed to have health benefits.\\n\\n\\n']\n"
     ]
    }
   ],
   "source": [
    "# Please refer to entrypoints/api_server.py for\n",
    "# the complete example.\n",
    "import asyncio\n",
    "from vllm import AsyncLLMEngine, AsyncEngineArgs, SamplingParams\n",
    "import time\n",
    "\n",
    "engine_args = AsyncEngineArgs(\n",
    "    model=\"google/gemma-2-9b-it\",\n",
    ")\n",
    "\n",
    "# initialize the engine and the example input\n",
    "engine = AsyncLLMEngine.from_engine_args(engine_args)\n",
    "example_input = {\n",
    "    \"prompt\": \"What is LLM?\",\n",
    "    \"stream\": False, # assume the non-streaming case\n",
    "    \"temperature\": 0.0,\n",
    "    \"request_id\": 0,\n",
    "}\n",
    "\n",
    "async def add_request(prompt, request_id, start):\n",
    "    print()\n",
    "    print(f\"request {request_id} started\")\n",
    "    \n",
    "    # start the generation\n",
    "    final_output = None\n",
    "    results_generator = engine.generate(prompt, SamplingParams(temperature=0.0, max_tokens=1000), request_id)\n",
    "    async for request_output in results_generator:\n",
    "        final_output = request_output\n",
    "\n",
    "    text_outputs = [prompt + output.text for output in final_output.outputs]\n",
    "\n",
    "    print()\n",
    "    print(f\"request_id: {request_id}\")\n",
    "    print(f\"calculation time: {time.time()-start}\")\n",
    "    print(f\"text_outputs: {text_outputs}\")\n",
    "    \n",
    "    return text_outputs\n",
    "    \n",
    "start = time.time()\n",
    "answer1 = asyncio.create_task(add_request(\"What is LLM?\", 0, start))\n",
    "answer2 = asyncio.create_task(add_request(\"What is Japan?\", 1, start))\n",
    "\n",
    "print(await answer1)\n",
    "print(await answer2)\n",
    "\n",
    "#answer1 = asyncio.run(add_request(\"What is LLM?\", 0, start))\n",
    "#answer2 = asyncio.run(add_request(\"What is Japan?\", 1, start))\n",
    "\n",
    "#print(\"answer1: \", answer1)\n",
    "#print(\"answer2: \", answer2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4dcd60ce-d4e7-45e7-a14d-42b05c803602",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "started at 05:57:48\n",
      "ended at 05:57:48\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_8971/4171694787.py:11: RuntimeWarning: coroutine 'say_after' was never awaited\n",
      "  say_after(1, 'hello')\n",
      "RuntimeWarning: Enable tracemalloc to get the object allocation traceback\n",
      "/tmp/ipykernel_8971/4171694787.py:13: RuntimeWarning: coroutine 'say_after' was never awaited\n",
      "  say_after(2, 'world')\n",
      "RuntimeWarning: Enable tracemalloc to get the object allocation traceback\n"
     ]
    }
   ],
   "source": [
    "import asyncio, time\n",
    "\n",
    "async def say_after(delay, what):\n",
    "    print(f'before_sleep: {what}')\n",
    "    await asyncio.sleep(delay)\n",
    "    print(what)\n",
    "\n",
    "async def execute():\n",
    "    print(f'started at {time.strftime(\"%X\")}')\n",
    "    #await say_after(1, 'hello')\n",
    "    say_after(1, 'hello')\n",
    "    #await say_after(2, 'world')\n",
    "    say_after(2, 'world')\n",
    "    print(f'ended at {time.strftime(\"%X\")}')\n",
    "\n",
    "event_loop = asyncio.get_running_loop()\n",
    "if event_loop.is_running():\n",
    "    task = asyncio.create_task(execute())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6cf22829-46dd-46e1-8287-7909bec7b082",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello, asyncio!\n"
     ]
    }
   ],
   "source": [
    "import asyncio,time\n",
    "\n",
    "async def my_coroutine():\n",
    "    print(\"Hello, asyncio!\")\n",
    "\n",
    "event_loop = asyncio.get_running_loop()\n",
    "if event_loop.is_running():\n",
    "    task = asyncio.create_task(my_coroutine())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c7339b3-e660-46f7-b8aa-17666bbdb183",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import asyncio\n",
    "from vllm import AsyncLLMEngine, AsyncEngineArgs, SamplingParams\n",
    "import time\n",
    "\n",
    "engine_args = AsyncEngineArgs(\n",
    "    model=\"google/gemma-2-9b-it\",\n",
    ")\n",
    "\n",
    "# initialize the engine and the example input\n",
    "engine = AsyncLLMEngine.from_engine_args(engine_args)\n",
    "start = time.time()\n",
    "\n",
    "async def add_request(prompt, request_id, start):\n",
    "    print()\n",
    "    print(f\"request {request_id} started\")\n",
    "    \n",
    "    # start the generation\n",
    "    final_output = None\n",
    "    results_generator = engine.generate(prompt, SamplingParams(temperature=0.0, max_tokens=1000), request_id)\n",
    "    async for request_output in results_generator:\n",
    "        # print(request_output) => for streaming\n",
    "        final_output = request_output\n",
    "\n",
    "    text_outputs = [prompt + output.text for output in final_output.outputs]\n",
    "\n",
    "    print()\n",
    "    print(f\"request_id: {request_id}\")\n",
    "    print(f\"calculation time: {time.time()-start}\")\n",
    "    #print(f\"text_outputs: {text_outputs}\")\n",
    "\n",
    "    if request_id == 0:\n",
    "        #task2 = asyncio.create_task(add_request(f\"Give me some keywords of the following sentence '''{final_output.outputs[0].text}'''\", 1, start))\n",
    "        #answer2 = await task2\n",
    "\n",
    "        answer2 = await add_request(f\"Give me some keywords of the following sentence '''{final_output.outputs[0].text}'''\", 1, start)\n",
    "\n",
    "        text_outputs += answer2\n",
    "    \n",
    "    return text_outputs\n",
    "\n",
    "\n",
    "async def get_answer():\n",
    "    #task1 = asyncio.create_task(add_request(\"What is LLM?\", 0, start))\n",
    "    #answer1 = await task1\n",
    "\n",
    "    answer1 = await add_request(\"What is LLM?\", 0, start)\n",
    "\n",
    "    print()\n",
    "    print()\n",
    "    print(\"======\")\n",
    "    print(answer1)\n",
    "\n",
    "\n",
    "await get_answer()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2bb20091-44b9-4ea8-9e0d-c3b0c6cce552",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Countdown: 5\n",
      "Countdown: 4\n",
      "Countdown: 3\n",
      "Countdown: 2\n",
      "Countdown: 1\n"
     ]
    }
   ],
   "source": [
    "from typing import AsyncGenerator\n",
    "\n",
    "# Define an asynchronous generator function\n",
    "async def countdown(start: int) -> AsyncGenerator[int, None]:\n",
    "    for i in range(start, 0, -1):\n",
    "        yield i\n",
    "        await asyncio.sleep(1)  # Simulate async work\n",
    "\n",
    "# Using the asynchronous generator\n",
    "import asyncio\n",
    "\n",
    "async def main():\n",
    "    async for number in countdown(5):\n",
    "        print(f\"Countdown: {number}\")\n",
    "\n",
    "# Run the async main function\n",
    "await main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "750250de-3aff-45d4-9a57-891a23aff7f3",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Task A: Compute factorial(2), currently i=2...\n",
      "Task B: Compute factorial(3), currently i=2...\n",
      "Task C: Compute factorial(4), currently i=2...\n",
      "Task A: factorial(2) = 2\n",
      "Task B: Compute factorial(3), currently i=3...\n",
      "Task C: Compute factorial(4), currently i=3...\n",
      "Task B: factorial(3) = 6\n",
      "Task C: Compute factorial(4), currently i=4...\n",
      "Task C: factorial(4) = 24\n",
      "[2, 6, 24]\n"
     ]
    }
   ],
   "source": [
    "import asyncio\n",
    "\n",
    "\n",
    "async def factorial(name, number):\n",
    "    f = 1\n",
    "    for i in range(2, number + 1):\n",
    "        print(f\"Task {name}: Compute factorial({number}), currently i={i}...\")\n",
    "        await asyncio.sleep(1)\n",
    "        f *= i\n",
    "    print(f\"Task {name}: factorial({number}) = {f}\")\n",
    "    return f\n",
    "\n",
    "async def execute():\n",
    "    task_a = asyncio.create_task(factorial('A', 2))\n",
    "    task_b = asyncio.create_task(factorial('B', 3))\n",
    "    task_c = asyncio.create_task(factorial('C', 4))\n",
    "\n",
    "    L = []\n",
    "    L.append(await task_a)\n",
    "    L.append(await task_b)\n",
    "    L.append(await task_c)\n",
    "\n",
    "    print(L)\n",
    "\n",
    "async def execute2():\n",
    "    # Schedule three calls *concurrently*:\n",
    "    L = await asyncio.gather(\n",
    "        factorial(\"A\", 2),\n",
    "        factorial(\"B\", 3),\n",
    "        factorial(\"C\", 4),\n",
    "    )\n",
    "    print(L)\n",
    "\n",
    "\n",
    "#asyncio.run(execute())\n",
    "await execute2()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4a1d4c64-ba9b-43b6-b536-a77188938201",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Task A: Compute factorial(2), currently i=2...\n",
      "Task B: Compute factorial(3), currently i=2...\n",
      "Task C: Compute factorial(4), currently i=2...\n",
      "Task A: factorial(2) = 2\n",
      "Task B: Compute factorial(3), currently i=3...\n",
      "Task C: Compute factorial(4), currently i=3...\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'cacaller_selfller_frame' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 43\u001b[0m\n\u001b[1;32m     39\u001b[0m     \u001b[38;5;28mprint\u001b[39m(L)\n\u001b[1;32m     42\u001b[0m \u001b[38;5;66;03m#asyncio.run(execute())\u001b[39;00m\n\u001b[0;32m---> 43\u001b[0m \u001b[38;5;28;01mawait\u001b[39;00m execute2()\n",
      "Cell \u001b[0;32mIn[1], line 34\u001b[0m, in \u001b[0;36mexecute2\u001b[0;34m()\u001b[0m\n\u001b[1;32m     32\u001b[0m \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mexecute2\u001b[39m():\n\u001b[1;32m     33\u001b[0m     \u001b[38;5;66;03m# Schedule three calls *concurrently*:\u001b[39;00m\n\u001b[0;32m---> 34\u001b[0m     L \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mawait\u001b[39;00m asyncio\u001b[38;5;241m.\u001b[39mgather(\n\u001b[1;32m     35\u001b[0m         factorial(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mA\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;241m2\u001b[39m),\n\u001b[1;32m     36\u001b[0m         factorial(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mB\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;241m3\u001b[39m),\n\u001b[1;32m     37\u001b[0m         factorial(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mC\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;241m4\u001b[39m),\n\u001b[1;32m     38\u001b[0m     )\n\u001b[1;32m     39\u001b[0m     \u001b[38;5;28mprint\u001b[39m(L)\n",
      "Cell \u001b[0;32mIn[1], line 15\u001b[0m, in \u001b[0;36mfactorial\u001b[0;34m(name, number)\u001b[0m\n\u001b[1;32m     13\u001b[0m current_frame \u001b[38;5;241m=\u001b[39m inspect\u001b[38;5;241m.\u001b[39mcurrentframe()\n\u001b[1;32m     14\u001b[0m caller_frame \u001b[38;5;241m=\u001b[39m current_frame\u001b[38;5;241m.\u001b[39mf_back\n\u001b[0;32m---> 15\u001b[0m caller_self \u001b[38;5;241m=\u001b[39m cacaller_selfller_frame\u001b[38;5;241m.\u001b[39mf_locals\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mself\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m) \u001b[38;5;66;03m# Get the instance that called the function\u001b[39;00m\n\u001b[1;32m     16\u001b[0m \u001b[38;5;28mprint\u001b[39m(caller_self)\n\u001b[1;32m     18\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m f\n",
      "\u001b[0;31mNameError\u001b[0m: name 'cacaller_selfller_frame' is not defined"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Task B: factorial(3) = 6\n",
      "Task C: Compute factorial(4), currently i=4...\n",
      "Task C: factorial(4) = 24\n"
     ]
    }
   ],
   "source": [
    "import asyncio, inspect\n",
    "\n",
    "\n",
    "async def factorial(name, number):\n",
    "    f = 1\n",
    "    for i in range(2, number + 1):\n",
    "        print(f\"Task {name}: Compute factorial({number}), currently i={i}...\")\n",
    "        await asyncio.sleep(1)\n",
    "        f *= i\n",
    "    print(f\"Task {name}: factorial({number}) = {f}\")\n",
    "\n",
    "    # get the instance which called this job_instance\n",
    "    current_frame = inspect.currentframe()\n",
    "    caller_frame = current_frame.f_back\n",
    "    caller_self = cacaller_selfller_frame.f_locals.get('self', None) # Get the instance that called the function\n",
    "    print(caller_self)\n",
    "    \n",
    "    return f\n",
    "\n",
    "async def execute():\n",
    "    task_a = asyncio.create_task(factorial('A', 2))\n",
    "    task_b = asyncio.create_task(factorial('B', 3))\n",
    "    task_c = asyncio.create_task(factorial('C', 4))\n",
    "\n",
    "    L = []\n",
    "    L.append(await task_a)\n",
    "    L.append(await task_b)\n",
    "    L.append(await task_c)\n",
    "\n",
    "    print(L)\n",
    "\n",
    "async def execute2():\n",
    "    # Schedule three calls *concurrently*:\n",
    "    L = await asyncio.gather(\n",
    "        factorial(\"A\", 2),\n",
    "        factorial(\"B\", 3),\n",
    "        factorial(\"C\", 4),\n",
    "    )\n",
    "    print(L)\n",
    "\n",
    "\n",
    "#asyncio.run(execute())\n",
    "await execute2()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2550e198-8de9-49f1-804c-a542ec031678",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calling method_in_superclass with args: (<__main__.SubClass object at 0x7a4b54267a00>,) and kwargs: {}\n",
      "In SuperClass method\n"
     ]
    }
   ],
   "source": [
    "def log_method_calls(cls):\n",
    "    original_methods = cls.__dict__.copy()\n",
    "    for method_name, method in original_methods.items():\n",
    "        if callable(method) and not method_name.startswith(\"__\"):\n",
    "            setattr(cls, method_name, log_call(method))\n",
    "    return cls\n",
    "\n",
    "def log_call(func):\n",
    "    def wrapper(*args, **kwargs):\n",
    "        print(f\"Calling {func.__name__} with args: {args} and kwargs: {kwargs}\")\n",
    "        return func(*args, **kwargs)\n",
    "    return wrapper\n",
    "\n",
    "@log_method_calls\n",
    "class SuperClass:\n",
    "    def method_in_superclass(self):\n",
    "        print(\"In SuperClass method\")\n",
    "\n",
    "class SubClass(SuperClass):\n",
    "    def method_in_subclass(self):\n",
    "        self.method_in_superclass()\n",
    "\n",
    "sub = SubClass()\n",
    "sub.method_in_subclass()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "26a72274-6466-411f-95c3-2018f72592dc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('MakeStrategy', 'AIMO2.MakeStrategy')"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from AIMO2.MakeStrategy import MakeStrategy\n",
    "MakeStrategy.__name__, MakeStrategy.__module__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8018da43-2753-4b25-b999-8bd4c5fd90ac",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bba4029d-75f5-4c93-bf36-cd0ed8c77316",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
